{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import data from the processing of data in the journal_training_word2vec notebook\n",
    "df = pickle.load(open('/mnt/mypartition/pubmed_nlp_research/C_article_replication_and_translational_classification/mixed_journals_article/df.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#testing just on article\n",
    "df = pickle.load(open('/mnt/mypartition/pubmed_nlp_research/C_article_replication_and_translational_classification/df.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#testing just on journals\n",
    "df = pickle.load(open('/mnt/mypartition/pubmed_nlp_research/C_article_replication_and_translational_classification/pickled_objects/dfJournals_trans_categories.p', 'rb'))\n",
    "val_art = pickle.load(open('/mnt/mypartition/pubmed_nlp_research/C_article_replication_and_translational_classification/df.p', 'rb'))\n",
    "df.columns = ['label', 'title', 'abstract', 'qualifier_terms', 'descriptor_terms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_art = pickle.load(open('/mnt/mypartition/pubmed_nlp_research/C_article_replication_and_translational_classification/mixed_journals_article/val_art.p', 'rb'))\n",
    "val_journals = pickle.load(open('/mnt/mypartition/pubmed_nlp_research/C_article_replication_and_translational_classification/mixed_journals_article/val_journals.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(354, 6)\n",
      "['pmid' 'title' 'abstract' 'qualifier_terms' 'descriptor_terms' 'label']\n",
      "['T0' 'T1/T2' 'T3/T4' 'TX']\n"
     ]
    }
   ],
   "source": [
    "print df.shape\n",
    "print df.columns.values\n",
    "print df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>qualifier_terms</th>\n",
       "      <th>descriptor_terms</th>\n",
       "      <th>label</th>\n",
       "      <th>boolean_label</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Pulmonary hypertension and other potentially f...</td>\n",
       "      <td>Systemic juvenile idiopathic arthritis (JIA) i...</td>\n",
       "      <td>complications diagnosis mortality diagnosis et...</td>\n",
       "      <td>Adolescent Arthritis, Juvenile Child Child, Pr...</td>\n",
       "      <td>T0</td>\n",
       "      <td>1</td>\n",
       "      <td>article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>Are increased weight and appetite useful indic...</td>\n",
       "      <td>During childhood and adolescence, physiologica...</td>\n",
       "      <td>physiology diagnosis physiopathology physiology</td>\n",
       "      <td>Adolescent Appetite Child Child, Preschool Dep...</td>\n",
       "      <td>T0</td>\n",
       "      <td>1</td>\n",
       "      <td>article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>How physician electronic health record screen ...</td>\n",
       "      <td>Use of electronic health records (EHRs) in pri...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>T3/T4</td>\n",
       "      <td>0</td>\n",
       "      <td>article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>Early identification and treatment of Alzheime...</td>\n",
       "      <td>Alzheimer's disease (AD) is a progressive neur...</td>\n",
       "      <td>diagnosis economics psychology therapy economi...</td>\n",
       "      <td>Alzheimer Disease Caregivers Cost Savings Cost...</td>\n",
       "      <td>T3/T4</td>\n",
       "      <td>0</td>\n",
       "      <td>article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>Caregiver person-centeredness and behavioral s...</td>\n",
       "      <td>Mealtime behavioral symptoms are distressing a...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>T3/T4</td>\n",
       "      <td>0</td>\n",
       "      <td>article</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "54   Pulmonary hypertension and other potentially f...   \n",
       "270  Are increased weight and appetite useful indic...   \n",
       "191  How physician electronic health record screen ...   \n",
       "213  Early identification and treatment of Alzheime...   \n",
       "320  Caregiver person-centeredness and behavioral s...   \n",
       "\n",
       "                                              abstract  \\\n",
       "54   Systemic juvenile idiopathic arthritis (JIA) i...   \n",
       "270  During childhood and adolescence, physiologica...   \n",
       "191  Use of electronic health records (EHRs) in pri...   \n",
       "213  Alzheimer's disease (AD) is a progressive neur...   \n",
       "320  Mealtime behavioral symptoms are distressing a...   \n",
       "\n",
       "                                       qualifier_terms  \\\n",
       "54   complications diagnosis mortality diagnosis et...   \n",
       "270    physiology diagnosis physiopathology physiology   \n",
       "191                                                      \n",
       "213  diagnosis economics psychology therapy economi...   \n",
       "320                                                      \n",
       "\n",
       "                                      descriptor_terms  label  boolean_label  \\\n",
       "54   Adolescent Arthritis, Juvenile Child Child, Pr...     T0              1   \n",
       "270  Adolescent Appetite Child Child, Preschool Dep...     T0              1   \n",
       "191                                                     T3/T4              0   \n",
       "213  Alzheimer Disease Caregivers Cost Savings Cost...  T3/T4              0   \n",
       "320                                                     T3/T4              0   \n",
       "\n",
       "     dataset  \n",
       "54   article  \n",
       "270  article  \n",
       "191  article  \n",
       "213  article  \n",
       "320  article  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>qualifier_terms</th>\n",
       "      <th>descriptor_terms</th>\n",
       "      <th>label</th>\n",
       "      <th>boolean_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31401</th>\n",
       "      <td>Loss of lysosomal ion channel transient recept...</td>\n",
       "      <td>Mucolipidosis type IV (MLIV) is a lysosomal st...</td>\n",
       "      <td>genetics metabolism genetics metabolism pathol...</td>\n",
       "      <td>Cathepsin B Cytoplasm HeLa Cells Humans Lysoso...</td>\n",
       "      <td>T0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73091</th>\n",
       "      <td>Bendamustine plus rituximab versus fludarabine...</td>\n",
       "      <td>Fludarabine-based chemoimmunotherapy with ritu...</td>\n",
       "      <td>adverse effects therapeutic use administration...</td>\n",
       "      <td>Aged Antineoplastic Combined Chemotherapy Prot...</td>\n",
       "      <td>T3/T4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22852</th>\n",
       "      <td>Oligonucleotide recombination enabled site-spe...</td>\n",
       "      <td>Recombineering refers to a strategy for engine...</td>\n",
       "      <td>genetics metabolism methods chemistry genetics...</td>\n",
       "      <td>Bacteria Mutagenesis, Site-Directed Oligonucle...</td>\n",
       "      <td>T0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70755</th>\n",
       "      <td>A Comparative biochemical study on two marine ...</td>\n",
       "      <td>Two marine endophytic bacteria were isolated f...</td>\n",
       "      <td>pharmacology pharmacology growth &amp; development...</td>\n",
       "      <td>Anti-Infective Agents Antioxidants Bacillus En...</td>\n",
       "      <td>T3/T4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42100</th>\n",
       "      <td>High therapeutic potential of positive alloste...</td>\n",
       "      <td>There are currently no clinically efficacious ...</td>\n",
       "      <td>pharmacology drug effects pathology physiology...</td>\n",
       "      <td>Animals Anti-Inflammatory Agents Astrocytes Br...</td>\n",
       "      <td>T1/T2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "31401  Loss of lysosomal ion channel transient recept...   \n",
       "73091  Bendamustine plus rituximab versus fludarabine...   \n",
       "22852  Oligonucleotide recombination enabled site-spe...   \n",
       "70755  A Comparative biochemical study on two marine ...   \n",
       "42100  High therapeutic potential of positive alloste...   \n",
       "\n",
       "                                                abstract  \\\n",
       "31401  Mucolipidosis type IV (MLIV) is a lysosomal st...   \n",
       "73091  Fludarabine-based chemoimmunotherapy with ritu...   \n",
       "22852  Recombineering refers to a strategy for engine...   \n",
       "70755  Two marine endophytic bacteria were isolated f...   \n",
       "42100  There are currently no clinically efficacious ...   \n",
       "\n",
       "                                         qualifier_terms  \\\n",
       "31401  genetics metabolism genetics metabolism pathol...   \n",
       "73091  adverse effects therapeutic use administration...   \n",
       "22852  genetics metabolism methods chemistry genetics...   \n",
       "70755  pharmacology pharmacology growth & development...   \n",
       "42100  pharmacology drug effects pathology physiology...   \n",
       "\n",
       "                                        descriptor_terms  label  boolean_label  \n",
       "31401  Cathepsin B Cytoplasm HeLa Cells Humans Lysoso...     T0              1  \n",
       "73091  Aged Antineoplastic Combined Chemotherapy Prot...  T3/T4              0  \n",
       "22852  Bacteria Mutagenesis, Site-Directed Oligonucle...     T0              1  \n",
       "70755  Anti-Infective Agents Antioxidants Bacillus En...  T3/T4              0  \n",
       "42100  Animals Anti-Inflammatory Agents Astrocytes Br...  T1/T2              0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_journals[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>qualifier_terms</th>\n",
       "      <th>descriptor_terms</th>\n",
       "      <th>label</th>\n",
       "      <th>boolean_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Where is the action? Action sentence processin...</td>\n",
       "      <td>According to an influential view of conceptual...</td>\n",
       "      <td>diagnosis etiology physiology complications ph...</td>\n",
       "      <td>Aged Aged, 80 and over Analysis of Variance Co...</td>\n",
       "      <td>T0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>Frontal gray matter reduction after breast can...</td>\n",
       "      <td>Cognitive changes related to cancer and its tr...</td>\n",
       "      <td>adverse effects therapeutic use pathology drug...</td>\n",
       "      <td>Adult Aged Antineoplastic Combined Chemotherap...</td>\n",
       "      <td>T3/T4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>Deriving competencies for mentors of clinical ...</td>\n",
       "      <td>Although the importance of research mentorship...</td>\n",
       "      <td>education education</td>\n",
       "      <td>Career Mobility Communication Ethics, Research...</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>Long-term satisfaction and predictors of use o...</td>\n",
       "      <td>Intracorporeal injections have low use rates a...</td>\n",
       "      <td>drug therapy etiology statistics &amp; numerical d...</td>\n",
       "      <td>Erectile Dysfunction Humans Injections Male Mi...</td>\n",
       "      <td>T3/T4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Association of 9-hydroxy risperidone concentra...</td>\n",
       "      <td>Risperidone has been used to treat behavioral ...</td>\n",
       "      <td>drug therapy adverse effects pharmacokinetics ...</td>\n",
       "      <td>Aged Aged, 80 and over Alzheimer Disease Antip...</td>\n",
       "      <td>T1/T2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "59   Where is the action? Action sentence processin...   \n",
       "329  Frontal gray matter reduction after breast can...   \n",
       "237  Deriving competencies for mentors of clinical ...   \n",
       "178  Long-term satisfaction and predictors of use o...   \n",
       "135  Association of 9-hydroxy risperidone concentra...   \n",
       "\n",
       "                                              abstract  \\\n",
       "59   According to an influential view of conceptual...   \n",
       "329  Cognitive changes related to cancer and its tr...   \n",
       "237  Although the importance of research mentorship...   \n",
       "178  Intracorporeal injections have low use rates a...   \n",
       "135  Risperidone has been used to treat behavioral ...   \n",
       "\n",
       "                                       qualifier_terms  \\\n",
       "59   diagnosis etiology physiology complications ph...   \n",
       "329  adverse effects therapeutic use pathology drug...   \n",
       "237                                education education   \n",
       "178  drug therapy etiology statistics & numerical d...   \n",
       "135  drug therapy adverse effects pharmacokinetics ...   \n",
       "\n",
       "                                      descriptor_terms  label  boolean_label  \n",
       "59   Aged Aged, 80 and over Analysis of Variance Co...     T0              1  \n",
       "329  Adult Aged Antineoplastic Combined Chemotherap...  T3/T4              0  \n",
       "237  Career Mobility Communication Ethics, Research...     TX              0  \n",
       "178  Erectile Dysfunction Humans Injections Male Mi...  T3/T4              0  \n",
       "135  Aged Aged, 80 and over Alzheimer Disease Antip...  T1/T2              0  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_art[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ROUND', 1)\n",
      "Sizes of train and test sets:\n",
      "77637\n",
      "19412\n",
      "Cleaning and parsing the training set abstracts...\n",
      "\n",
      "77637\n",
      "Creating the bag of words...\n",
      "\n",
      "5000\n",
      "(77637, 5000)\n",
      "Cleaning and parsing the test set abstracts...\n",
      "\n",
      "Adding tfidf weights and converting to Bag of Words...\n",
      "Training the random forest...\n",
      "3\n",
      "3\n",
      "Using the random forest to make sentiment label predictions...\n",
      "Scoring the test set\n",
      "{0: 0.9752431797421196, 1: 0.90251974542318292, 2: 0.90582853409396247}\n",
      "Cleaning and parsing validation set 1's abstracts...\n",
      "\n",
      "Scoring validation set 1...\n",
      "{0: 0.60995370370370361, 1: 0.73915055532702589, 2: 0.70597381342062193}\n",
      "------------------------------------------------------------------\n",
      "\n",
      "Average scores on test set: \n",
      "('T0: ', 0.9752431797421196)\n",
      "('T1/T2: ', 0.90251974542318292)\n",
      "('T3/T4: ', 0.90582853409396247)\n",
      "Average scores on article validation set: \n",
      "('T0: ', 0.60995370370370361)\n",
      "('T1/T2: ', 0.73915055532702589)\n",
      "('T3/T4: ', 0.70597381342062193)\n",
      "0:08:27\n"
     ]
    }
   ],
   "source": [
    "a = datetime.datetime.now().replace(microsecond=0)\n",
    "\n",
    "y = df['label'].values\n",
    "skf = StratifiedKFold(y, n_folds=5, shuffle=True)\n",
    "\n",
    "#include mesh terms in bag of words?\n",
    "mesh_terms = True\n",
    "#remove stopwords?\n",
    "stops = False\n",
    "#if we want to set max features or not for the BOWs\n",
    "maxF = 5000\n",
    "#stem words?\n",
    "stem = False\n",
    "\n",
    "j = 1\n",
    "avgT0 = []\n",
    "avgT1T2 = []\n",
    "avgT3T4 = []\n",
    "val_scores_journals_T0 = []\n",
    "val_scores_journals_T1T2 = []\n",
    "val_scores_journals_T3T4 = []\n",
    "val_scores_article_T0 = []\n",
    "val_scores_article_T1T2 = []\n",
    "val_scores_article_T3T4 = []\n",
    "\n",
    "\n",
    "for train_index, test_index in skf:\n",
    "    #get first test/train split and use\n",
    "    if j == 1:\n",
    "\n",
    "        print(\"ROUND\", j)\n",
    "\n",
    "        #use the indexes to subset the df pandas dataframe\n",
    "        train1, test1 = df.iloc[train_index], df.iloc[test_index]\n",
    "\n",
    "        print \"Sizes of train and test sets:\"\n",
    "        print len(train1)\n",
    "        print len(test1)\n",
    "\n",
    "        print \"Cleaning and parsing the training set abstracts...\\n\"\n",
    "        # Initialize an empty list to hold the clean reviews\n",
    "        clean_train_reviews = []\n",
    "\n",
    "        # Loop over each review; create an index i that goes from 0 to the length\n",
    "        # of the pandas df column\n",
    "        for i in range(train1[\"abstract\"].size):\n",
    "            # Call our function for each one, and add the result to the list of\n",
    "            # clean reviews\n",
    "            words = KaggleWord2VecUtility.review_to_words( train1[\"abstract\"].iloc[i], remove_stopwords=stops, stemmer=stem)\n",
    "            words = words + ' ' + KaggleWord2VecUtility.review_to_words( train1[\"title\"].iloc[i], remove_stopwords=stops, stemmer=stem)\n",
    "            if mesh_terms:\n",
    "                words = words + ' ' + KaggleWord2VecUtility.review_to_words( train1[\"descriptor_terms\"].iloc[i], remove_stopwords=stops, stemmer=stem)\n",
    "                words = words + ' ' + KaggleWord2VecUtility.review_to_words( train1[\"qualifier_terms\"].iloc[i], remove_stopwords=stops, stemmer=stem)\n",
    "            clean_train_reviews.append(words)\n",
    "\n",
    "        print len(clean_train_reviews)\n",
    "\n",
    "        #with tfidf word weighting\n",
    "        #uses L2 norm by default\n",
    "        print \"Creating the bag of words...\\n\"\n",
    "\n",
    "        # Initialize the \"TfidfVectorizer\" object, which is scikit-learn's\n",
    "        # bag of words tool.  \n",
    "        vectorizer = TfidfVectorizer(analyzer = \"word\",   \\\n",
    "                                 tokenizer = None,    \\\n",
    "                                 preprocessor = None, \\\n",
    "                                #already removed stopwords when preprocessing reviews into words\n",
    "                                 stop_words = None,\n",
    "                                max_features = maxF)\n",
    "\n",
    "        # fit_transform() does two functions: First, it fits the model\n",
    "        # and learns the vocabulary; second, it transforms our training data\n",
    "        # into feature vectors. The input to fit_transform should be a list of \n",
    "        # strings.\n",
    "        train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
    "        print len(vectorizer.get_feature_names())\n",
    "\n",
    "        # Numpy arrays are easy to work with, so convert the result to an \n",
    "        # array\n",
    "        train_data_features = train_data_features.toarray()\n",
    "        print train_data_features.shape\n",
    "\n",
    "        #Moving on to test set...\n",
    "        # Create an empty list and append the clean reviews one by one\n",
    "        clean_test_reviews = [] \n",
    "\n",
    "        print \"Cleaning and parsing the test set abstracts...\\n\"\n",
    "        for i in range(test1[\"abstract\"].size):    \n",
    "            words = KaggleWord2VecUtility.review_to_words( test1[\"abstract\"].iloc[i], remove_stopwords=stops, stemmer=stem)\n",
    "            words = words + ' ' + KaggleWord2VecUtility.review_to_words( test1[\"title\"].iloc[i], remove_stopwords=stops, stemmer=stem)\n",
    "            if mesh_terms:\n",
    "                words = words + ' ' + KaggleWord2VecUtility.review_to_words( test1[\"descriptor_terms\"].iloc[i], remove_stopwords=stops, stemmer=stem)\n",
    "                words = words + ' ' + KaggleWord2VecUtility.review_to_words( test1[\"qualifier_terms\"].iloc[i], remove_stopwords=stops, stemmer=stem)\n",
    "            clean_test_reviews.append(words)\n",
    "\n",
    "        print \"Adding tfidf weights and converting to Bag of Words...\"\n",
    "        # Get a bag of words for the test set, and convert to a numpy array\n",
    "        test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "        test_data_features = test_data_features.toarray()\n",
    "\n",
    "        #------------------------------------------------------------------------------------------------------------    \n",
    "        print \"Training the random forest...\"\n",
    "\n",
    "        #binarize labels\n",
    "        x = label_binarize(train1['label'], classes=['T0', 'T1/T2', 'T3/T4'])\n",
    "        n_classes = x.shape[1]\n",
    "        print n_classes\n",
    "\n",
    "        x_test = label_binarize(test1['label'], classes=['T0', 'T1/T2', 'T3/T4'])\n",
    "\n",
    "        # Initialize a Random Forest classifier with 100 trees\n",
    "        forest = OneVsRestClassifier(RandomForestClassifier(n_estimators = 100, max_depth=1000, n_jobs=-1))\n",
    "\n",
    "        # Fit the forest to the training set, using the bag of words as \n",
    "        # features and the sentiment labels as the response variable\n",
    "        #\n",
    "        # This may take a few minutes to run\n",
    "        forest = forest.fit( train_data_features, x )\n",
    "\n",
    "        #get the feature importance for each of the three fits\n",
    "        print len(forest.estimators_)\n",
    "        feats = []\n",
    "        for i in range(len(forest.estimators_)):\n",
    "            feats.append(forest.estimators_[i].feature_importances_)\n",
    "\n",
    "        print \"Using the random forest to make sentiment label predictions...\"\n",
    "        result = forest.predict(test_data_features)\n",
    "\n",
    "        print \"Scoring the test set\"\n",
    "\n",
    "        # Compute ROC curve and ROC area for each class\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        for i in range(n_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(x_test[:, i], result[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        print roc_auc\n",
    "\n",
    "        avgT0.append(roc_auc[0])\n",
    "        avgT1T2.append(roc_auc[1])\n",
    "        avgT3T4.append(roc_auc[2])\n",
    "\n",
    "        #---------------------------------------------------------------------------------------------------------------\n",
    "        #Moving on to validation set...\n",
    "        # Create an empty list and append the clean reviews one by one\n",
    "        clean_test_reviews_val_art = [] \n",
    "\n",
    "        print \"Cleaning and parsing validation set 1's abstracts...\\n\"\n",
    "        for i in range(val_art[\"abstract\"].size):    \n",
    "            words = KaggleWord2VecUtility.review_to_words( val_art[\"abstract\"].iloc[i], remove_stopwords=stops)\n",
    "            words = words + ' ' + KaggleWord2VecUtility.review_to_words( val_art[\"title\"].iloc[i], remove_stopwords=stops)\n",
    "            if mesh_terms:\n",
    "                words = words + ' ' + KaggleWord2VecUtility.review_to_words( val_art[\"descriptor_terms\"].iloc[i], remove_stopwords=stops)\n",
    "                words = words + ' ' + KaggleWord2VecUtility.review_to_words( val_art[\"qualifier_terms\"].iloc[i], remove_stopwords=stops)\n",
    "            clean_test_reviews_val_art.append(words)\n",
    "\n",
    "        # Get a bag of words for the validation set, and convert to a numpy array\n",
    "        test_data_features_val_art = vectorizer.transform(clean_test_reviews_val_art)\n",
    "        test_data_features_val_art = test_data_features_val_art.toarray()\n",
    "\n",
    "    #     #repeat for second validation set\n",
    "    #     clean_test_reviews_val_journals = [] \n",
    "\n",
    "    #     print \"Cleaning and parsing validation set 2's abstracts...\\n\"\n",
    "    #     for i in range(val_journals[\"abstract\"].size):    \n",
    "    #         words = KaggleWord2VecUtility.review_to_words( val_journals[\"abstract\"].iloc[i], remove_stopwords=stops)\n",
    "    #         words = words + ' ' + KaggleWord2VecUtility.review_to_words( val_journals[\"title\"].iloc[i], remove_stopwords=stops)\n",
    "    #         if mesh_terms:\n",
    "    #             words = words + ' ' + KaggleWord2VecUtility.review_to_words( val_journals[\"descriptor_terms\"].iloc[i], remove_stopwords=stops)\n",
    "    #             words = words + ' ' + KaggleWord2VecUtility.review_to_words( val_journals[\"qualifier_terms\"].iloc[i], remove_stopwords=stops)\n",
    "    #         clean_test_reviews_val_journals.append(words)\n",
    "\n",
    "    #     # Get a bag of words for the validation set, and convert to a numpy array\n",
    "    #     test_data_features_val_journals = vectorizer.transform(clean_test_reviews_val_journals)\n",
    "    #     test_data_features_val_journals = test_data_features_val_journals.toarray()\n",
    "\n",
    "        print \"Scoring validation set 1...\"\n",
    "        # Use the random forest to make sentiment label predictions\n",
    "        #using forest fitted on training data\n",
    "        result_val = forest.predict(test_data_features_val_art)\n",
    "\n",
    "        x_test_val = label_binarize(val_art['label'], classes=['T0', 'T1/T2', 'T3/T4'])\n",
    "\n",
    "        # Compute ROC curve and ROC area for each class\n",
    "        fpr_val = dict()\n",
    "        tpr_val = dict()\n",
    "        roc_auc_val = dict()\n",
    "        for i in range(n_classes):\n",
    "            fpr_val[i], tpr_val[i], _ = roc_curve(x_test_val[:, i], result_val[:, i])\n",
    "            roc_auc_val[i] = auc(fpr_val[i], tpr_val[i])\n",
    "\n",
    "        print roc_auc_val\n",
    "        val_scores_article_T0.append(roc_auc_val[0])\n",
    "        val_scores_article_T1T2.append(roc_auc_val[1])\n",
    "        val_scores_article_T3T4.append(roc_auc_val[2])  \n",
    "\n",
    "    #     print \"Scoring validation set 2...\"\n",
    "    #     # Use the random forest to make sentiment label predictions\n",
    "    #     #using forest fitted on training data\n",
    "    #     result_val = forest.predict(test_data_features_val_journals)\n",
    "\n",
    "    #     x_test_val = label_binarize(val_journals['label'], classes=['T0', 'T1/T2', 'T3/T4'])\n",
    "\n",
    "    #     # Compute ROC curve and ROC area for each class\n",
    "    #     fpr_val = dict()\n",
    "    #     tpr_val = dict()\n",
    "    #     roc_auc_val = dict()\n",
    "    #     for i in range(n_classes):\n",
    "    #         fpr_val[i], tpr_val[i], _ = roc_curve(x_test_val[:, i], result_val[:, i])\n",
    "    #         roc_auc_val[i] = auc(fpr_val[i], tpr_val[i])\n",
    "\n",
    "    #     print roc_auc_val\n",
    "    #     val_scores_journals_T0.append(roc_auc_val[0])\n",
    "    #     val_scores_journals_T1T2.append(roc_auc_val[1])\n",
    "    #     val_scores_journals_T3T4.append(roc_auc_val[2])\n",
    "\n",
    "        #----------------------------------------------------------------------------------------------------------------\n",
    "        #Testing makeup of training and testing between journals and article dataframes\n",
    "\n",
    "    #     print \"\"\n",
    "    #     print 'Makeup of training and test sets'\n",
    "    #     print len(train1.loc[train1['dataset'] == 'journal'])/ float(len(train1))\n",
    "    #     print len(test1.loc[test1['dataset'] == 'journal']) / float(len(test1))\n",
    "        print \"------------------------------------------------------------------\"\n",
    "        print \"\"\n",
    "    j = j + 1\n",
    "    \n",
    "        \n",
    "print \"Average scores on test set: \"\n",
    "print ('T0: ', sum(avgT0) / float(len(avgT0)))\n",
    "print ('T1/T2: ', sum(avgT1T2) / float(len(avgT1T2)))\n",
    "print ('T3/T4: ', sum(avgT3T4) / float(len(avgT3T4)))\n",
    "\n",
    "print \"Average scores on article validation set: \"\n",
    "print ('T0: ', sum(val_scores_article_T0) / float(len(val_scores_article_T0)))\n",
    "print ('T1/T2: ', sum(val_scores_article_T1T2) / float(len(val_scores_article_T1T2)))\n",
    "print ('T3/T4: ', sum(val_scores_article_T3T4) / float(len(val_scores_article_T3T4)))\n",
    "\n",
    "# print \"Average scores on journals validation set: \"\n",
    "# print ('T0: ', sum(val_scores_journals_T0) / float(len(val_scores_journals_T0)))\n",
    "# print ('T1/T2: ', sum(val_scores_journals_T1T2) / float(len(val_scores_journals_T1T2)))\n",
    "# print ('T3/T4: ', sum(val_scores_journals_T3T4) / float(len(val_scores_journals_T3T4)))\n",
    "\n",
    "b = datetime.datetime.now().replace(microsecond=0)\n",
    "print(b-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "class KaggleWord2VecUtility(object):\n",
    "    \"\"\"KaggleWord2VecUtility is a utility class for processing raw HTML text into segments for further learning\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def review_to_wordlist( review, remove_stopwords=False ):\n",
    "        # Function to convert a document to a sequence of words,\n",
    "        # optionally removing stop words.  Returns a list of words.\n",
    "        #\n",
    "        # 1. Remove HTML\n",
    "        review_text = BeautifulSoup(review).get_text()\n",
    "        #\n",
    "        # 2. Remove non-letters\n",
    "        review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "        #\n",
    "        # 3. Convert words to lower case and split them\n",
    "        words = review_text.lower().split()\n",
    "        #\n",
    "        # 4. Optionally remove stop words (false by default)\n",
    "        if remove_stopwords:\n",
    "            stops = set(stopwords.words(\"english\"))\n",
    "            words = [w for w in words if not w in stops]\n",
    "        #\n",
    "        # 5. Return a list of words\n",
    "        return(words)\n",
    "    \n",
    "    @staticmethod\n",
    "    def review_to_words( review, remove_stopwords=False, stemmer=False ):\n",
    "        # Function to convert a raw review to a string of words\n",
    "        # The input is a single string (a raw movie review), and \n",
    "        # the output is a single string (a preprocessed movie review)\n",
    "        #\n",
    "        # 1. Remove HTML\n",
    "        review_text = BeautifulSoup(review).get_text() \n",
    "        #\n",
    "        # 2. Remove non-letters        \n",
    "        review_text = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "        #\n",
    "        # 3. Convert to lower case, split into individual words\n",
    "        words = review_text.lower().split()                             \n",
    "        #\n",
    "        # 4. Optionally remove stop words (false by default)\n",
    "        if remove_stopwords:\n",
    "            stops = set(stopwords.words(\"english\"))\n",
    "            words = [w for w in words if not w in stops]   \n",
    "            \n",
    "        if stemmer:\n",
    "            words = [PorterStemmer().stem_word(w) for w in words]\n",
    "        #\n",
    "        # 6. Join the words back into one string separated by space, \n",
    "        # and return the result.\n",
    "        return( \" \".join( words ))   \n",
    "\n",
    "    # Define a function to split a review into parsed sentences\n",
    "    @staticmethod\n",
    "    def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "        # Function to split a review into parsed sentences. Returns a\n",
    "        # list of sentences, where each sentence is a list of words\n",
    "        #\n",
    "        # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "        raw_sentences = tokenizer.tokenize(review.decode('utf8').strip())\n",
    "        #\n",
    "        # 2. Loop over each sentence\n",
    "        sentences = []\n",
    "        for raw_sentence in raw_sentences:\n",
    "            # If a sentence is empty, skip it\n",
    "            if len(raw_sentence) > 0:\n",
    "                # Otherwise, call review_to_wordlist to get a list of words\n",
    "                sentences.append( KaggleWord2VecUtility.review_to_wordlist( raw_sentence, \\\n",
    "                  remove_stopwords ))\n",
    "        #\n",
    "        # Return the list of sentences (each sentence is a list of words,\n",
    "        # so this returns a list of lists\n",
    "        return sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
