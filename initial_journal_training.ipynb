{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get subset of MUSC records where journals equal those two chosen.\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.etree.ElementTree import Element\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir('/home/sarahwie/Documents/pubmed-nlp-research/output/')\n",
    "#os.chdir('/mnt/mypartition/Desktop2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#***WOULD NEED to bring this file over to the VM\n",
    "import sys\n",
    "sys.path.append('/home/sarahwie/Documents/pubmed-nlp-research/DeepLearningMovies_datasets/')\n",
    "from KaggleWord2VecUtility import KaggleWord2VecUtility\n",
    "DATADIR='/home/sarahwie/Documents/pubmed-nlp-research/DeepLearningMovies_datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting subset based on 2 journals (with abstracts and no Unicode encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree = ET.parse('/mnt/mypartition/Desktop2/testXMLwrite.xml')\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#using this file for our example\n",
    "xml_file = ET.parse('/home/sarahwie/Documents/zip/subset/zip/medline16n0189.xml')\n",
    "root = xml_file.getroot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code subsets the records down to those with the titles we are looking for (as well as those with abstracts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Can delete I think for real run\n",
    "#make new XML file\n",
    "rootNew = Element('MedlineCitationSet')\n",
    "children = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n",
      "363\n"
     ]
    }
   ],
   "source": [
    "#can delete I think for real run\n",
    "#replace titles here with those we're using\n",
    "titles = ['The Journal of biological chemistry', 'Journal of bacteriology']\n",
    "\n",
    "i = 0\n",
    "for record in root.findall('MedlineCitation'):\n",
    "    title = record.find('Article').find('Journal').find('Title').text\n",
    "    \n",
    "    if title in titles:\n",
    "        #for test run, also check if abstract\n",
    "        try:\n",
    "            abstract = record.find('Article').find('Abstract').find('AbstractText').text\n",
    "            if type(abstract) != str:\n",
    "                print i\n",
    "            i = i + 1\n",
    "            #append to subset file\n",
    "            children.append(record)\n",
    "        except AttributeError:\n",
    "            #dont include if no abstract\n",
    "            #something random to hold place of indent:\n",
    "            x = 0\n",
    "\n",
    "rootNew.extend(children)\n",
    "\n",
    "#create tree and write to file\n",
    "tree = ET.ElementTree(rootNew)\n",
    "tree.write(\"subsetTwoJournals.xml\")\n",
    "        \n",
    "pickle.dump(rootNew, open('rootTwoJournals.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarahwie/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:3: DeprecationWarning: This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "#Can delete I think for real run\n",
    "#size of new subset\n",
    "print len(rootNew.getchildren())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting training and testing sets as pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 journal  \\\n",
      "363  The Journal of biological chemistry   \n",
      "\n",
      "                                              abstract  \n",
      "363  Metallothionein was purified under anaerobic c...  \n"
     ]
    }
   ],
   "source": [
    "#convert XML subset to pandas dataframe\n",
    "def iter_docs(author):\n",
    "    author_attr = author.attrib\n",
    "    for record in author.findall('MedlineCitation'):\n",
    "        doc_dict = author_attr.copy()\n",
    "        #doc_dict.update(record.attrib)\n",
    "        doc_dict['journal'] = record.find('Article').find('Journal').find('Title').text\n",
    "        abstract = record.find('Article').find('Abstract').find('AbstractText').text\n",
    "        #make sure all abstracts are in string format to avoid later sentence parsing issues (two in test run were unicode)\n",
    "        if type(abstract) != str:\n",
    "            abstract = abstract.encode('utf8')\n",
    "        doc_dict['abstract'] = abstract\n",
    "        yield doc_dict\n",
    "        \n",
    "df = pd.DataFrame(list(iter_docs(rootNew)))\n",
    "df = df[['journal', 'abstract']]\n",
    "print df.iloc[363:364,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Journal of bacteriology\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>journal</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Journal of bacteriology</td>\n",
       "      <td>ompB mutants of Escherichia coli K-12 are mark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Journal of bacteriology</td>\n",
       "      <td>Several lysosomal glycosidase activities were ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Journal of bacteriology</td>\n",
       "      <td>The initiation of patulin biosynthesis in subm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   journal                                           abstract\n",
       "0  Journal of bacteriology  ompB mutants of Escherichia coli K-12 are mark...\n",
       "1  Journal of bacteriology  Several lysosomal glycosidase activities were ...\n",
       "2  Journal of bacteriology  The initiation of patulin biosynthesis in subm..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print df.loc[1][0]\n",
    "#don't know why no quotation marks like when using pandas.to_csv in original w2v inversion\n",
    "#but seems to work fine so ignore for now\n",
    "df.loc[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#split into train and test\n",
    "fracTrain = 0.75\n",
    "nSamples = df.shape[0]\n",
    "order = np.random.permutation(nSamples) # come up with a random ordering\n",
    "splitIndex = int(np.round(nSamples*fracTrain))\n",
    "train1 = df.ix[order[:splitIndex],:]\n",
    "test1 = df.ix[order[splitIndex:],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(316, 2)\n",
      "(106, 2)\n"
     ]
    }
   ],
   "source": [
    "print train1.shape\n",
    "print test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 journal  \\\n",
      "371  The Journal of biological chemistry   \n",
      "\n",
      "                                              abstract  \n",
      "371  An icosadeoxyribonucleotide containing the sev...  \n"
     ]
    }
   ],
   "source": [
    "#print first record in test set (note has different index value, indexed instead by location (pos. 0))\n",
    "print test1.iloc[[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Parsing Sentences & Building Word2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"Most strains of Escherichia coli K-12 are unable to use the enzyme IIA/IIB (enzyme IIMan) complex of the phosphoenolpyruvate:sugar phosphotransferase system (PTS) in anaerobic growth and therefore cannot utilize glucosamine anaerobically. Introduction into these strains of a ptsG mutation, which eliminates activity of the enzyme IIIGlc/IIB' complex of the PTS, resulted in inability to grow anaerobically on glucose and mannose. Derivative strains able to grow anaerobically on glucosamine had mutations at a locus close to man, the gene coding for phosphomannose isomerase, and had higher enzyme IIA/IIB activities during anaerobic growth than did the parental strain. These results establish a locus affecting function of enzyme IIA/IIB that maps distant from ptsM, the probable structural gene for enzyme IIB.\""
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = train1[\"abstract\"].iloc[0]\n",
    "review = review.encode('utf8')\n",
    "review.decode('utf8')\n",
    "#KaggleWord2VecUtility.review_to_sentences(review, tokenizer)\n",
    "\n",
    "#flatten list\n",
    "#[item for sublist in lst for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gets a list of positive review indexes and negative review indexes. Takes those respective reviews and parses them into sentences and appends to a list of positive and negative sentences, respectively. Also does the same for the unlabelled training sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    }
   ],
   "source": [
    "# ****** Split the training set into clean sentences\n",
    "#\n",
    "sentences_pos = []  # Initialize an empty list of sentences\n",
    "sentences_neg = []  # Initialize an empty list of sentences\n",
    "\n",
    "#here change to include all journal name labels of positive and negative\n",
    "inxs_pos = np.where(train1[\"journal\"] == 'The Journal of biological chemistry')[0].tolist()\n",
    "inxs_neg = np.where(train1[\"journal\"] == 'Journal of bacteriology')[0].tolist()\n",
    "\n",
    "print \"Parsing sentences from training set\"\n",
    "for inx in inxs_pos:\n",
    "    review = train1[\"abstract\"].iloc[inx]\n",
    "    sentences_pos += KaggleWord2VecUtility.review_to_sentences(review, tokenizer)\n",
    "\n",
    "for inx in inxs_neg:\n",
    "    review = train1[\"abstract\"].iloc[inx]\n",
    "    sentences_neg += KaggleWord2VecUtility.review_to_sentences(review, tokenizer)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'proteolysis', u'of', u'nereis', u'cuticle', u'collagen', u'by', u'two', u'bacterial', u'collagenases', u'was', u'investigated', u'using', u'viscosimetry', u'enzyme', u'kinetics', u'sodium', u'dodecyl', u'sulfate', u'polyacrylamide', u'gel', u'electrophoresis', u'and', u'ion', u'exchange', u'chromatography', u'of', u'collagenolytic', u'peptides'], [u'collagenase', u'of', u'the', u'marine', u'vibrio', u'b', u'completely', u'degrades', u'native', u'cuticle', u'collagen', u'at', u'degress', u'c', u'with', u'a', u'turnover', u'number', u'times', u'greater', u'than', u'that', u'of', u'the', u'clostridial', u'collagenase']]\n"
     ]
    }
   ],
   "source": [
    "print sentences_pos[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same with all sentences to build the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    }
   ],
   "source": [
    "# ****** Split the labeled and unlabeled training sets into clean sentences\n",
    "#\n",
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print \"Parsing sentences from training set\"\n",
    "for review in train1[\"abstract\"]:\n",
    "    sentences += KaggleWord2VecUtility.review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Build Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=0, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "\n",
    "## create a w2v learner \n",
    "basemodel = Word2Vec(\n",
    "    workers=multiprocessing.cpu_count(), # use your cores\n",
    "    iter=3, # iter = sweeps of SGD through the data; more is better\n",
    "    hs=1, negative=0 # we only have scoring for the hierarchical softmax setup\n",
    "    )\n",
    "print basemodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basemodel.build_vocab(sentences) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Word2Vec model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53286"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model 0 corresponds with the Journal of bacteriology\n",
    "#and model 1 the Journal of Biological Chemistry\n",
    "\n",
    "from copy import deepcopy\n",
    "models = [deepcopy(basemodel) for i in range(2)]\n",
    "models[0].train(sentences_neg, total_examples=len(sentences_neg) )\n",
    "models[1].train(sentences_pos, total_examples=len(sentences_pos) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Applying inversion on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "docprob takes two lists\n",
    "* docs: a list of documents, each of which is a list of sentences\n",
    "* models: the candidate word2vec models (each potential class)\n",
    "\n",
    "it returns the array of class probabilities.  Everything is done in-memory.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd # for quick summing within doc\n",
    "\n",
    "def docprob(docs, mods):\n",
    "    # score() takes a list [s] of sentences here; could also be a sentence generator\n",
    "    sentlist = [s for d in docs for s in d]\n",
    "    # the log likelihood of each sentence in this review under each w2v representation\n",
    "    llhd = np.array( [ m.score(sentlist, len(sentlist)) for m in mods ] )\n",
    "    # now exponentiate to get likelihoods, \n",
    "    lhd = np.exp(llhd - llhd.max(axis=0)) # subtract row max to avoid numeric overload\n",
    "    # normalize across models (stars) to get sentence-star probabilities\n",
    "    #all this transposing business does is make it so the total probability of a word \n",
    "    #   equals 1 between the 2 arrays (positive prob and negative).\n",
    "    #and the pandas data frame just puts everything into rows/columns format for easy viz\n",
    "    prob = pd.DataFrame( (lhd/lhd.sum(axis=0)).transpose() )\n",
    "    # and finally average the sentence probabilities to get the review probability\n",
    "    prob[\"doc\"] = [i for i,d in enumerate(docs) for s in d]\n",
    "    prob = prob.groupby(\"doc\").mean()\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in the test set as a list of a list of words\n",
    "docs = []\n",
    "for review in test1[\"abstract\"]:\n",
    "    docs.append(KaggleWord2VecUtility.review_to_sentences(review, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'an', u'icosadeoxyribonucleotide', u'containing', u'the', u'several', u'features', u'found', u'in', u'prokaryotic', u'mrna', u'ribosome', u'binding', u'sites', u'has', u'been', u'synthesized'], [u'this', u'sequence', u'can', u'stimulate', u'the', u'binding', u'of', u'initiator', u'fmet', u'trnaf', u'to', u'the', u'ribosome', u'to', u'form', u'a', u'stable', u's', u'initiation', u'complex', u'identical', u'with', u'those', u'induced', u'by', u'natural', u'messengers'], [u'the', u'binding', u'of', u'this', u'synthetic', u'ribosome', u'binding', u'site', u'is', u'absolutely', u'dependent', u'upon', u'initiation', u'factor', u'if', u'and', u'the', u'bound', u'fmet', u'trnaf', u'is', u'sensitive', u'to', u'puromycin', u'indicating', u'the', u'formation', u'of', u'a', u'functional', u'initiation', u'complex'], [u'a', u'heptadecadeoxyribonucleotide', u'identical', u'with', u'the', u'icosanucleotide', u'but', u'lacking', u'the', u'terminal', u'a', u't', u'g', u'codon', u'can', u'also', u'stimulate', u'the', u'stable', u'binding', u'of', u'fmet', u'trnaf', u'to', u'the', u'ribosome', u'suggesting', u'that', u'the', u'selection', u'of', u'the', u'proper', u'a', u'u', u'g', u'initiation', u'codon', u'by', u'fmet', u'trnaf', u'is', u'subsequent', u'to', u'and', u'a', u'result', u'of', u'the', u'recognition', u'and', u'binding', u'of', u'the', u'fmet', u'trnaf'], [u's', u'ribosome', u'complex', u'to', u'the', u'initiation', u'site'], [u'the', u'prospect', u'of', u'ligating', u'a', u'similar', u'synthetic', u'ribosome', u'binding', u'site', u'in', u'front', u'of', u'a', u'eukaryotic', u'gene', u'for', u'cloning', u'in', u'an', u'appropriate', u'prokaryotic', u'vector', u'to', u'assure', u'the', u'expresion', u'of', u'the', u'protein', u'is', u'discussed']]\n"
     ]
    }
   ],
   "source": [
    "print docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the probs (note we give docprob our test set plus the models)\n",
    "probs = docprob(docs,models).astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.847698</td>\n",
       "      <td>0.152302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.971511</td>\n",
       "      <td>0.0284893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.800854</td>\n",
       "      <td>0.199146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.748073</td>\n",
       "      <td>0.251927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.764333</td>\n",
       "      <td>0.235667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.856744</td>\n",
       "      <td>0.143256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.933766</td>\n",
       "      <td>0.066234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.768516</td>\n",
       "      <td>0.231484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.992793</td>\n",
       "      <td>0.00720701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.958979</td>\n",
       "      <td>0.0410205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.888178</td>\n",
       "      <td>0.111822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.902187</td>\n",
       "      <td>0.0978125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.930588</td>\n",
       "      <td>0.0694121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.997664</td>\n",
       "      <td>0.00233592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.69541</td>\n",
       "      <td>0.30459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.885092</td>\n",
       "      <td>0.114908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.800709</td>\n",
       "      <td>0.199291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.999546</td>\n",
       "      <td>0.000454305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.582073</td>\n",
       "      <td>0.417927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.876979</td>\n",
       "      <td>0.123021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.807148</td>\n",
       "      <td>0.192852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.705346</td>\n",
       "      <td>0.294654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.966768</td>\n",
       "      <td>0.0332321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.710448</td>\n",
       "      <td>0.289552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.967701</td>\n",
       "      <td>0.0322994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.812192</td>\n",
       "      <td>0.187808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.795694</td>\n",
       "      <td>0.204306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.780914</td>\n",
       "      <td>0.219086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.804756</td>\n",
       "      <td>0.195244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.98667</td>\n",
       "      <td>0.0133298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.832577</td>\n",
       "      <td>0.167423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.770767</td>\n",
       "      <td>0.229233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.661874</td>\n",
       "      <td>0.338126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.699908</td>\n",
       "      <td>0.300092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.90755</td>\n",
       "      <td>0.0924503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.8693</td>\n",
       "      <td>0.1307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.853871</td>\n",
       "      <td>0.146129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.816041</td>\n",
       "      <td>0.183959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.728203</td>\n",
       "      <td>0.271797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.875299</td>\n",
       "      <td>0.124701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.578923</td>\n",
       "      <td>0.421077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.975368</td>\n",
       "      <td>0.024632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.831001</td>\n",
       "      <td>0.168999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.868035</td>\n",
       "      <td>0.131965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.571725</td>\n",
       "      <td>0.428275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.912911</td>\n",
       "      <td>0.087089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.985242</td>\n",
       "      <td>0.0147584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.975314</td>\n",
       "      <td>0.0246861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.964457</td>\n",
       "      <td>0.035543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.987197</td>\n",
       "      <td>0.0128032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.943334</td>\n",
       "      <td>0.0566658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.992001</td>\n",
       "      <td>0.00799907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.754448</td>\n",
       "      <td>0.245552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.829532</td>\n",
       "      <td>0.170468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.838734</td>\n",
       "      <td>0.161266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.861794</td>\n",
       "      <td>0.138206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.785438</td>\n",
       "      <td>0.214562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.999318</td>\n",
       "      <td>0.000682138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.740507</td>\n",
       "      <td>0.259493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.909087</td>\n",
       "      <td>0.0909128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0            1\n",
       "doc                       \n",
       "0    0.847698     0.152302\n",
       "1    0.971511    0.0284893\n",
       "2    0.800854     0.199146\n",
       "3    0.748073     0.251927\n",
       "4    0.764333     0.235667\n",
       "5    0.856744     0.143256\n",
       "6    0.933766     0.066234\n",
       "7    0.768516     0.231484\n",
       "8    0.992793   0.00720701\n",
       "9    0.958979    0.0410205\n",
       "10   0.888178     0.111822\n",
       "11   0.902187    0.0978125\n",
       "12   0.930588    0.0694121\n",
       "13   0.997664   0.00233592\n",
       "14    0.69541      0.30459\n",
       "15   0.885092     0.114908\n",
       "16   0.800709     0.199291\n",
       "17   0.999546  0.000454305\n",
       "18   0.582073     0.417927\n",
       "19   0.876979     0.123021\n",
       "20   0.807148     0.192852\n",
       "21   0.705346     0.294654\n",
       "22   0.966768    0.0332321\n",
       "23   0.710448     0.289552\n",
       "24   0.967701    0.0322994\n",
       "25   0.812192     0.187808\n",
       "26   0.795694     0.204306\n",
       "27   0.780914     0.219086\n",
       "28   0.804756     0.195244\n",
       "29    0.98667    0.0133298\n",
       "..        ...          ...\n",
       "76   0.832577     0.167423\n",
       "77   0.770767     0.229233\n",
       "78   0.661874     0.338126\n",
       "79   0.699908     0.300092\n",
       "80    0.90755    0.0924503\n",
       "81     0.8693       0.1307\n",
       "82   0.853871     0.146129\n",
       "83   0.816041     0.183959\n",
       "84   0.728203     0.271797\n",
       "85   0.875299     0.124701\n",
       "86   0.578923     0.421077\n",
       "87   0.975368     0.024632\n",
       "88   0.831001     0.168999\n",
       "89   0.868035     0.131965\n",
       "90   0.571725     0.428275\n",
       "91   0.912911     0.087089\n",
       "92   0.985242    0.0147584\n",
       "93   0.975314    0.0246861\n",
       "94   0.964457     0.035543\n",
       "95   0.987197    0.0128032\n",
       "96   0.943334    0.0566658\n",
       "97   0.992001   0.00799907\n",
       "98   0.754448     0.245552\n",
       "99   0.829532     0.170468\n",
       "100  0.838734     0.161266\n",
       "101  0.861794     0.138206\n",
       "102  0.785438     0.214562\n",
       "103  0.999318  0.000682138\n",
       "104  0.740507     0.259493\n",
       "105  0.909087    0.0909128\n",
       "\n",
       "[106 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = np.ones((probs.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106,)\n"
     ]
    }
   ],
   "source": [
    "print np.shape(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions[np.where(probs.iloc[:,0] > 0.5)] = 0 # The first column is the negative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print predictions.shape\n",
    "print test1[\"sentiment\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print np.size(np.where(predictions == test1[\"sentiment\"]))*1./np.size(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
