{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from xml.etree.ElementTree import Element\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk.data\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "from copy import deepcopy\n",
    "import datetime\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#on VM\n",
    "os.chdir('/mnt/mypartition/Desktop2/pubmed_nlp_research/DeepLearningMovies_datasets/')\n",
    "import KaggleWord2VecUtility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#on local:\n",
    "import sys\n",
    "sys.path.append('/home/sarahwie/Documents/pubmed-nlp-research/DeepLearningMovies_datasets/')\n",
    "from KaggleWord2VecUtility import KaggleWord2VecUtility\n",
    "DATADIR='/home/sarahwie/Documents/pubmed-nlp-research/DeepLearningMovies_datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking that XML input has correct journals and in correct format (with abstracts and no Unicode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ignore for now\n",
    "# def combine_xml(files):\n",
    "#     first = None\n",
    "#     for filename in files:\n",
    "#         data = ET.parse(filename).getroot()\n",
    "#         if first is None:\n",
    "#             first = data\n",
    "#         else:\n",
    "#             first.extend(data)\n",
    "#     if first is not None:\n",
    "#         return first\n",
    "\n",
    "# a = datetime.datetime.now().replace(microsecond=0)\n",
    "  \n",
    "# #replace these URLs!\n",
    "# #rootComb = combine_xml(['/home/sarahwie/Documents/pubmed_result_clinical.xml','/home/sarahwie/Documents/pubmed_result_basic_science.xml'])\n",
    "# rootComb = combine_xml(['/mnt/mypartition/pubmed_result_clinical.xml','/mnt/mypartition/pubmed_result_basic_science.xml'])\n",
    "\n",
    "# b = datetime.datetime.now().replace(microsecond=0)\n",
    "# print(b-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print len(rootComb.getchildren())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code subsets the records down to those with abstracts and checks that correct journal names have been queried."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uses too much memory- don't use now that we know all records have correct title and abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make new XML file\n",
    "rootNew = Element('MedlineCitationSet')\n",
    "children = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titles = ['Journal of general internal medicine', 'Annals of emergency medicine', 'Pediatrics', \n",
    "                  'The Journal of pediatrics', 'Journal of clinical hypertension (Greenwich, Conn.)', 'Annals of medicine', \n",
    "                  'Academic emergency medicine : official journal of the Society for Academic Emergency Medicine', \n",
    "                  'Annals of internal medicine','The Journal of biological chemistry','Nucleic acids research', 'Cell', \n",
    "                      'Methods in molecular biology (Clifton, N.J.)', 'Nature cell biology', \n",
    "                      'Progress in biophysics and molecular biology', 'Molecular and cellular biology']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/mypartition/pubmed_result_clinical.xml\n",
      "/mnt/mypartition/pubmed_result_basic_science.xml\n",
      "44717\n",
      "0\n",
      "0:02:27\n"
     ]
    }
   ],
   "source": [
    "# a = datetime.datetime.now().replace(microsecond=0)\n",
    "\n",
    "# #replace these URLs!\n",
    "# filenames = ['/mnt/mypartition/pubmed_result_clinical.xml','/mnt/mypartition/pubmed_result_basic_science.xml']   \n",
    "\n",
    "# i = 0\n",
    "# #read in files\n",
    "# j = 0\n",
    "\n",
    "# xml_file = ET.parse(file)\n",
    "# root = xml_file.getroot()\n",
    "# for record in root.findall('PubmedArticle'):\n",
    "#     title = record.find('MedlineCitation').find('Article').find('Journal').find('Title').text\n",
    "#     #check that all title names match list\n",
    "#     if title in titles:\n",
    "#         try:\n",
    "#             abstract = record.find('MedlineCitation').find('Article').find('Abstract').find('AbstractText').text\n",
    "#             #append to subset file\n",
    "#             children.append(record)\n",
    "#             i = i + 1\n",
    "#         except AttributeError:\n",
    "#             #dont include if no abstract\n",
    "#             print 'no abstract!'\n",
    "\n",
    "#     else: \n",
    "#         print title\n",
    "#         j = j+1\n",
    "\n",
    "# #print num records obtained (should be 44717)\n",
    "# print i\n",
    "# #print num. records with wrong titles\n",
    "# print j\n",
    "# b = datetime.datetime.now().replace(microsecond=0)\n",
    "# print(b-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a = datetime.datetime.now().replace(microsecond=0)\n",
    "\n",
    "# rootNew.extend(children)\n",
    "\n",
    "# #create tree and write to file\n",
    "# #tree = ET.ElementTree(rootNew)\n",
    "        \n",
    "# pickle.dump(rootNew, open('/mnt/mypartition/Desktop2/rootJournalsSubset.p', 'wb'))\n",
    "\n",
    "# b = datetime.datetime.now().replace(microsecond=0)\n",
    "# print(b-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#size of new subset- should be 44717\n",
    "print len(rootNew.getchildren())\n",
    "#verify that this is the correct number: yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *New plan: run the below separately for both files and then append the pandas dataframes in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert XML to Pandas dataframes (include title, abstract, mesh terms, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titlesClinical = ['Journal of general internal medicine', 'Annals of emergency medicine', 'Pediatrics', \n",
    "                  'The Journal of pediatrics', 'Journal of clinical hypertension (Greenwich, Conn.)', 'Annals of medicine', \n",
    "                  'Academic emergency medicine : official journal of the Society for Academic Emergency Medicine', \n",
    "                  'Annals of internal medicine']\n",
    "\n",
    "titlesBasicScience = ['The Journal of biological chemistry','Nucleic acids research', 'Cell', \n",
    "                      'Methods in molecular biology (Clifton, N.J.)', 'Nature cell biology', \n",
    "                      'Progress in biophysics and molecular biology', 'Molecular and cellular biology']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def iter_docs(author):\n",
    "    author_attr = author.attrib\n",
    "    for record in author.findall('PubmedArticle'):\n",
    "        doc_dict = author_attr.copy()\n",
    "\n",
    "        title = record.find('MedlineCitation').find('Article').find('ArticleTitle').text\n",
    "        doc_dict['title'] = title\n",
    "        journal = record.find('MedlineCitation').find('Article').find('Journal').find('Title').text\n",
    "        doc_dict['journal'] = journal\n",
    "        \n",
    "        abstract = ''\n",
    "        for abst in record.find('MedlineCitation').find('Article').find('Abstract').findall('AbstractText'):\n",
    "            if abst.text != None:\n",
    "                if type(abst.text) != str:\n",
    "                    abst.text = abst.text.encode('utf8')\n",
    "                abstract = abstract + abst.text\n",
    "        doc_dict['abstract'] = abstract\n",
    "        \n",
    "        #get MeSH terms as strings to simplify later processing\n",
    "        meshDescriptor = ''\n",
    "        meshQualifier = ''\n",
    "        if record.find('MedlineCitation').find('MeshHeadingList') is not None:\n",
    "            for mesh in record.find('MedlineCitation').find('MeshHeadingList').findall('MeshHeading'):\n",
    "                if mesh.find('DescriptorName') is not None:\n",
    "                    for desc in mesh.findall('DescriptorName'):\n",
    "                        if meshDescriptor == '':\n",
    "                            meshDescriptor = desc.text\n",
    "                        else:\n",
    "                            meshDescriptor = meshDescriptor + ' ' + desc.text\n",
    "                if mesh.find('QualifierName') is not None:\n",
    "                    for qual in mesh.findall('QualifierName'):\n",
    "                        if meshQualifier == '':\n",
    "                            meshQualifier = qual.text\n",
    "                        else:\n",
    "                            meshQualifier = meshQualifier + ' ' + qual.text\n",
    "        doc_dict['qualifier_terms'] = meshQualifier\n",
    "        doc_dict['descriptor_terms'] = meshDescriptor\n",
    "        if journal in titlesClinical:\n",
    "            doc_dict['sentiment'] = 0\n",
    "        elif journal in titlesBasicScience:\n",
    "            doc_dict['sentiment'] = 1\n",
    "        else:\n",
    "            print 'title not in lists'\n",
    "            print journal\n",
    "        yield doc_dict\n",
    "        \n",
    "#df = pd.DataFrame(list(iter_docs(rootNew)))\n",
    "#df = df[['sentiment', 'journal', 'title', 'abstract', 'qualifier_terms', 'descriptor_terms']]\n",
    "#print df\n",
    "\n",
    "#shuffle rows- necessary??\n",
    "#df = df.iloc[np.random.permutation(len(df))]\n",
    "#df = df.reset_index(drop=True)\n",
    "#print df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/mypartition/pubmed_result_clinical.xml\n",
      "Index([u'sentiment', u'journal', u'title', u'abstract', u'qualifier_terms',\n",
      "       u'descriptor_terms'],\n",
      "      dtype='object')\n",
      "/mnt/mypartition/pubmed_result_basic_science.xml\n",
      "Index([u'sentiment', u'journal', u'title', u'abstract', u'qualifier_terms',\n",
      "       u'descriptor_terms'],\n",
      "      dtype='object')\n",
      "0:02:36\n"
     ]
    }
   ],
   "source": [
    "a = datetime.datetime.now().replace(microsecond=0)\n",
    "\n",
    "filenames = ['/mnt/mypartition/pubmed_result_clinical.xml','/mnt/mypartition/pubmed_result_basic_science.xml']   \n",
    "\n",
    "i = 0\n",
    "for file in filenames:\n",
    "    print file\n",
    "    xml_file = ET.parse(file)\n",
    "    root = xml_file.getroot()\n",
    "    if i == 0:\n",
    "        df1 = pd.DataFrame(list(iter_docs(root)))\n",
    "        df1 = df1[['sentiment', 'journal', 'title', 'abstract', 'qualifier_terms', 'descriptor_terms']]\n",
    "    elif i == 1:\n",
    "        df2 = pd.DataFrame(list(iter_docs(root)))\n",
    "        df2 = df2[['sentiment', 'journal', 'title', 'abstract', 'qualifier_terms', 'descriptor_terms']]\n",
    "    i = i + 1\n",
    "    \n",
    "b = datetime.datetime.now().replace(microsecond=0)\n",
    "print(b-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19826, 6)\n",
      "(24891, 6)\n"
     ]
    }
   ],
   "source": [
    "print df1.shape\n",
    "print df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>journal</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>qualifier_terms</th>\n",
       "      <th>descriptor_terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Academic emergency medicine : official journal...</td>\n",
       "      <td>Accuracy of Point-of-Care Lung Ultrasonography...</td>\n",
       "      <td>To determine the test performance characterist...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Academic emergency medicine : official journal...</td>\n",
       "      <td>Patient and process factors associated with ty...</td>\n",
       "      <td>In-hospital factors contribute more to delayed...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Academic emergency medicine : official journal...</td>\n",
       "      <td>Comparative Trends and Downstream Outcomes of ...</td>\n",
       "      <td>Coronary computerized tomography angiography (...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Academic emergency medicine : official journal...</td>\n",
       "      <td>The Man in Hospital Room 1233.</td>\n",
       "      <td>As I got off the elevator approaching my uncle...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Academic emergency medicine : official journal...</td>\n",
       "      <td>Extravasation Risk Using Ultrasound Guided Per...</td>\n",
       "      <td>Ultrasound guided intravenous catheter (USGIV)...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            journal  \\\n",
       "0          0  Academic emergency medicine : official journal...   \n",
       "1          0  Academic emergency medicine : official journal...   \n",
       "2          0  Academic emergency medicine : official journal...   \n",
       "3          0  Academic emergency medicine : official journal...   \n",
       "4          0  Academic emergency medicine : official journal...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Accuracy of Point-of-Care Lung Ultrasonography...   \n",
       "1  Patient and process factors associated with ty...   \n",
       "2  Comparative Trends and Downstream Outcomes of ...   \n",
       "3                     The Man in Hospital Room 1233.   \n",
       "4  Extravasation Risk Using Ultrasound Guided Per...   \n",
       "\n",
       "                                            abstract qualifier_terms  \\\n",
       "0  To determine the test performance characterist...                   \n",
       "1  In-hospital factors contribute more to delayed...                   \n",
       "2  Coronary computerized tomography angiography (...                   \n",
       "3  As I got off the elevator approaching my uncle...                   \n",
       "4  Ultrasound guided intravenous catheter (USGIV)...                   \n",
       "\n",
       "  descriptor_terms  \n",
       "0                   \n",
       "1                   \n",
       "2                   \n",
       "3                   \n",
       "4                   "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>journal</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>qualifier_terms</th>\n",
       "      <th>descriptor_terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Cell</td>\n",
       "      <td>SnapShot: Mechanical Forces in Development II.</td>\n",
       "      <td>The forces shaping an organism are not exclusi...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Cell</td>\n",
       "      <td>Real-Time Imaging of Translation on Single mRN...</td>\n",
       "      <td>Translation is under tight spatial and tempora...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Cell</td>\n",
       "      <td>Dynamics of Translation of Single mRNA Molecul...</td>\n",
       "      <td>Regulation of mRNA translation, the process by...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Cell</td>\n",
       "      <td>Lung Adenocarcinoma Distally Rewires Hepatic C...</td>\n",
       "      <td>The circadian clock controls metabolic and phy...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Cell</td>\n",
       "      <td>Temporal Stability of the Human Skin Microbiome.</td>\n",
       "      <td>Biogeography and individuality shape the struc...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment journal                                              title  \\\n",
       "0          1    Cell     SnapShot: Mechanical Forces in Development II.   \n",
       "1          1    Cell  Real-Time Imaging of Translation on Single mRN...   \n",
       "2          1    Cell  Dynamics of Translation of Single mRNA Molecul...   \n",
       "3          1    Cell  Lung Adenocarcinoma Distally Rewires Hepatic C...   \n",
       "4          1    Cell   Temporal Stability of the Human Skin Microbiome.   \n",
       "\n",
       "                                            abstract qualifier_terms  \\\n",
       "0  The forces shaping an organism are not exclusi...                   \n",
       "1  Translation is under tight spatial and tempora...                   \n",
       "2  Regulation of mRNA translation, the process by...                   \n",
       "3  The circadian clock controls metabolic and phy...                   \n",
       "4  Biogeography and individuality shape the struc...                   \n",
       "\n",
       "  descriptor_terms  \n",
       "0                   \n",
       "1                   \n",
       "2                   \n",
       "3                   \n",
       "4                   "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00\n"
     ]
    }
   ],
   "source": [
    "frames = [df1, df2]\n",
    "dfJournalsSubset = pd.concat(frames, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44717, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>journal</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>qualifier_terms</th>\n",
       "      <th>descriptor_terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19824</th>\n",
       "      <td>0</td>\n",
       "      <td>Pediatrics</td>\n",
       "      <td>Intracranial hemorrhage as the initial manifes...</td>\n",
       "      <td>Intracranial hemorrhage in a term neonate is a...</td>\n",
       "      <td>etiology pathology blood complications diagnos...</td>\n",
       "      <td>Apnea Brain Carbohydrate Metabolism, Inborn Er...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19825</th>\n",
       "      <td>0</td>\n",
       "      <td>Pediatrics</td>\n",
       "      <td>Immunizations for patients with metabolic diso...</td>\n",
       "      <td>Individuals with underlying metabolic disorder...</td>\n",
       "      <td>immunology therapy immunology therapy contrain...</td>\n",
       "      <td>Adolescent Amino Acid Metabolism, Inborn Error...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19826</th>\n",
       "      <td>1</td>\n",
       "      <td>Cell</td>\n",
       "      <td>SnapShot: Mechanical Forces in Development II.</td>\n",
       "      <td>The forces shaping an organism are not exclusi...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19827</th>\n",
       "      <td>1</td>\n",
       "      <td>Cell</td>\n",
       "      <td>Real-Time Imaging of Translation on Single mRN...</td>\n",
       "      <td>Translation is under tight spatial and tempora...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19828</th>\n",
       "      <td>1</td>\n",
       "      <td>Cell</td>\n",
       "      <td>Dynamics of Translation of Single mRNA Molecul...</td>\n",
       "      <td>Regulation of mRNA translation, the process by...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19829</th>\n",
       "      <td>1</td>\n",
       "      <td>Cell</td>\n",
       "      <td>Lung Adenocarcinoma Distally Rewires Hepatic C...</td>\n",
       "      <td>The circadian clock controls metabolic and phy...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment     journal  \\\n",
       "19824          0  Pediatrics   \n",
       "19825          0  Pediatrics   \n",
       "19826          1        Cell   \n",
       "19827          1        Cell   \n",
       "19828          1        Cell   \n",
       "19829          1        Cell   \n",
       "\n",
       "                                                   title  \\\n",
       "19824  Intracranial hemorrhage as the initial manifes...   \n",
       "19825  Immunizations for patients with metabolic diso...   \n",
       "19826     SnapShot: Mechanical Forces in Development II.   \n",
       "19827  Real-Time Imaging of Translation on Single mRN...   \n",
       "19828  Dynamics of Translation of Single mRNA Molecul...   \n",
       "19829  Lung Adenocarcinoma Distally Rewires Hepatic C...   \n",
       "\n",
       "                                                abstract  \\\n",
       "19824  Intracranial hemorrhage in a term neonate is a...   \n",
       "19825  Individuals with underlying metabolic disorder...   \n",
       "19826  The forces shaping an organism are not exclusi...   \n",
       "19827  Translation is under tight spatial and tempora...   \n",
       "19828  Regulation of mRNA translation, the process by...   \n",
       "19829  The circadian clock controls metabolic and phy...   \n",
       "\n",
       "                                         qualifier_terms  \\\n",
       "19824  etiology pathology blood complications diagnos...   \n",
       "19825  immunology therapy immunology therapy contrain...   \n",
       "19826                                                      \n",
       "19827                                                      \n",
       "19828                                                      \n",
       "19829                                                      \n",
       "\n",
       "                                        descriptor_terms  \n",
       "19824  Apnea Brain Carbohydrate Metabolism, Inborn Er...  \n",
       "19825  Adolescent Amino Acid Metabolism, Inborn Error...  \n",
       "19826                                                     \n",
       "19827                                                     \n",
       "19828                                                     \n",
       "19829                                                     "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print dfJournalsSubset.shape\n",
    "dfJournalsSubset[19824:19830]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(dfJournalsSubset, open('/mnt/mypartition/Desktop2/pubmed_nlp_research/pickled_objects/dfJournalsSubset.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Inversion with 5-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44717\n"
     ]
    }
   ],
   "source": [
    "df = pickle.load(open('/mnt/mypartition/Desktop2/pubmed_nlp_research/pickled_objects/dfJournalsSubset.p', 'rb'))\n",
    "print len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "docprob takes two lists\n",
    "* docs: a list of documents, each of which is a list of sentences\n",
    "* models: the candidate word2vec models (each potential class)\n",
    "\n",
    "it returns the array of class probabilities.  Everything is done in-memory.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd # for quick summing within doc\n",
    "\n",
    "def docprob(docs, mods):\n",
    "    # score() takes a list [s] of sentences here; could also be a sentence generator\n",
    "    sentlist = [s for d in docs for s in d]\n",
    "    # the log likelihood of each sentence in this review under each w2v representation\n",
    "    llhd = np.array( [ m.score(sentlist, len(sentlist)) for m in mods ] )\n",
    "    # now exponentiate to get likelihoods, \n",
    "    lhd = np.exp(llhd - llhd.max(axis=0)) # subtract row max to avoid numeric overload\n",
    "    # normalize across models (stars) to get sentence-star probabilities\n",
    "    #all this transposing business does is make it so the total probability of a word \n",
    "    #   equals 1 between the 2 arrays (positive prob and negative).\n",
    "    #and the pandas data frame just puts everything into rows/columns format for easy viz\n",
    "    prob = pd.DataFrame( (lhd/lhd.sum(axis=0)).transpose() )\n",
    "    # and finally average the sentence probabilities to get the review probability\n",
    "    prob[\"doc\"] = [i for i,d in enumerate(docs) for s in d]\n",
    "    prob = prob.groupby(\"doc\").mean()\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ROUND', 1)\n",
      "161\n",
      "176\n",
      "41\n",
      "44\n",
      "Parsing sentences from training set\n",
      "Building and training w2v models\n",
      "Parsing test sentences\n",
      "scoring test set\n",
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "85\n",
      "0.5\n",
      "0:00:02\n"
     ]
    }
   ],
   "source": [
    "a = datetime.datetime.now().replace(microsecond=0)\n",
    "\n",
    "#5-fold stratified cross validation\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "y = df['sentiment'].values\n",
    "skf = StratifiedKFold(y, n_folds=5, shuffle=True)\n",
    "\n",
    "i = 1\n",
    "avg = []\n",
    "for train_index, test_index in skf:\n",
    "    \n",
    "    print(\"ROUND\", i)\n",
    "    i = i + 1\n",
    "    \n",
    "    #use the indexes to subset the df pandas dataframe\n",
    "    train1, test1 = df.iloc[train_index], df.iloc[test_index]\n",
    "\n",
    "    print len(np.where(train1['sentiment'] == 0)[0])\n",
    "    print len(np.where(train1['sentiment'] == 1)[0])\n",
    "    print len(np.where(test1['sentiment'] == 0)[0])\n",
    "    print len(np.where(test1['sentiment'] == 1)[0])\n",
    "\n",
    "    # ****** Split the training set into clean sentences\n",
    "    #\n",
    "    sentences_pos = []  # Initialize an empty list of sentences\n",
    "    sentences_neg = []  # Initialize an empty list of sentences\n",
    "\n",
    "    #here change to include all journal name labels of positive and negative\n",
    "    inxs_pos = np.where(train1['sentiment'] == 1)[0].tolist()\n",
    "    inxs_neg = np.where(train1['sentiment'] == 0)[0].tolist()\n",
    "\n",
    "    print \"Parsing sentences from training set\"\n",
    "    for inx in inxs_pos:\n",
    "        review = train1[\"abstract\"].iloc[inx]\n",
    "        title = train1['title'].iloc[inx]\n",
    "        descriptors = train1['descriptor_terms'].iloc[inx]\n",
    "        qualifiers = train1['qualifier_terms'].iloc[inx]\n",
    "        sentences_pos += KaggleWord2VecUtility.review_to_sentences(review, tokenizer)\n",
    "        sentences_pos += KaggleWord2VecUtility.review_to_sentences(title, tokenizer)\n",
    "        sentences_pos += KaggleWord2VecUtility.review_to_sentences(descriptors, tokenizer)         \n",
    "        sentences_pos += KaggleWord2VecUtility.review_to_sentences(qualifiers, tokenizer)       \n",
    "        \n",
    "        \n",
    "    for inx in inxs_neg:\n",
    "        review = train1[\"abstract\"].iloc[inx]\n",
    "        title = train1['title'].iloc[inx]\n",
    "        descriptors = train1['descriptor_terms'].iloc[inx]\n",
    "        qualifiers = train1['qualifier_terms'].iloc[inx]\n",
    "        sentences_neg += KaggleWord2VecUtility.review_to_sentences(review, tokenizer)\n",
    "        sentences_neg += KaggleWord2VecUtility.review_to_sentences(title, tokenizer)\n",
    "        sentences_neg += KaggleWord2VecUtility.review_to_sentences(descriptors, tokenizer)         \n",
    "        sentences_neg += KaggleWord2VecUtility.review_to_sentences(qualifiers, tokenizer)       \n",
    "        \n",
    "    # ****** Split the labeled and unlabeled training sets into clean sentences\n",
    "    #\n",
    "    sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "    for i in range(len(train1)):\n",
    "        review = train1['abstract'][i]\n",
    "        title = train1['title'][i]\n",
    "        descriptors = train1['descriptor_terms'][i]\n",
    "        qualifiers = train1['qualifier_terms'][i]\n",
    "        sentences += KaggleWord2VecUtility.review_to_sentences(review, tokenizer)\n",
    "        sentences += KaggleWord2VecUtility.review_to_sentences(title, tokenizer)\n",
    "        sentences += KaggleWord2VecUtility.review_to_sentences(descriptors, tokenizer)\n",
    "        sentences += KaggleWord2VecUtility.review_to_sentences(qualifiers, tokenizer)\n",
    "\n",
    "\n",
    "    print \"Building and training w2v models\"\n",
    "    ## create a w2v learner \n",
    "    basemodel = Word2Vec(\n",
    "        workers=multiprocessing.cpu_count(), # use your cores\n",
    "        iter=3, # iter = sweeps of SGD through the data; more is better\n",
    "        hs=1, negative=0 # we only have scoring for the hierarchical softmax setup\n",
    "        )\n",
    "    basemodel.build_vocab(sentences) \n",
    "\n",
    "    #train models\n",
    "    models = [deepcopy(basemodel) for y in range(2)]\n",
    "    models[0].train(sentences_neg, total_examples=len(sentences_neg) )\n",
    "    models[1].train(sentences_pos, total_examples=len(sentences_pos) )\n",
    "\n",
    "    print \"Parsing test sentences\"\n",
    "    # read in the test set as a list of a list of words\n",
    "    docs = []\n",
    "    i = 0\n",
    "    for review in test1[\"abstract\"]:\n",
    "        if i == 0:\n",
    "            docs.append(KaggleWord2VecUtility.review_to_sentences(review, tokenizer))\n",
    "\n",
    "\n",
    "    print \"scoring test set\"\n",
    "    # get the probs (note we give docprob our test set plus the models)\n",
    "    probs = docprob(docs,models).astype(object)\n",
    "\n",
    "    predictions = np.ones((probs.shape[0]))\n",
    "\n",
    "    #print test1.loc[0:50,['sentiment','abstract']]\n",
    "    #print probs[0:10]\n",
    "\n",
    "    predictions[np.where(probs.iloc[:,0] > 0.5)] = 0 # The first column is the negative model\n",
    "    print predictions\n",
    "\n",
    "    #think these line up\n",
    "    #ok so issue is just that document probabilities are so much higher scored for 0 than 1 for some reason.\n",
    "    #Cant figure out why...most likely not a bug but just the word2vec model not performing great\n",
    "    score = roc_auc_score(test1[\"sentiment\"], predictions)\n",
    "    #score = np.size(np.where(predictions == test1[\"sentiment\"]))*1./np.size(predictions)\n",
    "    print score\n",
    "    \n",
    "b = datetime.datetime.now().replace(microsecond=0)\n",
    "print(b-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ROUND', 1)\n",
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarahwie/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building and training w2v models\n",
      "Parsing test sentences\n",
      "scoring test set\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "85\n",
      "0.510532150776\n",
      "('ROUND', 2)\n",
      "Parsing sentences from training set\n",
      "Building and training w2v models\n",
      "Parsing test sentences\n",
      "scoring test set\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "85\n",
      "0.487804878049\n",
      "('ROUND', 3)\n",
      "Parsing sentences from training set\n",
      "Building and training w2v models\n",
      "Parsing test sentences\n",
      "scoring test set\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "84\n",
      "0.510227272727\n",
      "('ROUND', 4)\n",
      "Parsing sentences from training set\n",
      "Building and training w2v models\n",
      "Parsing test sentences\n",
      "scoring test set\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "84\n",
      "0.5\n",
      "('ROUND', 5)\n",
      "Parsing sentences from training set\n",
      "Building and training w2v models\n",
      "Parsing test sentences\n",
      "scoring test set\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "84\n",
      "0.5\n",
      "('average of 5 rotations:', 0.50171286031042128)\n",
      "0:00:11\n"
     ]
    }
   ],
   "source": [
    "a = datetime.datetime.now().replace(microsecond=0)\n",
    "\n",
    "#5-fold stratified cross validation\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "#because no validation set, 4/5 of values go to train and 1/5 to test\n",
    "#is this too high?***\n",
    "#even though we shuffle, not as randomly distributed as the former method was\n",
    "y = df['sentiment'].values\n",
    "skf = StratifiedKFold(y, n_folds=5, shuffle=True)\n",
    "\n",
    "i = 1\n",
    "avg = []\n",
    "for train_index, test_index in skf:\n",
    "    print(\"ROUND\", i)\n",
    "    i = i + 1\n",
    "    #use the indexes to subset the df pandas dataframe\n",
    "    train1, test1 = df.iloc[train_index], df.iloc[test_index]\n",
    "    \n",
    "    # ****** Split the training set into clean sentences\n",
    "    #\n",
    "    sentences_pos = []  # Initialize an empty list of sentences\n",
    "    sentences_neg = []  # Initialize an empty list of sentences\n",
    "\n",
    "    #here change to include all journal name labels of positive and negative\n",
    "    inxs_pos = np.where(train1['sentiment'] == 1)[0].tolist()\n",
    "    inxs_neg = np.where(train1['sentiment'] == 0)[0].tolist()\n",
    "\n",
    "    print \"Parsing sentences from training set\"\n",
    "    for inx in inxs_pos:\n",
    "        review = train1[\"abstract\"].iloc[inx]\n",
    "        sentences_pos += KaggleWord2VecUtility.review_to_sentences(review, tokenizer)\n",
    "\n",
    "    for inx in inxs_neg:\n",
    "        review = train1[\"abstract\"].iloc[inx]\n",
    "        sentences_neg += KaggleWord2VecUtility.review_to_sentences(review, tokenizer) \n",
    "        \n",
    "    # ****** Split the labeled and unlabeled training sets into clean sentences\n",
    "    #\n",
    "    sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "    for review in train1[\"abstract\"]:\n",
    "        sentences += KaggleWord2VecUtility.review_to_sentences(review, tokenizer)\n",
    "    \n",
    "    print \"Building and training w2v models\"\n",
    "    ## create a w2v learner \n",
    "    basemodel = Word2Vec(\n",
    "        workers=multiprocessing.cpu_count(), # use your cores\n",
    "        iter=3, # iter = sweeps of SGD through the data; more is better\n",
    "        hs=1, negative=0 # we only have scoring for the hierarchical softmax setup\n",
    "        )\n",
    "    basemodel.build_vocab(sentences) \n",
    "    \n",
    "    #train models\n",
    "    models = [deepcopy(basemodel) for y in range(2)]\n",
    "    models[0].train(sentences_neg, total_examples=len(sentences_neg) )\n",
    "    models[1].train(sentences_pos, total_examples=len(sentences_pos) )\n",
    "    \n",
    "    print \"Parsing test sentences\"\n",
    "    # read in the test set as a list of a list of words\n",
    "    docs = []\n",
    "    for review in test1[\"abstract\"]:\n",
    "        docs.append(KaggleWord2VecUtility.review_to_sentences(review, tokenizer))\n",
    "    \n",
    "    print \"scoring test set\"\n",
    "    # get the probs (note we give docprob our test set plus the models)\n",
    "    probs = docprob(docs,models).astype(object)\n",
    "    \n",
    "    predictions = np.ones((probs.shape[0]))\n",
    "    \n",
    "    predictions[np.where(probs.iloc[:,0] > 0.5)] = 0 # The first column is the negative model\n",
    "    print predictions\n",
    "    print np.size(predictions)\n",
    "    \n",
    "    score = roc_auc_score(test1[\"sentiment\"], predictions)\n",
    "    #score = np.size(np.where(predictions == test1[\"sentiment\"]))*1./np.size(predictions)\n",
    "    print score\n",
    "    #append to average\n",
    "    avg.append(score)\n",
    "\n",
    "\n",
    "print(\"average of 5 rotations:\", sum(avg)/float(len(avg)))\n",
    "    \n",
    "b = datetime.datetime.now().replace(microsecond=0)\n",
    "print(b-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   3   5   6   7   9  11  12  13  14  15  16  17  18  19  20  21\n",
      "  22  23  25  26  27  28  29  30  32  33  34  35  36  37  38  41  42  44\n",
      "  45  46  47  48  49  50  51  52  54  55  56  57  58  59  60  61  64  65\n",
      "  67  68  69  70  71  72  73  74  75  76  77  78  79  82  83  84  86  87\n",
      "  89  90  91  92  93  95  96  97  98  99 100 101 102 103 104 106 107 108\n",
      " 109 112 113 114 116 118 119 120 121 122 123 124 125 126 127 129 130 131\n",
      " 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 150 151 153\n",
      " 154 155 156 157 158 159 160 161 162 164 165 166 167 168 169 170 171 172\n",
      " 174 178 179 180 181 183 184 185 186 188 189 190 191 192 193 194 198 199\n",
      " 202 203 204 206 207 208 209 210 212 213 214 215 216 219 220 221 223 224\n",
      " 225 226 227 228 229 231 232 233 234 235 236 237 238 239 240 242 243 244\n",
      " 246 247 248 249 250 251 252 253 254 255 256 257 259 260 261 262 263 264\n",
      " 265 266 267 268 270 272 273 274 275 276 278 279 280 282 284 285 286 287\n",
      " 288 289 290 292 293 294 295 296 297 299 300 301 302 303 305 307 308 309\n",
      " 311 314 316 317 318 320 322]\n",
      "[  2   4   8  10  24  31  39  40  43  53  62  63  66  80  81  85  88  94\n",
      " 105 110 111 115 117 128 147 148 149 152 163 173 175 176 177 182 187 195\n",
      " 196 197 200 201 205 211 217 218 222 230 241 245 258 269 271 277 281 283\n",
      " 291 298 304 306 310 312 313 315 319 321]\n",
      "     sentiment                              journal  \\\n",
      "0            0                       Brain research   \n",
      "1            0                       Brain research   \n",
      "2            0                       Brain research   \n",
      "3            0                       Brain research   \n",
      "4            0                       Brain research   \n",
      "5            0                       Brain research   \n",
      "6            0                       Brain research   \n",
      "7            0                       Brain research   \n",
      "8            0                       Brain research   \n",
      "9            0                       Brain research   \n",
      "10           0                       Brain research   \n",
      "11           0                       Brain research   \n",
      "12           0                       Brain research   \n",
      "13           0                       Brain research   \n",
      "14           0                       Brain research   \n",
      "15           1  The Journal of biological chemistry   \n",
      "16           1  The Journal of biological chemistry   \n",
      "17           1  The Journal of biological chemistry   \n",
      "18           1  The Journal of biological chemistry   \n",
      "19           1  The Journal of biological chemistry   \n",
      "20           1  The Journal of biological chemistry   \n",
      "21           1  The Journal of biological chemistry   \n",
      "22           1  The Journal of biological chemistry   \n",
      "23           1  The Journal of biological chemistry   \n",
      "24           1  The Journal of biological chemistry   \n",
      "25           1  The Journal of biological chemistry   \n",
      "26           1  The Journal of biological chemistry   \n",
      "27           1  The Journal of biological chemistry   \n",
      "28           1  The Journal of biological chemistry   \n",
      "29           1  The Journal of biological chemistry   \n",
      "..         ...                                  ...   \n",
      "249          1  The Journal of biological chemistry   \n",
      "250          1  The Journal of biological chemistry   \n",
      "251          1  The Journal of biological chemistry   \n",
      "252          1  The Journal of biological chemistry   \n",
      "253          1  The Journal of biological chemistry   \n",
      "254          1  The Journal of biological chemistry   \n",
      "255          1  The Journal of biological chemistry   \n",
      "256          1  The Journal of biological chemistry   \n",
      "257          1  The Journal of biological chemistry   \n",
      "258          1  The Journal of biological chemistry   \n",
      "259          1  The Journal of biological chemistry   \n",
      "260          1  The Journal of biological chemistry   \n",
      "261          1  The Journal of biological chemistry   \n",
      "262          1  The Journal of biological chemistry   \n",
      "263          1  The Journal of biological chemistry   \n",
      "264          1  The Journal of biological chemistry   \n",
      "265          1  The Journal of biological chemistry   \n",
      "266          1  The Journal of biological chemistry   \n",
      "267          1  The Journal of biological chemistry   \n",
      "268          1  The Journal of biological chemistry   \n",
      "269          1  The Journal of biological chemistry   \n",
      "270          1  The Journal of biological chemistry   \n",
      "271          1  The Journal of biological chemistry   \n",
      "272          1  The Journal of biological chemistry   \n",
      "273          1  The Journal of biological chemistry   \n",
      "274          1  The Journal of biological chemistry   \n",
      "275          1  The Journal of biological chemistry   \n",
      "276          1  The Journal of biological chemistry   \n",
      "277          1  The Journal of biological chemistry   \n",
      "278          1  The Journal of biological chemistry   \n",
      "\n",
      "                                              abstract  \n",
      "0    The dorsolateral quadrant of the lateral septa...  \n",
      "1    Changes of the internal (d) and external diame...  \n",
      "2    The neuranatomical location and pharmacologica...  \n",
      "3    The neuroanatomical organization of the effere...  \n",
      "4    The role of nerve growth factor (NGF) in the d...  \n",
      "5    The recurrence every 24 h of glucocorticoid el...  \n",
      "6    Dopamine-beta-hydroxylase (DBH) and norepineph...  \n",
      "7    Locally applied kainic acid was used in order ...  \n",
      "8    Using extracellular unit recording and microio...  \n",
      "9    The organization of globus pallidus (GP) proje...  \n",
      "10   The development of opiate mechanisms in the gu...  \n",
      "11   A regular slow wave theta rhythm can be record...  \n",
      "12   Fibers descending in the lateral columns (LC) ...  \n",
      "13   The incorporation of 35SO4(2-) and [3H]galacto...  \n",
      "14   Selectivity in the reinnervation of denervated...  \n",
      "15   The location of the functionally important -SH...  \n",
      "16   The pattern of proteins synthesized by chicken...  \n",
      "17   Preferential labeling of COOH-terminal sequenc...  \n",
      "18   Urinary acidification in the turtle urinary bl...  \n",
      "19   It has been demonstrated that the reaction of ...  \n",
      "20   The tetrameric form of a Desulfovibrio gigas f...  \n",
      "21   A new pigment from Halobacterium halobium has ...  \n",
      "22   The binding reaction of NADH to liver alcohol ...  \n",
      "23   The pokeweed antiviral protein and the toxin r...  \n",
      "24   Transport of methotrexate by L1210 sensitive a...  \n",
      "25   The rate of transport of alpha-aminoisobutyric...  \n",
      "26   Surface aromatic residues of bovine alpha-lact...  \n",
      "27   Cvc- mutants of Escherichia coli are deficient...  \n",
      "28   Total glycopeptides from human K-562 cells, la...  \n",
      "29   Liver glycogen phosphorylase associated with t...  \n",
      "..                                                 ...  \n",
      "249  A DNA-dependent ATPase with a molecular weight...  \n",
      "250  Submitochondrial particles obtained from skele...  \n",
      "251  Rabbits were made anemic to different extents ...  \n",
      "252  The fibroblast is a differentiated mesenchymal...  \n",
      "253  Conformational difference surrounding the coen...  \n",
      "254  The kinetic properties of human placental aden...  \n",
      "255  In order to examine the mechanism of the acute...  \n",
      "256  Fourteen clones were isolated in culture from ...  \n",
      "257  The bile acid-conjugating enzyme cholyl-coenzy...  \n",
      "258  The protein synthesis elongation factor EF-Tu,...  \n",
      "259  A theoretical analysis has been derived which ...  \n",
      "260  The mechanism of beef heart mitochondrial ATPa...  \n",
      "261  The reaction mechanism of melilotate hydroxyla...  \n",
      "262  A beta-galactoside alpha 1 leads to 2 fucosylt...  \n",
      "263  The acceptor substrate specificity and kinetic...  \n",
      "264  Metallothionein was purified under anaerobic c...  \n",
      "265  Membrane fluidity increases within 10 min afte...  \n",
      "266  We introduce two new, rapid procedures. One is...  \n",
      "267  A serine esterase with potent proteolytic acti...  \n",
      "268  Neurites were prepared by a novel method from ...  \n",
      "269  Immunoprecipitation of uniformly labeled yeast...  \n",
      "270  The uptake and metabolism of lymphatic large c...  \n",
      "271  Glutamine synthetase specific activity increas...  \n",
      "272  An icosadeoxyribonucleotide containing the sev...  \n",
      "273  pppA(2'p5'A)n-1 ((2'-5')(A)n) synthetase is on...  \n",
      "274  Stopped flow kinetic studies have been used to...  \n",
      "275  Alkaline-sensitive strains of Escherichia coli...  \n",
      "276  5-Dimethylaminonaphthalene-1-sulfonyl (dansyl)...  \n",
      "277  Removal of the terminal sialic acid residues f...  \n",
      "278  Monovalent Fab fragments from immunoglobulins,...  \n",
      "\n",
      "[259 rows x 3 columns]\n",
      "     sentiment                              journal  \\\n",
      "146          0                       Brain research   \n",
      "147          0                       Brain research   \n",
      "148          0                       Brain research   \n",
      "149          0                       Brain research   \n",
      "150          0                       Brain research   \n",
      "151          0                       Brain research   \n",
      "152          0                       Brain research   \n",
      "153          0                       Brain research   \n",
      "154          0                       Brain research   \n",
      "155          0                       Brain research   \n",
      "156          0                       Brain research   \n",
      "157          0                       Brain research   \n",
      "158          0                       Brain research   \n",
      "159          0                       Brain research   \n",
      "160          0                       Brain research   \n",
      "161          0                       Brain research   \n",
      "162          0                       Brain research   \n",
      "163          0                       Brain research   \n",
      "164          0                       Brain research   \n",
      "165          0                       Brain research   \n",
      "279          1  The Journal of biological chemistry   \n",
      "280          1  The Journal of biological chemistry   \n",
      "281          1  The Journal of biological chemistry   \n",
      "282          1  The Journal of biological chemistry   \n",
      "283          1  The Journal of biological chemistry   \n",
      "284          1  The Journal of biological chemistry   \n",
      "285          1  The Journal of biological chemistry   \n",
      "286          1  The Journal of biological chemistry   \n",
      "287          1  The Journal of biological chemistry   \n",
      "288          1  The Journal of biological chemistry   \n",
      "..         ...                                  ...   \n",
      "293          1  The Journal of biological chemistry   \n",
      "294          1  The Journal of biological chemistry   \n",
      "295          1  The Journal of biological chemistry   \n",
      "296          1  The Journal of biological chemistry   \n",
      "297          1  The Journal of biological chemistry   \n",
      "298          1  The Journal of biological chemistry   \n",
      "299          1  The Journal of biological chemistry   \n",
      "300          1  The Journal of biological chemistry   \n",
      "301          1  The Journal of biological chemistry   \n",
      "302          1  The Journal of biological chemistry   \n",
      "303          1  The Journal of biological chemistry   \n",
      "304          1  The Journal of biological chemistry   \n",
      "305          1  The Journal of biological chemistry   \n",
      "306          1  The Journal of biological chemistry   \n",
      "307          1  The Journal of biological chemistry   \n",
      "308          1  The Journal of biological chemistry   \n",
      "309          1  The Journal of biological chemistry   \n",
      "310          1  The Journal of biological chemistry   \n",
      "311          1  The Journal of biological chemistry   \n",
      "312          1  The Journal of biological chemistry   \n",
      "313          1  The Journal of biological chemistry   \n",
      "314          1  The Journal of biological chemistry   \n",
      "315          1  The Journal of biological chemistry   \n",
      "316          1  The Journal of biological chemistry   \n",
      "317          1  The Journal of biological chemistry   \n",
      "318          1  The Journal of biological chemistry   \n",
      "319          1  The Journal of biological chemistry   \n",
      "320          1  The Journal of biological chemistry   \n",
      "321          1  The Journal of biological chemistry   \n",
      "322          1  The Journal of biological chemistry   \n",
      "\n",
      "                                              abstract  \n",
      "146  Various stressful manipulations in rats (cold-...  \n",
      "147  The location of both afferent and efferent car...  \n",
      "148  Afferents of the lateral (LH) and ventromedial...  \n",
      "149  Efferent projections of the optic tectum were ...  \n",
      "150  The afferent projections to the primate amygda...  \n",
      "151  Astrogliogenesis in the human fetal cerebellum...  \n",
      "152  The ependymal and supraependymal cells of the ...  \n",
      "153  Surgical stress did not elevate plasma cortico...  \n",
      "154  The connection between the date of formation o...  \n",
      "155  The effects of unilateral medial forebrain bun...  \n",
      "156  The interaction between tonic labyrinth or nec...  \n",
      "157  When frog nerve-muscle preparations are stimul...  \n",
      "158  The purpose of this study was to examine the t...  \n",
      "159  The quantitative aspects of formaldehyde induc...  \n",
      "160  The rate and pattern of seizure development pr...  \n",
      "161  Primary cultures of fetal mouse brain and spin...  \n",
      "162  Unilateral transections of the brachium conjun...  \n",
      "163  L-[35S]Methionine was injected into the dorsal...  \n",
      "164  A study has been made of the regulation of evo...  \n",
      "165  The effect of monomeric acrylamide, a potent n...  \n",
      "279  In Escherichia coli, amino acid starvation tri...  \n",
      "280  The carbon monoxide binding kinetics of the is...  \n",
      "281  The activities of the mitochondrial NAD(P)+- a...  \n",
      "282  Cytochrome f has been purified from spinach ch...  \n",
      "283  The thermal and guanidine hydrochloride (GdnHC...  \n",
      "284  The cholinergic mouse neuroblastoma cell line ...  \n",
      "285  Heparosan N-sulfate D-glucuronosyl 5-epimerase...  \n",
      "286  The association rate constants for the interac...  \n",
      "287  Creatine kinase does not catalyze the scrambli...  \n",
      "288  Human plasm contains at least two distinct kin...  \n",
      "..                                                 ...  \n",
      "293  Saturation analysis of equilibrium binding of ...  \n",
      "294  In the rat hepatocyte, whether freshly separat...  \n",
      "295  The first quantitative measurements of the eff...  \n",
      "296  Kinetic studies on the unfolding of pepsinogen...  \n",
      "297  Cyanogen bromide fragments of alpha 1-protease...  \n",
      "298  Human alpha 1-protease inhibitor has three oli...  \n",
      "299  Previous observations suggested that pyridoxal...  \n",
      "300  Cytosol prepared from homogenates of bone from...  \n",
      "301  In previous studies in this laboratory, highly...  \n",
      "302  Tryptophan hydroxylase (EC 1.14.16.4) from rat...  \n",
      "303  The reactions of human hemoglobin with a serie...  \n",
      "304  The reactions of 13 isonitriles with deoxyhemo...  \n",
      "305  An enzyme present in rabbit liver microsomes h...  \n",
      "306  Several fluorinated derivatives of p-hydroxybe...  \n",
      "307  An eight-iron, eight-sulfur ferredoxin from Rh...  \n",
      "308  A dianionic spin label, 1-L-glutamate-5-N-(1-o...  \n",
      "309  Endogenous lactose-binding proteins from adult...  \n",
      "310  The specificity of the cyclic AMP-dependent pr...  \n",
      "311  An incomplete precursor of lipid A produced by...  \n",
      "312  The incomplete lipid A precursor produced by a...  \n",
      "313  The effect of cerulenin on conversion of an ac...  \n",
      "314  The intrinsic isotope effect on the reduction ...  \n",
      "315  Two human breast cancer cell lines (T-47D and ...  \n",
      "316  A wide variety of double-stranded DNA template...  \n",
      "317  Late in the life cycle of the single-stranded ...  \n",
      "318  A DNA-dependent RNA polymerase has been purifi...  \n",
      "319  We have obtained evidence in vivo for the intr...  \n",
      "320  The major aminoacyl-tRNA synthetase complex (t...  \n",
      "321  A DNA-dependent RNA polymerase has been extrac...  \n",
      "322  The irreversible inactivation by heat of the F...  \n",
      "\n",
      "[64 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#confirming index match to dataframe for the last fold\n",
    "print train_index\n",
    "print test_index\n",
    "print y_train\n",
    "print y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentiment         journal  \\\n",
      "2          0  Brain research   \n",
      "\n",
      "                                            abstract  \n",
      "2  The neuranatomical location and pharmacologica...  \n"
     ]
    }
   ],
   "source": [
    "#print first record in test set (note has different index value, indexed instead by location (pos. 0))\n",
    "print test1.iloc[[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Import KaggleWord2VecUtility since didn't work from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "class KaggleWord2VecUtility(object):\n",
    "    \"\"\"KaggleWord2VecUtility is a utility class for processing raw HTML text into segments for further learning\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def review_to_wordlist( review, remove_stopwords=False ):\n",
    "        # Function to convert a document to a sequence of words,\n",
    "        # optionally removing stop words.  Returns a list of words.\n",
    "        #\n",
    "        # 1. Remove HTML\n",
    "        review_text = BeautifulSoup(review).get_text()\n",
    "        #\n",
    "        # 2. Remove non-letters\n",
    "        review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "        #\n",
    "        # 3. Convert words to lower case and split them\n",
    "        words = review_text.lower().split()\n",
    "        #\n",
    "        # 4. Optionally remove stop words (false by default)\n",
    "        if remove_stopwords:\n",
    "            stops = set(stopwords.words(\"english\"))\n",
    "            words = [w for w in words if not w in stops]\n",
    "        #\n",
    "        # 5. Return a list of words\n",
    "        return(words)\n",
    "    \n",
    "    @staticmethod\n",
    "    def review_to_words( review, remove_stopwords=False ):\n",
    "        # Function to convert a raw review to a string of words\n",
    "        # The input is a single string (a raw movie review), and \n",
    "        # the output is a single string (a preprocessed movie review)\n",
    "        #\n",
    "        # 1. Remove HTML\n",
    "        review_text = BeautifulSoup(review).get_text() \n",
    "        #\n",
    "        # 2. Remove non-letters        \n",
    "        review_text = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "        #\n",
    "        # 3. Convert to lower case, split into individual words\n",
    "        words = review_text.lower().split()                             \n",
    "        #\n",
    "        # 4. Optionally remove stop words (false by default)\n",
    "        if remove_stopwords:\n",
    "            stops = set(stopwords.words(\"english\"))\n",
    "            words = [w for w in words if not w in stops]   \n",
    "        #\n",
    "        # 6. Join the words back into one string separated by space, \n",
    "        # and return the result.\n",
    "        return( \" \".join( words ))   \n",
    "\n",
    "    # Define a function to split a review into parsed sentences\n",
    "    @staticmethod\n",
    "    def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "        # Function to split a review into parsed sentences. Returns a\n",
    "        # list of sentences, where each sentence is a list of words\n",
    "        #\n",
    "        # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "        raw_sentences = tokenizer.tokenize(review.decode('utf8').strip())\n",
    "        #\n",
    "        # 2. Loop over each sentence\n",
    "        sentences = []\n",
    "        for raw_sentence in raw_sentences:\n",
    "            # If a sentence is empty, skip it\n",
    "            if len(raw_sentence) > 0:\n",
    "                # Otherwise, call review_to_wordlist to get a list of words\n",
    "                sentences.append( KaggleWord2VecUtility.review_to_wordlist( raw_sentence, \\\n",
    "                  remove_stopwords ))\n",
    "        #\n",
    "        # Return the list of sentences (each sentence is a list of words,\n",
    "        # so this returns a list of lists\n",
    "        return sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
