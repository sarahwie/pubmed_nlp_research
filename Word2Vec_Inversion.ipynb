{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.data\n",
    "import logging\n",
    "import numpy as np  # Make sure that numpy is imported\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/sarahwie/Documents/pubmed-nlp-research/DeepLearningMovies_datasets/')\n",
    "from KaggleWord2VecUtility import KaggleWord2VecUtility\n",
    "DATADIR='/home/sarahwie/Documents/pubmed-nlp-research/DeepLearningMovies_datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read data from files\n",
    "train = pd.read_csv( os.path.join(DATADIR, 'labeledTrainData.tsv'), header=0, delimiter=\"\\t\", quoting=3 )\n",
    "unlabeled_train = pd.read_csv( os.path.join(DATADIR, \"unlabeledTrainData.tsv\"), header=0,  delimiter=\"\\t\", quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews and 50000 unlabeled reviews\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify the number of reviews that were read (100,000 in total)\n",
    "print \"Read %d labeled train reviews \" \\\n",
    "     \"and %d unlabeled reviews\\n\" % (train[\"review\"].size, unlabeled_train[\"review\"].size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\\\"The Classic War of the Worlds\\\" by Timothy Hines is a very entertaining film that obviously goes to great effort and lengths to faithfully recreate H. G. Wells' classic book. Mr. Hines succeeds in doing so. I, and those who watched his film with me, appreciated the fact that it was not the standard, predictable Hollywood fare that comes out every year, e.g. the Spielberg version with Tom Cruise that had only the slightest resemblance to the book. Obviously, everyone looks for different things in a movie. Those who envision themselves as amateur \\\"critics\\\" look only to criticize everything they can. Others rate a movie on more important bases,like being entertained, which is why most people never agree with the \\\"critics\\\". We enjoyed the effort Mr. Hines put into being faithful to H.G. Wells' classic novel, and we found it to be very entertaining. This made it easy to overlook what the \\\"critics\\\" perceive to be its shortcomings.\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print train.ix[1,][2]\n",
    "train.loc[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get an even split train/test set from our labelled training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "because Kaggle's test set has no labels so we need our own labelled test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fracTrain = 0.5\n",
    "nSamples = train.shape[0]\n",
    "order = np.random.permutation(nSamples) # come up with a random ordering\n",
    "splitIndex = int(np.round(nSamples*fracTrain))\n",
    "train1 = train.ix[order[:splitIndex],:]\n",
    "test1 = train.ix[order[splitIndex:],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12500, 3)\n",
      "(12500, 3)\n"
     ]
    }
   ],
   "source": [
    "print train1.shape\n",
    "print test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id  sentiment                                             review\n",
      "21590  \"240_1\"          0  \"I never like to comment on a good film but wh...\n"
     ]
    }
   ],
   "source": [
    "print test1.iloc[[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(train1, open('train1.p', 'wb'))\n",
    "pickle.dump(test1, open('test1.p', 'wb'))\n",
    "pickle.dump(unlabeled_train, open('unlabeled_train.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec inversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some stuff to split the labelled training set into positive and negative reviews. NOTE: takes a long time to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gets a list of positive review indexes and negative review indexes. Takes those respective reviews and parses them into sentences and appends to a list of positive and negative sentences, respectively. Also does the same for the unlabelled training sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarahwie/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n",
      "/home/sarahwie/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:198: UserWarning: \".\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  '\"%s\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.' % markup)\n",
      "/home/sarahwie/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:207: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from unlabeled set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarahwie/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:207: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/home/sarahwie/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:207: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/home/sarahwie/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:207: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/home/sarahwie/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:198: UserWarning: \"..\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  '\"%s\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.' % markup)\n",
      "/home/sarahwie/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:207: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/home/sarahwie/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:207: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
     ]
    }
   ],
   "source": [
    "# ****** Split the labeled and unlabeled training sets into clean sentences\n",
    "#\n",
    "sentences_pos = []  # Initialize an empty list of sentences\n",
    "sentences_neg = []  # Initialize an empty list of sentences\n",
    "sentences_unlabelled = []  # Initialize an empty list of sentences\n",
    "\n",
    "inxs_pos = np.where(train1[\"sentiment\"] == 1)[0].tolist()\n",
    "inxs_neg = np.where(train1[\"sentiment\"] == 0)[0].tolist()\n",
    "\n",
    "print \"Parsing sentences from training set\"\n",
    "for inx in inxs_pos:\n",
    "    review = train1[\"review\"].iloc[inx]\n",
    "    sentences_pos += KaggleWord2VecUtility.review_to_sentences(review, tokenizer)\n",
    "\n",
    "for inx in inxs_neg:\n",
    "    review = train1[\"review\"].iloc[inx]\n",
    "    sentences_neg += KaggleWord2VecUtility.review_to_sentences(review, tokenizer)    \n",
    "    \n",
    "print \"Parsing sentences from unlabeled set\"\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences_unlabelled += KaggleWord2VecUtility.review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same for all sentences without splitting to build a vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n",
      "Parsing sentences from unlabeled set\n"
     ]
    }
   ],
   "source": [
    "# ****** Split the labeled and unlabeled training sets into clean sentences\n",
    "#\n",
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print \"Parsing sentences from training set\"\n",
    "for review in train1[\"review\"]:\n",
    "    sentences += KaggleWord2VecUtility.review_to_sentences(review, tokenizer)\n",
    "\n",
    "print \"Parsing sentences from unlabeled set\"\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += KaggleWord2VecUtility.review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some stuff from Taddy's code (much simpler than Kaggle method for building Word2Vec models). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=0, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "\n",
    "## create a w2v learner \n",
    "basemodel = Word2Vec(\n",
    "    workers=multiprocessing.cpu_count(), # use your cores\n",
    "    iter=3, # iter = sweeps of SGD through the data; more is better\n",
    "    hs=1, negative=0 # we only have scoring for the hierarchical softmax setup\n",
    "    )\n",
    "print basemodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "basemodel.build_vocab(sentences) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train 2 models off of the base model (one pos & one neg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3301132"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "models = [deepcopy(basemodel) for i in range(2)]\n",
    "models[0].train(sentences_neg, total_examples=len(sentences_neg) )\n",
    "models[1].train(sentences_pos, total_examples=len(sentences_pos) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(models, open('models.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inversion of the distributed representations\n",
    "\n",
    "At this point, we have 5 different word2vec language representations. Each 'model' has been trained conditional (i.e., limited to) text from a specific star rating. We will apply Bayes rule to go from p(text|stars) to p(stars|text).\n",
    "For any new sentence we can obtain its likelihood (lhd; actually, the composite likelihood approximation; see the paper) using the score function in the word2vec class. We get the likelihood for each sentence in the first test review, then convert to a probability over star ratings. Every sentence in the review is evaluated separately and the final star rating of the review is an average vote of all the sentences. This is all in the following handy wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "docprob takes two lists\n",
    "* docs: a list of documents, each of which is a list of sentences\n",
    "* models: the candidate word2vec models (each potential class)\n",
    "\n",
    "it returns the array of class probabilities.  Everything is done in-memory.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd # for quick summing within doc\n",
    "\n",
    "def docprob(docs, mods):\n",
    "    # score() takes a list [s] of sentences here; could also be a sentence generator\n",
    "    sentlist = [s for d in docs for s in d]\n",
    "    # the log likelihood of each sentence in this review under each w2v representation\n",
    "    llhd = np.array( [ m.score(sentlist, len(sentlist)) for m in mods ] )\n",
    "    # now exponentiate to get likelihoods, \n",
    "    lhd = np.exp(llhd - llhd.max(axis=0)) # subtract row max to avoid numeric overload\n",
    "    # normalize across models (stars) to get sentence-star probabilities\n",
    "    #all this transposing business does is make it so the total probability of a word \n",
    "    #   equals 1 between the 2 arrays (positive prob and negative).\n",
    "    #and the pandas data frame just puts everything into rows/columns format for easy viz\n",
    "    prob = pd.DataFrame( (lhd/lhd.sum(axis=0)).transpose() )\n",
    "    # and finally average the sentence probabilities to get the review probability\n",
    "    prob[\"doc\"] = [i for i,d in enumerate(docs) for s in d]\n",
    "    prob = prob.groupby(\"doc\").mean()\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read in the test set as a list of a list of words\n",
    "docs = []\n",
    "for review in test1[\"review\"]:\n",
    "    docs.append(KaggleWord2VecUtility.review_to_sentences(review, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'this', u'movie', u'is', u'like', u'the', u'thousand', u'cat', u'and', u'mouse', u'movies', u'that', u'preceded', u'it'], [u'the', u'following', u'may', u'look', u'like', u'a', u'spoiler', u'but', u'it', u'really', u'just', u'describes', u'a', u'large', u'class', u'of', u'movies', u'there', u'is', u'the', u'passionate', u'wise', u'main', u'character', u'his', u'goofy', u'but', u'well', u'meaning', u'sidekick', u'with', u'his', u'ill', u'placed', u'attempts', u'at', u'humorous', u'comments', u'the', u'initially', u'hostile', u'but', u'soon', u'softened', u'gorgeous', u'lady', u'who', u'triggers', u'the', u'inevitable', u'unlikely', u'love', u'story', u'the', u'loved', u'ones', u'taken', u'hostage', u'and', u'of', u'course', u'the', u'careless', u'evil', u'adversary', u'with', u'his', u'brutal', u'minions'], [u'everybody', u'has', u'seen', u'tons', u'of', u'these', u'movies', u'already', u'and', u'national', u'treasure', u'is', u'like', u'any', u'one', u'of', u'them', u'with', u'only', u'a', u'slightly', u'modified', u'wrapping'], [u'every', u'turn', u'of', u'the', u'story', u'was', u'easily', u'predicted', u'and', u'i', u'can', u'assure', u'you', u'i', u'am', u'not', u'the', u'sharpest', u'tool', u'in', u'the', u'shed'], [u'i', u'am', u'quite', u'tired', u'of', u'feeling', u'tricked', u'for', u'money', u'after', u'exiting', u'the', u'theater', u'from', u'a', u'hollywood', u'movie', u'and', u'if', u'you', u'have', u'ever', u'felt', u'that', u'way', u'too', u'heed', u'my', u'warning', u'stay', u'miles', u'away', u'from', u'this', u'movie']]\n"
     ]
    }
   ],
   "source": [
    "print docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probstest = docprob(docs[0:2], models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.484133</td>\n",
       "      <td>0.515867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.573073</td>\n",
       "      <td>0.426927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1\n",
       "doc                    \n",
       "0    0.484133  0.515867\n",
       "1    0.573073  0.426927"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probstest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set example\n",
    "As an example, we apply the inversion on the full test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the probs (note we give docprob our test set plus the models)\n",
    "probs = docprob(docs,models).astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.484133</td>\n",
       "      <td>0.515867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.573073</td>\n",
       "      <td>0.426927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.261172</td>\n",
       "      <td>0.738828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0482365</td>\n",
       "      <td>0.951763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.536646</td>\n",
       "      <td>0.463354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.335723</td>\n",
       "      <td>0.664277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.527615</td>\n",
       "      <td>0.472385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0064288</td>\n",
       "      <td>0.993571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.594172</td>\n",
       "      <td>0.405828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.992335</td>\n",
       "      <td>0.00766541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.29742</td>\n",
       "      <td>0.70258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.464517</td>\n",
       "      <td>0.535483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.270625</td>\n",
       "      <td>0.729375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.808919</td>\n",
       "      <td>0.191081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0987737</td>\n",
       "      <td>0.901226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.665317</td>\n",
       "      <td>0.334683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0963713</td>\n",
       "      <td>0.903629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.68365</td>\n",
       "      <td>0.31635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.35162</td>\n",
       "      <td>0.64838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.86684</td>\n",
       "      <td>0.13316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.474654</td>\n",
       "      <td>0.525346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.5399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.674869</td>\n",
       "      <td>0.325131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.939341</td>\n",
       "      <td>0.0606587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.111452</td>\n",
       "      <td>0.888548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.994973</td>\n",
       "      <td>0.00502709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.992251</td>\n",
       "      <td>0.00774896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.421071</td>\n",
       "      <td>0.578929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.443014</td>\n",
       "      <td>0.556986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>7.93681e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12470</th>\n",
       "      <td>0.650802</td>\n",
       "      <td>0.349198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12471</th>\n",
       "      <td>0.758446</td>\n",
       "      <td>0.241554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12472</th>\n",
       "      <td>0.331747</td>\n",
       "      <td>0.668253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12473</th>\n",
       "      <td>0.0944917</td>\n",
       "      <td>0.905508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12474</th>\n",
       "      <td>0.6507</td>\n",
       "      <td>0.3493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12475</th>\n",
       "      <td>0.193575</td>\n",
       "      <td>0.806425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12476</th>\n",
       "      <td>0.388473</td>\n",
       "      <td>0.611527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12477</th>\n",
       "      <td>0.216336</td>\n",
       "      <td>0.783664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12478</th>\n",
       "      <td>0.998954</td>\n",
       "      <td>0.00104552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12479</th>\n",
       "      <td>0.326922</td>\n",
       "      <td>0.673078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12480</th>\n",
       "      <td>0.994521</td>\n",
       "      <td>0.00547858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12481</th>\n",
       "      <td>0.24166</td>\n",
       "      <td>0.75834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12482</th>\n",
       "      <td>0.327032</td>\n",
       "      <td>0.672968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12483</th>\n",
       "      <td>0.299782</td>\n",
       "      <td>0.700218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12484</th>\n",
       "      <td>0.570366</td>\n",
       "      <td>0.429634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12485</th>\n",
       "      <td>0.132481</td>\n",
       "      <td>0.867519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12486</th>\n",
       "      <td>0.318156</td>\n",
       "      <td>0.681844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12487</th>\n",
       "      <td>0.182005</td>\n",
       "      <td>0.817995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12488</th>\n",
       "      <td>0.00147527</td>\n",
       "      <td>0.998525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12489</th>\n",
       "      <td>0.500038</td>\n",
       "      <td>0.499962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12490</th>\n",
       "      <td>0.784035</td>\n",
       "      <td>0.215965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12491</th>\n",
       "      <td>0.737708</td>\n",
       "      <td>0.262292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12492</th>\n",
       "      <td>0.525019</td>\n",
       "      <td>0.474981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12493</th>\n",
       "      <td>0.970344</td>\n",
       "      <td>0.0296556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12494</th>\n",
       "      <td>0.657329</td>\n",
       "      <td>0.342671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12495</th>\n",
       "      <td>0.416442</td>\n",
       "      <td>0.583558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12496</th>\n",
       "      <td>0.287663</td>\n",
       "      <td>0.712337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12497</th>\n",
       "      <td>0.544478</td>\n",
       "      <td>0.455522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12498</th>\n",
       "      <td>0.750915</td>\n",
       "      <td>0.249085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12499</th>\n",
       "      <td>0.491285</td>\n",
       "      <td>0.508715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0            1\n",
       "doc                           \n",
       "0        0.484133     0.515867\n",
       "1        0.573073     0.426927\n",
       "2        0.261172     0.738828\n",
       "3       0.0482365     0.951763\n",
       "4        0.536646     0.463354\n",
       "5        0.335723     0.664277\n",
       "6        0.527615     0.472385\n",
       "7       0.0064288     0.993571\n",
       "8        0.594172     0.405828\n",
       "9        0.992335   0.00766541\n",
       "10        0.29742      0.70258\n",
       "11       0.464517     0.535483\n",
       "12       0.270625     0.729375\n",
       "13       0.808919     0.191081\n",
       "14      0.0987737     0.901226\n",
       "15       0.665317     0.334683\n",
       "16      0.0963713     0.903629\n",
       "17        0.68365      0.31635\n",
       "18        0.35162      0.64838\n",
       "19        0.86684      0.13316\n",
       "20       0.474654     0.525346\n",
       "21         0.4601       0.5399\n",
       "22       0.674869     0.325131\n",
       "23       0.939341    0.0606587\n",
       "24       0.111452     0.888548\n",
       "25       0.994973   0.00502709\n",
       "26       0.992251   0.00774896\n",
       "27       0.421071     0.578929\n",
       "28       0.443014     0.556986\n",
       "29              1  7.93681e-12\n",
       "...           ...          ...\n",
       "12470    0.650802     0.349198\n",
       "12471    0.758446     0.241554\n",
       "12472    0.331747     0.668253\n",
       "12473   0.0944917     0.905508\n",
       "12474      0.6507       0.3493\n",
       "12475    0.193575     0.806425\n",
       "12476    0.388473     0.611527\n",
       "12477    0.216336     0.783664\n",
       "12478    0.998954   0.00104552\n",
       "12479    0.326922     0.673078\n",
       "12480    0.994521   0.00547858\n",
       "12481     0.24166      0.75834\n",
       "12482    0.327032     0.672968\n",
       "12483    0.299782     0.700218\n",
       "12484    0.570366     0.429634\n",
       "12485    0.132481     0.867519\n",
       "12486    0.318156     0.681844\n",
       "12487    0.182005     0.817995\n",
       "12488  0.00147527     0.998525\n",
       "12489    0.500038     0.499962\n",
       "12490    0.784035     0.215965\n",
       "12491    0.737708     0.262292\n",
       "12492    0.525019     0.474981\n",
       "12493    0.970344    0.0296556\n",
       "12494    0.657329     0.342671\n",
       "12495    0.416442     0.583558\n",
       "12496    0.287663     0.712337\n",
       "12497    0.544478     0.455522\n",
       "12498    0.750915     0.249085\n",
       "12499    0.491285     0.508715\n",
       "\n",
       "[12500 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(probs, open('probs.p', 'wb'))\n",
    "pickle.dump(docs, open('docs.p','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A numpy array of ones for the number of docs that we're predicting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = np.ones((probs.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12500,)\n"
     ]
    }
   ],
   "source": [
    "print np.shape(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions[np.where(probs.iloc[:,0] > 0.5)] = 0 # The first column is the negative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  0.,  1., ...,  0.,  0.,  1.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12500,)\n",
      "(12500,)\n"
     ]
    }
   ],
   "source": [
    "print predictions.shape\n",
    "print test1[\"sentiment\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.86696\n"
     ]
    }
   ],
   "source": [
    "print np.size(np.where(predictions == test1[\"sentiment\"]))*1./np.size(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(predictions, open('predictions.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### probsentences was created by commenting out the \"prob = prob.groupby(\"doc\").mean()\" line in the function and then getting probs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(probssentences, open('probssentences.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5986</td>\n",
       "      <td>0.4014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00792023</td>\n",
       "      <td>0.99208</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.369248</td>\n",
       "      <td>0.630752</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.445318</td>\n",
       "      <td>0.554682</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.999581</td>\n",
       "      <td>0.000419095</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00449226</td>\n",
       "      <td>0.995508</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.918007</td>\n",
       "      <td>0.0819928</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.55223</td>\n",
       "      <td>0.44777</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.986808</td>\n",
       "      <td>0.0131918</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.721142</td>\n",
       "      <td>0.278858</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.111639</td>\n",
       "      <td>0.888361</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.717194</td>\n",
       "      <td>0.282805</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0193013</td>\n",
       "      <td>0.980699</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0159692</td>\n",
       "      <td>0.984031</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0224181</td>\n",
       "      <td>0.977582</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.986999</td>\n",
       "      <td>0.0130013</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0593864</td>\n",
       "      <td>0.940614</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0795348</td>\n",
       "      <td>0.920465</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.00578835</td>\n",
       "      <td>0.994212</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.00138163</td>\n",
       "      <td>0.998618</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0            1 doc\n",
       "0       0.5986       0.4014   0\n",
       "1   0.00792023      0.99208   0\n",
       "2     0.369248     0.630752   0\n",
       "3     0.445318     0.554682   0\n",
       "4     0.999581  0.000419095   0\n",
       "5   0.00449226     0.995508   1\n",
       "6     0.918007    0.0819928   1\n",
       "7      0.55223      0.44777   1\n",
       "8     0.986808    0.0131918   1\n",
       "9     0.721142     0.278858   1\n",
       "10    0.111639     0.888361   1\n",
       "11    0.717194     0.282805   1\n",
       "12   0.0193013     0.980699   2\n",
       "13   0.0159692     0.984031   2\n",
       "14   0.0224181     0.977582   2\n",
       "15    0.986999    0.0130013   2\n",
       "16   0.0593864     0.940614   3\n",
       "17   0.0795348     0.920465   3\n",
       "18  0.00578835     0.994212   3\n",
       "19  0.00138163     0.998618   4"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probssentences[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
