{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform on subset of movie reviews (to compare to abstracts). With cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from xml.etree.ElementTree import Element\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk.data\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "from copy import deepcopy\n",
    "import datetime\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/sarahwie/Documents/pubmed-nlp-research/DeepLearningMovies_datasets/')\n",
    "from KaggleWord2VecUtility import KaggleWord2VecUtility\n",
    "DATADIR='/home/sarahwie/Documents/pubmed-nlp-research/DeepLearningMovies_datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read data from files\n",
    "train = pd.read_csv( os.path.join(DATADIR, 'labeledTrainData.tsv'), header=0, delimiter=\"\\t\", quoting=3 )\n",
    "unlabeled_train = pd.read_csv( os.path.join(DATADIR, \"unlabeledTrainData.tsv\"), header=0,  delimiter=\"\\t\", quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get training and testing sets using 5-fold stratified cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#subset dataset to about the same size as the journal sample we were using\n",
    "df = train.sample(422)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "docprob takes two lists\n",
    "* docs: a list of documents, each of which is a list of sentences\n",
    "* models: the candidate word2vec models (each potential class)\n",
    "\n",
    "it returns the array of class probabilities.  Everything is done in-memory.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd # for quick summing within doc\n",
    "\n",
    "def docprob(docs, mods):\n",
    "    # score() takes a list [s] of sentences here; could also be a sentence generator\n",
    "    sentlist = [s for d in docs for s in d]\n",
    "    # the log likelihood of each sentence in this review under each w2v representation\n",
    "    llhd = np.array( [ m.score(sentlist, len(sentlist)) for m in mods ] )\n",
    "    # now exponentiate to get likelihoods, \n",
    "    lhd = np.exp(llhd - llhd.max(axis=0)) # subtract row max to avoid numeric overload\n",
    "    # normalize across models (stars) to get sentence-star probabilities\n",
    "    #all this transposing business does is make it so the total probability of a word \n",
    "    #   equals 1 between the 2 arrays (positive prob and negative).\n",
    "    #and the pandas data frame just puts everything into rows/columns format for easy viz\n",
    "    prob = pd.DataFrame( (lhd/lhd.sum(axis=0)).transpose() )\n",
    "    # and finally average the sentence probabilities to get the review probability\n",
    "    prob[\"doc\"] = [i for i,d in enumerate(docs) for s in d]\n",
    "    prob = prob.groupby(\"doc\").mean()\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from unlabeled set\n",
      "('ROUND', 1)\n",
      "Parsing sentences from training set\n",
      "Parsing sentences from training set\n",
      "Parsing sentences from unlabeled set\n",
      "Building and training w2v models\n",
      "Parsing test sentences\n",
      "scoring test set\n",
      "[ 0.  0.  0.  0.  1.  1.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  1.  0.\n",
      "  0.  1.  0.  1.  0.  1.  0.  1.  0.  0.  1.  1.  0.  1.  0.  0.  1.  0.\n",
      "  1.  0.  0.  0.  0.  1.  1.  0.  1.  0.  1.  0.  1.  1.  1.  0.  1.  1.\n",
      "  0.  1.  0.  0.  1.  1.  1.  0.  1.  0.  0.  0.  1.  0.  1.  1.  0.  0.\n",
      "  1.  0.  1.  1.  0.  1.  0.  1.  1.  1.  1.  0.  1.]\n",
      "0.657982261641\n",
      "('ROUND', 2)\n",
      "Parsing sentences from training set\n",
      "Parsing sentences from training set\n",
      "Parsing sentences from unlabeled set\n",
      "Building and training w2v models\n",
      "Parsing test sentences\n",
      "scoring test set\n",
      "[ 0.  1.  1.  0.  1.  1.  1.  1.  0.  0.  0.  0.  1.  1.  1.  0.  1.  0.\n",
      "  1.  0.  1.  1.  1.  0.  1.  1.  1.  1.  1.  1.  0.  0.  1.  0.  1.  1.\n",
      "  0.  1.  1.  1.  0.  1.  1.  0.  1.  0.  1.  0.  1.  0.  1.  1.  1.  0.\n",
      "  1.  0.  1.  0.  1.  0.  0.  1.  1.  0.  1.  0.  1.  0.  0.  1.  0.  1.\n",
      "  0.  0.  1.  0.  0.  0.  0.  0.  1.  1.  1.  0.  1.]\n",
      "0.614190687361\n",
      "('ROUND', 3)\n",
      "Parsing sentences from training set\n",
      "Parsing sentences from training set\n",
      "Parsing sentences from unlabeled set\n",
      "Building and training w2v models\n",
      "Parsing test sentences\n",
      "scoring test set\n",
      "[ 0.  1.  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  1.  1.  1.  1.  1.  1.\n",
      "  1.  0.  1.  0.  1.  1.  1.  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  1.  0.  1.  1.  1.  1.  1.  0.\n",
      "  1.  1.  1.  1.  0.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "0.601441241685\n",
      "('ROUND', 4)\n",
      "Parsing sentences from training set\n",
      "Parsing sentences from training set\n",
      "Parsing sentences from unlabeled set\n",
      "Building and training w2v models\n",
      "Parsing test sentences\n",
      "scoring test set\n",
      "[ 1.  1.  0.  0.  1.  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  0.  1.  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  0.  1.  1.  0.  1.  1.  1.  1.  1.  1.  1.  0.  1.  1.\n",
      "  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "0.568633011912\n",
      "('ROUND', 5)\n",
      "Parsing sentences from training set\n",
      "Parsing sentences from training set\n",
      "Parsing sentences from unlabeled set\n",
      "Building and training w2v models\n",
      "Parsing test sentences\n",
      "scoring test set\n",
      "[ 1.  1.  1.  0.  1.  1.  1.  1.  1.  0.  1.  0.  1.  1.  1.  1.  0.  0.\n",
      "  0.  1.  1.  0.  1.  1.  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.\n",
      "  1.  1.  1.  1.  1.  0.  0.  1.  1.  1.  1.  1.  0.  1.  0.  0.  1.  1.\n",
      "  1.  0.  1.  1.  1.  1.  1.  1.  0.  1.  1.  0.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  0.  1.  1.  1.]\n",
      "0.492151162791\n",
      "('average of 5 rotations:', 0.58687967307791478)\n",
      "0:18:10\n"
     ]
    }
   ],
   "source": [
    "a = datetime.datetime.now().replace(microsecond=0)\n",
    "\n",
    "#5-fold stratified cross validation\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "#because no validation set, 4/5 of values go to train and 1/5 to test\n",
    "#is this too high?***\n",
    "#even though we shuffle, not as randomly distributed as the former method was\n",
    "y = df['sentiment'].values\n",
    "skf = StratifiedKFold(y, n_folds=5, shuffle=True)\n",
    "\n",
    "#get all unlabelled sentences (doesn't change through each loop of CV)\n",
    "sentences_unlabelled = []  # Initialize an empty list of sentences\n",
    "print \"Parsing sentences from unlabeled set\"\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences_unlabelled += KaggleWord2VecUtility.review_to_sentences(review, tokenizer)\n",
    "\n",
    "i = 1\n",
    "avg = []\n",
    "for train_index, test_index in skf:\n",
    "    print(\"ROUND\", i)\n",
    "    i = i + 1\n",
    "    #use the indexes to subset the df pandas dataframe\n",
    "    train1, test1 = df.iloc[train_index], df.iloc[test_index]\n",
    "    \n",
    "    # ****** Split the training set into clean sentences\n",
    "    #\n",
    "    sentences_pos = []  # Initialize an empty list of sentences\n",
    "    sentences_neg = []  # Initialize an empty list of sentences\n",
    "\n",
    "    #here change to include all journal name labels of positive and negative\n",
    "    inxs_pos = np.where(train1['sentiment'] == 1)[0].tolist()\n",
    "    inxs_neg = np.where(train1['sentiment'] == 0)[0].tolist()\n",
    "\n",
    "    print \"Parsing sentences from training set\"\n",
    "    for inx in inxs_pos:\n",
    "        review = train1[\"review\"].iloc[inx]\n",
    "        sentences_pos += KaggleWord2VecUtility.review_to_sentences(review, tokenizer)\n",
    "\n",
    "    for inx in inxs_neg:\n",
    "        review = train1[\"review\"].iloc[inx]\n",
    "        sentences_neg += KaggleWord2VecUtility.review_to_sentences(review, tokenizer) \n",
    "        \n",
    "    # ****** Split the labeled and unlabeled training sets into clean sentences\n",
    "    #\n",
    "    sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "    print \"Parsing sentences from training set\"\n",
    "    for review in train1[\"review\"]:\n",
    "        sentences += KaggleWord2VecUtility.review_to_sentences(review, tokenizer)\n",
    "\n",
    "    print \"Parsing sentences from unlabeled set\"\n",
    "    for review in unlabeled_train[\"review\"]:\n",
    "        sentences += KaggleWord2VecUtility.review_to_sentences(review, tokenizer)\n",
    "    \n",
    "    print \"Building and training w2v models\"\n",
    "    ## create a w2v learner \n",
    "    basemodel = Word2Vec(\n",
    "        workers=multiprocessing.cpu_count(), # use your cores\n",
    "        iter=3, # iter = sweeps of SGD through the data; more is better\n",
    "        hs=1, negative=0 # we only have scoring for the hierarchical softmax setup\n",
    "        )\n",
    "    basemodel.build_vocab(sentences) \n",
    "    \n",
    "    #train models\n",
    "    models = [deepcopy(basemodel) for y in range(2)]\n",
    "    models[0].train(sentences_neg, total_examples=len(sentences_neg) )\n",
    "    models[1].train(sentences_pos, total_examples=len(sentences_pos) )\n",
    "    \n",
    "    print \"Parsing test sentences\"\n",
    "    # read in the test set as a list of a list of words\n",
    "    docs = []\n",
    "    for review in test1[\"review\"]:\n",
    "        docs.append(KaggleWord2VecUtility.review_to_sentences(review, tokenizer))\n",
    "    \n",
    "    print \"scoring test set\"\n",
    "    # get the probs (note we give docprob our test set plus the models)\n",
    "    probs = docprob(docs,models).astype(object)\n",
    "    \n",
    "    predictions = np.ones((probs.shape[0]))\n",
    "    \n",
    "    predictions[np.where(probs.iloc[:,0] > 0.5)] = 0 # The first column is the negative model\n",
    "    print predictions\n",
    "    \n",
    "    score = roc_auc_score(test1[\"sentiment\"], predictions)\n",
    "    #score = np.size(np.where(predictions == test1[\"sentiment\"]))*1./np.size(predictions)\n",
    "    print score\n",
    "    #append to average\n",
    "    avg.append(score)\n",
    "\n",
    "\n",
    "print(\"average of 5 rotations:\", sum(avg)/float(len(avg)))\n",
    "    \n",
    "b = datetime.datetime.now().replace(microsecond=0)\n",
    "print(b-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
