{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: a continuation from Histograms_sentences_scores_feature_selection.ipynb\n",
    "\n",
    "We calculate feature importance off of the test set's features because only the test set's sentences get scored (probssentences). Is this legal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import datetime\n",
    "import numpy as np\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probssentences = pickle.load(open('/home/sarahwie/Documents/pubmed-nlp-research/pickled_objects/probssentences.p', 'rb'))\n",
    "test1 = pickle.load(open('/home/sarahwie/Documents/pubmed-nlp-research/pickled_objects/test1.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = pickle.load(open('/home/sarahwie/Documents/pubmed-nlp-research/pickled_objects/features.p', 'rb'))\n",
    "labels = pickle.load(open('/home/sarahwie/Documents/pubmed-nlp-research/pickled_objects/labels.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 30.10172574   4.54135379   3.89958616   3.5142616    3.63725996\n",
      "   3.78929883   3.66782008   3.77067032   4.92372908  38.15429444]\n"
     ]
    }
   ],
   "source": [
    "#get feature weights for each sentence score from 0 to 1 in bins of 10\n",
    "clf = ExtraTreesClassifier()\n",
    "clf = clf.fit(features, labels)\n",
    "weights = clf.feature_importances_  \n",
    "#Normalize so all are greater than 1\n",
    "weights = weights * 100\n",
    "print weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#is the issue that the ET classifier used is fit on the test data? I only want the weights, not the model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:44\n"
     ]
    }
   ],
   "source": [
    "#Apply to the actual sentence scores\n",
    "a = datetime.datetime.now().replace(microsecond=0)\n",
    "\n",
    "probssentences['0_weighted'] = 9.99\n",
    "probssentences['1_weighted'] = 9.99\n",
    "\n",
    "for i in range(len(probssentences)):\n",
    "    valneg = probssentences.iloc[i][0]\n",
    "    inxneg = 99999\n",
    "    if 0 <= valneg < 0.1:\n",
    "        inxneg = 0\n",
    "    elif 0.1 <= valneg < 0.2:\n",
    "        inxneg = 1\n",
    "    elif 0.2 <= valneg < 0.3:\n",
    "        inxneg = 2\n",
    "    elif 0.3 <= valneg < 0.4:\n",
    "        inxneg = 3\n",
    "    elif 0.4 <= valneg < 0.5:\n",
    "        inxneg = 4\n",
    "    elif 0.5 <= valneg < 0.6:\n",
    "        inxneg = 5\n",
    "    elif 0.6 <= valneg < 0.7:\n",
    "        inxneg = 6\n",
    "    elif 0.7 <= valneg < 0.8:\n",
    "        inxneg = 7\n",
    "    elif 0.8 <= valneg < 0.9:\n",
    "        inxneg = 8\n",
    "    elif 0.9 <= valneg <= 1.0:\n",
    "        inxneg = 9\n",
    "    probssentences.set_value(i, '0_weighted', valneg * weights[inxneg])\n",
    "    \n",
    "    valpos = probssentences.iloc[i][1]\n",
    "    inxpos = 9.9\n",
    "    if 0 <= valpos < 0.1:\n",
    "        inxpos = 0\n",
    "    elif 0.1 <= valpos < 0.2:\n",
    "        inxpos = 1\n",
    "    elif 0.2 <= valpos < 0.3:\n",
    "        inxpos = 2\n",
    "    elif 0.3 <= valpos < 0.4:\n",
    "        inxpos = 3\n",
    "    elif 0.4 <= valpos < 0.5:\n",
    "        inxpos = 4\n",
    "    elif 0.5 <= valpos < 0.6:\n",
    "        inxpos = 5\n",
    "    elif 0.6 <= valpos < 0.7:\n",
    "        inxpos = 6\n",
    "    elif 0.7 <= valpos < 0.8:\n",
    "        inxpos = 7\n",
    "    elif 0.8 <= valpos < 0.9:\n",
    "        inxpos = 8\n",
    "    elif 0.9 <= valpos <= 1.0:\n",
    "        inxpos = 9\n",
    "    probssentences.set_value(i, '1_weighted', valpos * weights[inxpos])\n",
    "    \n",
    "b = datetime.datetime.now().replace(microsecond=0)\n",
    "print b-a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0            1 doc  0_weighted  1_weighted\n",
      "0       0.5986       0.4014   0    2.090120    1.371669\n",
      "1   0.00792023      0.99208   0    0.286979   32.462023\n",
      "2     0.369248     0.630752   0    1.288374    2.317645\n",
      "3     0.445318     0.554682   0    1.521748    1.936771\n",
      "4     0.999581  0.000419095   0   32.707468    0.015185\n",
      "5   0.00449226     0.995508   1    0.162771   32.574188\n",
      "6     0.918007    0.0819928   1   30.038278    2.970906\n",
      "7      0.55223      0.44777   1    1.928212    1.530124\n",
      "8     0.986808    0.0131918   1   32.289531    0.477988\n",
      "9     0.721142     0.278858   1    2.790239    1.073478\n",
      "10    0.111639     0.888361   1    0.524421    4.047701\n",
      "11    0.717194     0.282805   1    2.774965    1.088675\n",
      "12   0.0193013     0.980699   2    0.699358   32.089618\n",
      "13   0.0159692     0.984031   2    0.578622   32.198651\n",
      "14   0.0224181     0.977582   2    0.812290   31.987635\n",
      "15    0.986999    0.0130013   2   32.295764    0.471085\n",
      "16   0.0593864     0.940614   3    2.151792   30.777987\n",
      "17   0.0795348     0.920465   3    2.881841   30.118707\n",
      "18  0.00578835     0.994212   3    0.209733   32.531778\n",
      "19  0.00138163     0.998618   4    0.050062   32.675972\n"
     ]
    }
   ],
   "source": [
    "print probssentences[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'probssentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-614983f5fd5f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprobssentences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"doc\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'probssentences' is not defined"
     ]
    }
   ],
   "source": [
    "prob = probssentences.groupby(\"doc\").mean()\n",
    "print prob[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12500,)\n"
     ]
    }
   ],
   "source": [
    "preds = np.ones((prob.shape[0]))\n",
    "print preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds[np.where(prob.iloc[:,0] >  prob.iloc[:,1])] = 0 # The first column is the negative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12500,)\n",
      "(12500,)\n"
     ]
    }
   ],
   "source": [
    "print preds.shape\n",
    "print test1[\"sentiment\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.86848\n"
     ]
    }
   ],
   "source": [
    "print np.size(np.where(preds == test1[\"sentiment\"]))*1./np.size(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.868160030313\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print roc_auc_score(test1[\"sentiment\"], preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Especially considering the weights are fit from the test data itself, we would expect to see a big increase in performance, which we're not getting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why? :/ could have something to do with the fact that we fit a classifier on the test data which is pretty wacky. Or could be the way the weights were applied to the sentence scores (unlikely). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
