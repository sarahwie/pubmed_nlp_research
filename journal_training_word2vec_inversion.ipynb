{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get subset of MUSC records where journals equal those two chosen.\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.etree.ElementTree import Element\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#on VM\n",
    "os.chdir('/mnt/mypartition/Desktop2/pubmed_nlp_research/DeepLearningMovies_datasets/')\n",
    "import KaggleWord2VecUtility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#on local:\n",
    "import sys\n",
    "sys.path.append('/home/sarahwie/Documents/pubmed-nlp-research/DeepLearningMovies_datasets/')\n",
    "from KaggleWord2VecUtility import KaggleWord2VecUtility\n",
    "DATADIR='/home/sarahwie/Documents/pubmed-nlp-research/DeepLearningMovies_datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one-time run\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting subset based on 2 journals (with abstracts and no Unicode encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree = ET.parse('/mnt/mypartition/Desktop2/testXMLwrite.xml')\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#using this file for our example\n",
    "xml_file = ET.parse('/home/sarahwie/Documents/zip/subset/zip/medline16n0189.xml')\n",
    "root = xml_file.getroot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code subsets the records down to those with the titles we are looking for (as well as those with abstracts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Can delete I think for real run\n",
    "#make new XML file\n",
    "rootNew = Element('MedlineCitationSet')\n",
    "children = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'unicode'>\n",
      "<type 'unicode'>\n"
     ]
    }
   ],
   "source": [
    "#can delete I think for real run\n",
    "#replace titles here with those we're using\n",
    "#titles = ['Physical review letters', 'Stroke; a journal of cerebral circulation']\n",
    "titles = ['The Journal of biological chemistry','Brain research']\n",
    "\n",
    "i = 0\n",
    "for record in root.findall('MedlineCitation'):\n",
    "    title = record.find('Article').find('Journal').find('Title').text\n",
    "    \n",
    "    if title in titles:\n",
    "        #for test run, also check if abstract\n",
    "        try:\n",
    "            abstract = record.find('Article').find('Abstract').find('AbstractText').text\n",
    "            if type(abstract) != str:\n",
    "                i = i + 1\n",
    "                print type(abstract)\n",
    "            #append to subset file\n",
    "            children.append(record)\n",
    "        except AttributeError:\n",
    "            #dont include if no abstract\n",
    "            #something random to hold place of indent:\n",
    "            x = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rootNew.extend(children)\n",
    "\n",
    "#create tree and write to file\n",
    "tree = ET.ElementTree(rootNew)\n",
    "tree.write(\"subsetTwoJournals.xml\")\n",
    "        \n",
    "pickle.dump(rootNew, open('rootTwoJournals.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarahwie/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:3: DeprecationWarning: This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "#Can delete I think for real run\n",
    "#size of new subset\n",
    "print len(rootNew.getchildren())\n",
    "#verify that this is the correct number: yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting training and testing sets as pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sentiment                              journal  \\\n",
      "0            0                       Brain research   \n",
      "1            0                       Brain research   \n",
      "2            0                       Brain research   \n",
      "3            0                       Brain research   \n",
      "4            0                       Brain research   \n",
      "5            0                       Brain research   \n",
      "6            0                       Brain research   \n",
      "7            0                       Brain research   \n",
      "8            0                       Brain research   \n",
      "9            0                       Brain research   \n",
      "10           0                       Brain research   \n",
      "11           0                       Brain research   \n",
      "12           0                       Brain research   \n",
      "13           0                       Brain research   \n",
      "14           0                       Brain research   \n",
      "15           1  The Journal of biological chemistry   \n",
      "16           1  The Journal of biological chemistry   \n",
      "17           1  The Journal of biological chemistry   \n",
      "18           1  The Journal of biological chemistry   \n",
      "19           1  The Journal of biological chemistry   \n",
      "20           1  The Journal of biological chemistry   \n",
      "21           1  The Journal of biological chemistry   \n",
      "22           1  The Journal of biological chemistry   \n",
      "23           1  The Journal of biological chemistry   \n",
      "24           1  The Journal of biological chemistry   \n",
      "25           1  The Journal of biological chemistry   \n",
      "26           1  The Journal of biological chemistry   \n",
      "27           1  The Journal of biological chemistry   \n",
      "28           1  The Journal of biological chemistry   \n",
      "29           1  The Journal of biological chemistry   \n",
      "..         ...                                  ...   \n",
      "293          1  The Journal of biological chemistry   \n",
      "294          1  The Journal of biological chemistry   \n",
      "295          1  The Journal of biological chemistry   \n",
      "296          1  The Journal of biological chemistry   \n",
      "297          1  The Journal of biological chemistry   \n",
      "298          1  The Journal of biological chemistry   \n",
      "299          1  The Journal of biological chemistry   \n",
      "300          1  The Journal of biological chemistry   \n",
      "301          1  The Journal of biological chemistry   \n",
      "302          1  The Journal of biological chemistry   \n",
      "303          1  The Journal of biological chemistry   \n",
      "304          1  The Journal of biological chemistry   \n",
      "305          1  The Journal of biological chemistry   \n",
      "306          1  The Journal of biological chemistry   \n",
      "307          1  The Journal of biological chemistry   \n",
      "308          1  The Journal of biological chemistry   \n",
      "309          1  The Journal of biological chemistry   \n",
      "310          1  The Journal of biological chemistry   \n",
      "311          1  The Journal of biological chemistry   \n",
      "312          1  The Journal of biological chemistry   \n",
      "313          1  The Journal of biological chemistry   \n",
      "314          1  The Journal of biological chemistry   \n",
      "315          1  The Journal of biological chemistry   \n",
      "316          1  The Journal of biological chemistry   \n",
      "317          1  The Journal of biological chemistry   \n",
      "318          1  The Journal of biological chemistry   \n",
      "319          1  The Journal of biological chemistry   \n",
      "320          1  The Journal of biological chemistry   \n",
      "321          1  The Journal of biological chemistry   \n",
      "322          1  The Journal of biological chemistry   \n",
      "\n",
      "                                              abstract  \n",
      "0    The dorsolateral quadrant of the lateral septa...  \n",
      "1    Changes of the internal (d) and external diame...  \n",
      "2    The neuranatomical location and pharmacologica...  \n",
      "3    The neuroanatomical organization of the effere...  \n",
      "4    The role of nerve growth factor (NGF) in the d...  \n",
      "5    The recurrence every 24 h of glucocorticoid el...  \n",
      "6    Dopamine-beta-hydroxylase (DBH) and norepineph...  \n",
      "7    Locally applied kainic acid was used in order ...  \n",
      "8    Using extracellular unit recording and microio...  \n",
      "9    The organization of globus pallidus (GP) proje...  \n",
      "10   The development of opiate mechanisms in the gu...  \n",
      "11   A regular slow wave theta rhythm can be record...  \n",
      "12   Fibers descending in the lateral columns (LC) ...  \n",
      "13   The incorporation of 35SO4(2-) and [3H]galacto...  \n",
      "14   Selectivity in the reinnervation of denervated...  \n",
      "15   The location of the functionally important -SH...  \n",
      "16   The pattern of proteins synthesized by chicken...  \n",
      "17   Preferential labeling of COOH-terminal sequenc...  \n",
      "18   Urinary acidification in the turtle urinary bl...  \n",
      "19   It has been demonstrated that the reaction of ...  \n",
      "20   The tetrameric form of a Desulfovibrio gigas f...  \n",
      "21   A new pigment from Halobacterium halobium has ...  \n",
      "22   The binding reaction of NADH to liver alcohol ...  \n",
      "23   The pokeweed antiviral protein and the toxin r...  \n",
      "24   Transport of methotrexate by L1210 sensitive a...  \n",
      "25   The rate of transport of alpha-aminoisobutyric...  \n",
      "26   Surface aromatic residues of bovine alpha-lact...  \n",
      "27   Cvc- mutants of Escherichia coli are deficient...  \n",
      "28   Total glycopeptides from human K-562 cells, la...  \n",
      "29   Liver glycogen phosphorylase associated with t...  \n",
      "..                                                 ...  \n",
      "293  Saturation analysis of equilibrium binding of ...  \n",
      "294  In the rat hepatocyte, whether freshly separat...  \n",
      "295  The first quantitative measurements of the eff...  \n",
      "296  Kinetic studies on the unfolding of pepsinogen...  \n",
      "297  Cyanogen bromide fragments of alpha 1-protease...  \n",
      "298  Human alpha 1-protease inhibitor has three oli...  \n",
      "299  Previous observations suggested that pyridoxal...  \n",
      "300  Cytosol prepared from homogenates of bone from...  \n",
      "301  In previous studies in this laboratory, highly...  \n",
      "302  Tryptophan hydroxylase (EC 1.14.16.4) from rat...  \n",
      "303  The reactions of human hemoglobin with a serie...  \n",
      "304  The reactions of 13 isonitriles with deoxyhemo...  \n",
      "305  An enzyme present in rabbit liver microsomes h...  \n",
      "306  Several fluorinated derivatives of p-hydroxybe...  \n",
      "307  An eight-iron, eight-sulfur ferredoxin from Rh...  \n",
      "308  A dianionic spin label, 1-L-glutamate-5-N-(1-o...  \n",
      "309  Endogenous lactose-binding proteins from adult...  \n",
      "310  The specificity of the cyclic AMP-dependent pr...  \n",
      "311  An incomplete precursor of lipid A produced by...  \n",
      "312  The incomplete lipid A precursor produced by a...  \n",
      "313  The effect of cerulenin on conversion of an ac...  \n",
      "314  The intrinsic isotope effect on the reduction ...  \n",
      "315  Two human breast cancer cell lines (T-47D and ...  \n",
      "316  A wide variety of double-stranded DNA template...  \n",
      "317  Late in the life cycle of the single-stranded ...  \n",
      "318  A DNA-dependent RNA polymerase has been purifi...  \n",
      "319  We have obtained evidence in vivo for the intr...  \n",
      "320  The major aminoacyl-tRNA synthetase complex (t...  \n",
      "321  A DNA-dependent RNA polymerase has been extrac...  \n",
      "322  The irreversible inactivation by heat of the F...  \n",
      "\n",
      "[323 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#convert XML subset to pandas dataframe\n",
    "def iter_docs(author):\n",
    "    author_attr = author.attrib\n",
    "    #titles_pos = ['Physical review letters']\n",
    "    #titles_neg = ['Stroke; a journal of cerebral circulation']\n",
    "    titles_pos = ['The Journal of biological chemistry']\n",
    "    titles_neg = ['Brain research']\n",
    "    for record in author.findall('MedlineCitation'):\n",
    "        doc_dict = author_attr.copy()\n",
    "        #doc_dict.update(record.attrib)\n",
    "        title = record.find('Article').find('Journal').find('Title').text\n",
    "        doc_dict['journal'] = title\n",
    "        abstract = record.find('Article').find('Abstract').find('AbstractText').text\n",
    "        #make sure all abstracts are in string format to avoid later sentence parsing issues (69 in test run were unicode)\n",
    "        if type(abstract) != str:\n",
    "            abstract = abstract.encode('utf8')\n",
    "        doc_dict['abstract'] = abstract\n",
    "        #add sentiment label based on journal title\n",
    "        if title in titles_neg:\n",
    "            doc_dict['sentiment'] = 0\n",
    "        elif title in titles_pos:\n",
    "            doc_dict['sentiment'] = 1\n",
    "        yield doc_dict\n",
    "        \n",
    "df = pd.DataFrame(list(iter_docs(rootNew)))\n",
    "df = df[['sentiment', 'journal', 'abstract']]\n",
    "print df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brain research\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>journal</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Brain research</td>\n",
       "      <td>The dorsolateral quadrant of the lateral septa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Brain research</td>\n",
       "      <td>Changes of the internal (d) and external diame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Brain research</td>\n",
       "      <td>The neuranatomical location and pharmacologica...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment         journal  \\\n",
       "0          0  Brain research   \n",
       "1          0  Brain research   \n",
       "2          0  Brain research   \n",
       "\n",
       "                                            abstract  \n",
       "0  The dorsolateral quadrant of the lateral septa...  \n",
       "1  Changes of the internal (d) and external diame...  \n",
       "2  The neuranatomical location and pharmacologica...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print df.loc[1][1]\n",
    "#don't know why no quotation marks like when using pandas.to_csv in original w2v inversion\n",
    "#but seems to work fine so ignore for now\n",
    "df.loc[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pickle.load(open('/home/sarahwie/Documents/pubmed-nlp-research/pickled_objects/dfJournals.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#split into train and test\n",
    "#since so little data, consider using cross-validation?\n",
    "fracTrain = 0.75\n",
    "nSamples = df.shape[0]\n",
    "order = np.random.permutation(nSamples) # come up with a random ordering\n",
    "splitIndex = int(np.round(nSamples*fracTrain))\n",
    "train1 = df.ix[order[:splitIndex],:]\n",
    "test1 = df.ix[order[splitIndex:],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(316, 3)\n",
      "(106, 3)\n"
     ]
    }
   ],
   "source": [
    "print train1.shape\n",
    "print test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sentiment                              journal  \\\n",
      "360          1  The Journal of biological chemistry   \n",
      "\n",
      "                                              abstract  \n",
      "360  The reaction mechanism of melilotate hydroxyla...  \n"
     ]
    }
   ],
   "source": [
    "#print first record in test set (note has different index value, indexed instead by location (pos. 0))\n",
    "print test1.iloc[[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Parsing Sentences & Building Word2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gets a list of positive review indexes and negative review indexes. Takes those respective reviews and parses them into sentences and appends to a list of positive and negative sentences, respectively. Also does the same for the unlabelled training sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    }
   ],
   "source": [
    "# ****** Split the training set into clean sentences\n",
    "#\n",
    "sentences_pos = []  # Initialize an empty list of sentences\n",
    "sentences_neg = []  # Initialize an empty list of sentences\n",
    "\n",
    "#here change to include all journal name labels of positive and negative\n",
    "inxs_pos = np.where(train1['sentiment'] == 1)[0].tolist()\n",
    "inxs_neg = np.where(train1['sentiment'] == 0)[0].tolist()\n",
    "\n",
    "print \"Parsing sentences from training set\"\n",
    "for inx in inxs_pos:\n",
    "    review = train1[\"abstract\"].iloc[inx]\n",
    "    sentences_pos += KaggleWord2VecUtility.review_to_sentences(review, tokenizer)\n",
    "\n",
    "for inx in inxs_neg:\n",
    "    review = train1[\"abstract\"].iloc[inx]\n",
    "    sentences_neg += KaggleWord2VecUtility.review_to_sentences(review, tokenizer)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'we', u'report', u'the', u'first', u'observation', u'of', u'diffractive', u'j', u'psi', u'mu', u'mu', u'production', u'in', u'pp', u'collisions', u'at', u'root', u'square', u's', u'tev'], [u'diffractive', u'events', u'are', u'identified', u'by', u'their', u'rapidity', u'gap', u'signature']]\n"
     ]
    }
   ],
   "source": [
    "print sentences_pos[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same with all sentences to build the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    }
   ],
   "source": [
    "# ****** Split the labeled and unlabeled training sets into clean sentences\n",
    "#\n",
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print \"Parsing sentences from training set\"\n",
    "for review in train1[\"abstract\"]:\n",
    "    sentences += KaggleWord2VecUtility.review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Build Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=0, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "\n",
    "## create a w2v learner \n",
    "basemodel = Word2Vec(\n",
    "    workers=multiprocessing.cpu_count(), # use your cores\n",
    "    iter=3, # iter = sweeps of SGD through the data; more is better\n",
    "    hs=1, negative=0 # we only have scoring for the hierarchical softmax setup\n",
    "    )\n",
    "print basemodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "basemodel.build_vocab(sentences) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Word2Vec model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53387"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model 0 corresponds with Stroke\n",
    "#and model 1 with Physical Review Letters\n",
    "\n",
    "from copy import deepcopy\n",
    "models = [deepcopy(basemodel) for i in range(2)]\n",
    "models[0].train(sentences_neg, total_examples=len(sentences_neg) )\n",
    "models[1].train(sentences_pos, total_examples=len(sentences_pos) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Applying inversion on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "docprob takes two lists\n",
    "* docs: a list of documents, each of which is a list of sentences\n",
    "* models: the candidate word2vec models (each potential class)\n",
    "\n",
    "it returns the array of class probabilities.  Everything is done in-memory.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd # for quick summing within doc\n",
    "\n",
    "def docprob(docs, mods):\n",
    "    # score() takes a list [s] of sentences here; could also be a sentence generator\n",
    "    sentlist = [s for d in docs for s in d]\n",
    "    # the log likelihood of each sentence in this review under each w2v representation\n",
    "    llhd = np.array( [ m.score(sentlist, len(sentlist)) for m in mods ] )\n",
    "    # now exponentiate to get likelihoods, \n",
    "    lhd = np.exp(llhd - llhd.max(axis=0)) # subtract row max to avoid numeric overload\n",
    "    # normalize across models (stars) to get sentence-star probabilities\n",
    "    #all this transposing business does is make it so the total probability of a word \n",
    "    #   equals 1 between the 2 arrays (positive prob and negative).\n",
    "    #and the pandas data frame just puts everything into rows/columns format for easy viz\n",
    "    prob = pd.DataFrame( (lhd/lhd.sum(axis=0)).transpose() )\n",
    "    # and finally average the sentence probabilities to get the review probability\n",
    "    prob[\"doc\"] = [i for i,d in enumerate(docs) for s in d]\n",
    "    prob = prob.groupby(\"doc\").mean()\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in the test set as a list of a list of words\n",
    "docs = []\n",
    "for review in test1[\"abstract\"]:\n",
    "    docs.append(KaggleWord2VecUtility.review_to_sentences(review, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'we', u'report', u'a', u'measurement', u'of', u'the', u'lambda', u'b', u'lifetime', u'in', u'the', u'exclusive', u'decay', u'lambda', u'b', u'j', u'psi', u'lambda', u'in', u'pp', u'collisions', u'at', u'square', u'root', u's', u'tev', u'using', u'an', u'integrated', u'luminosity', u'of', u'fb', u'of', u'data', u'collected', u'by', u'the', u'cdf', u'ii', u'detector', u'at', u'the', u'fermilab', u'tevatron'], [u'using', u'fully', u'reconstructed', u'decays', u'we', u'measure', u'tau', u'lambda', u'b', u'stat', u'syst', u'ps'], [u'this', u'is', u'the', u'single', u'most', u'precise', u'measurement', u'of', u'tau', u'lambda', u'b', u'and', u'is', u'sigma', u'higher', u'than', u'the', u'current', u'world', u'average']]\n"
     ]
    }
   ],
   "source": [
    "print docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the probs (note we give docprob our test set plus the models)\n",
    "probs = docprob(docs,models).astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.84574</td>\n",
       "      <td>0.15426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.835586</td>\n",
       "      <td>0.164414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.754181</td>\n",
       "      <td>0.245819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.986099</td>\n",
       "      <td>0.0139012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.978837</td>\n",
       "      <td>0.0211629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.916456</td>\n",
       "      <td>0.0835435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.989629</td>\n",
       "      <td>0.0103714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.939087</td>\n",
       "      <td>0.0609127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.797745</td>\n",
       "      <td>0.202255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.774109</td>\n",
       "      <td>0.225891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.853992</td>\n",
       "      <td>0.146008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.973365</td>\n",
       "      <td>0.0266351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.871685</td>\n",
       "      <td>0.128315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.766684</td>\n",
       "      <td>0.233316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.905282</td>\n",
       "      <td>0.0947184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.86504</td>\n",
       "      <td>0.13496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.892877</td>\n",
       "      <td>0.107123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.912309</td>\n",
       "      <td>0.0876913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.762149</td>\n",
       "      <td>0.237851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.850196</td>\n",
       "      <td>0.149804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.965057</td>\n",
       "      <td>0.0349434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.820994</td>\n",
       "      <td>0.179006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.645599</td>\n",
       "      <td>0.354401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.892844</td>\n",
       "      <td>0.107156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.787767</td>\n",
       "      <td>0.212233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.929463</td>\n",
       "      <td>0.070537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.867042</td>\n",
       "      <td>0.132958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.864789</td>\n",
       "      <td>0.135211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.708147</td>\n",
       "      <td>0.291853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.830299</td>\n",
       "      <td>0.169701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.716007</td>\n",
       "      <td>0.283993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.869791</td>\n",
       "      <td>0.130209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.698584</td>\n",
       "      <td>0.301416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.905267</td>\n",
       "      <td>0.0947332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.89698</td>\n",
       "      <td>0.10302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.888316</td>\n",
       "      <td>0.111684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.892615</td>\n",
       "      <td>0.107385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.899615</td>\n",
       "      <td>0.100385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.857996</td>\n",
       "      <td>0.142005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.816158</td>\n",
       "      <td>0.183842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.985008</td>\n",
       "      <td>0.0149922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.936764</td>\n",
       "      <td>0.0632364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.772299</td>\n",
       "      <td>0.227701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.783785</td>\n",
       "      <td>0.216215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.982768</td>\n",
       "      <td>0.0172325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.841404</td>\n",
       "      <td>0.158595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.831834</td>\n",
       "      <td>0.168166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.985013</td>\n",
       "      <td>0.0149869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.716297</td>\n",
       "      <td>0.283703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.938926</td>\n",
       "      <td>0.0610741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.748914</td>\n",
       "      <td>0.251086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.998783</td>\n",
       "      <td>0.00121692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.992326</td>\n",
       "      <td>0.00767358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.982363</td>\n",
       "      <td>0.0176374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.955121</td>\n",
       "      <td>0.0448788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.992835</td>\n",
       "      <td>0.00716466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.930989</td>\n",
       "      <td>0.0690115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.888522</td>\n",
       "      <td>0.111478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.964405</td>\n",
       "      <td>0.0355954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.844799</td>\n",
       "      <td>0.155201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0           1\n",
       "doc                      \n",
       "0     0.84574     0.15426\n",
       "1    0.835586    0.164414\n",
       "2    0.754181    0.245819\n",
       "3    0.986099   0.0139012\n",
       "4    0.978837   0.0211629\n",
       "5    0.916456   0.0835435\n",
       "6    0.989629   0.0103714\n",
       "7    0.939087   0.0609127\n",
       "8    0.797745    0.202255\n",
       "9    0.774109    0.225891\n",
       "10   0.853992    0.146008\n",
       "11   0.973365   0.0266351\n",
       "12   0.871685    0.128315\n",
       "13   0.766684    0.233316\n",
       "14   0.905282   0.0947184\n",
       "15    0.86504     0.13496\n",
       "16   0.892877    0.107123\n",
       "17   0.912309   0.0876913\n",
       "18   0.762149    0.237851\n",
       "19   0.850196    0.149804\n",
       "20   0.965057   0.0349434\n",
       "21   0.820994    0.179006\n",
       "22   0.645599    0.354401\n",
       "23   0.892844    0.107156\n",
       "24   0.787767    0.212233\n",
       "25   0.929463    0.070537\n",
       "26   0.867042    0.132958\n",
       "27   0.864789    0.135211\n",
       "28   0.708147    0.291853\n",
       "29   0.830299    0.169701\n",
       "..        ...         ...\n",
       "76   0.716007    0.283993\n",
       "77   0.869791    0.130209\n",
       "78   0.698584    0.301416\n",
       "79   0.905267   0.0947332\n",
       "80    0.89698     0.10302\n",
       "81   0.888316    0.111684\n",
       "82   0.892615    0.107385\n",
       "83   0.899615    0.100385\n",
       "84   0.857996    0.142005\n",
       "85   0.816158    0.183842\n",
       "86   0.985008   0.0149922\n",
       "87   0.936764   0.0632364\n",
       "88   0.772299    0.227701\n",
       "89   0.783785    0.216215\n",
       "90   0.982768   0.0172325\n",
       "91   0.841404    0.158595\n",
       "92   0.831834    0.168166\n",
       "93   0.985013   0.0149869\n",
       "94   0.716297    0.283703\n",
       "95   0.938926   0.0610741\n",
       "96   0.748914    0.251086\n",
       "97   0.998783  0.00121692\n",
       "98   0.992326  0.00767358\n",
       "99   0.982363   0.0176374\n",
       "100  0.955121   0.0448788\n",
       "101  0.992835  0.00716466\n",
       "102  0.930989   0.0690115\n",
       "103  0.888522    0.111478\n",
       "104  0.964405   0.0355954\n",
       "105  0.844799    0.155201\n",
       "\n",
       "[106 rows x 2 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = np.ones((probs.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106,)\n"
     ]
    }
   ],
   "source": [
    "print np.shape(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions[np.where(probs.iloc[:,0] > 0.5)] = 0 # The first column is the negative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106,)\n",
      "(106,)\n"
     ]
    }
   ],
   "source": [
    "print predictions.shape\n",
    "print test1[\"sentiment\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.481132075472\n"
     ]
    }
   ],
   "source": [
    "print np.size(np.where(predictions == test1[\"sentiment\"]))*1./np.size(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TRAIN:', array([ 59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,\n",
      "        72,  73,  74,  75,  76,  77,  84,  85,  86,  87,  88,  89,  90,\n",
      "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
      "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
      "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
      "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
      "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
      "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
      "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
      "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
      "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
      "       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
      "       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
      "       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
      "       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
      "       299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
      "       312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322]), 'TEST:', array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 78, 79, 80, 81, 82, 83]))\n",
      "('TRAIN:', array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
      "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
      "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
      "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
      "        52,  53,  54,  55,  56,  57,  58,  78,  79,  80,  81,  82,  83,\n",
      "       105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117,\n",
      "       118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130,\n",
      "       131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143,\n",
      "       144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156,\n",
      "       157, 158, 159, 160, 161, 162, 163, 164, 165, 191, 192, 193, 194,\n",
      "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
      "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
      "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
      "       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
      "       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
      "       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
      "       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
      "       299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
      "       312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322]), 'TEST:', array([ 59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,\n",
      "        72,  73,  74,  75,  76,  77,  84,  85,  86,  87,  88,  89,  90,\n",
      "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
      "       104, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177,\n",
      "       178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190]))\n",
      "('TRAIN:', array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
      "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
      "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
      "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
      "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
      "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
      "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
      "       104, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137,\n",
      "       138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150,\n",
      "       151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163,\n",
      "       164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176,\n",
      "       177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189,\n",
      "       190, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
      "       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
      "       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
      "       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
      "       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
      "       299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
      "       312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322]), 'TEST:', array([105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117,\n",
      "       118, 119, 120, 121, 122, 123, 124, 125, 191, 192, 193, 194, 195,\n",
      "       196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208,\n",
      "       209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221,\n",
      "       222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234]))\n",
      "('TRAIN:', array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
      "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
      "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
      "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
      "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
      "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
      "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
      "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
      "       117, 118, 119, 120, 121, 122, 123, 124, 125, 146, 147, 148, 149,\n",
      "       150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162,\n",
      "       163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175,\n",
      "       176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188,\n",
      "       189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201,\n",
      "       202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214,\n",
      "       215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227,\n",
      "       228, 229, 230, 231, 232, 233, 234, 279, 280, 281, 282, 283, 284,\n",
      "       285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297,\n",
      "       298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310,\n",
      "       311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322]), 'TEST:', array([126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138,\n",
      "       139, 140, 141, 142, 143, 144, 145, 235, 236, 237, 238, 239, 240,\n",
      "       241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
      "       254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266,\n",
      "       267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278]))\n",
      "('TRAIN:', array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
      "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
      "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
      "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
      "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
      "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
      "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
      "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
      "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
      "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
      "       143, 144, 145, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175,\n",
      "       176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188,\n",
      "       189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201,\n",
      "       202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214,\n",
      "       215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227,\n",
      "       228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240,\n",
      "       241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
      "       254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266,\n",
      "       267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278]), 'TEST:', array([146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158,\n",
      "       159, 160, 161, 162, 163, 164, 165, 279, 280, 281, 282, 283, 284,\n",
      "       285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297,\n",
      "       298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310,\n",
      "       311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322]))\n"
     ]
    }
   ],
   "source": [
    "#testing cross-validation\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "y = df['sentiment'].values\n",
    "skf = StratifiedKFold(y, n_folds=5)\n",
    "#len(skf)\n",
    "#print(skf)  \n",
    "for train_index, test_index in skf:\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print y_train\n",
    "print y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#issues: need to use the indexes to subset the df pandas dataframe, not y\n",
    "#also: need to get FIVE sets: 3 for training, 1 for testing and 1 for validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Import KaggleWord2VecUtility since didn't work from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "class KaggleWord2VecUtility(object):\n",
    "    \"\"\"KaggleWord2VecUtility is a utility class for processing raw HTML text into segments for further learning\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def review_to_wordlist( review, remove_stopwords=False ):\n",
    "        # Function to convert a document to a sequence of words,\n",
    "        # optionally removing stop words.  Returns a list of words.\n",
    "        #\n",
    "        # 1. Remove HTML\n",
    "        review_text = BeautifulSoup(review).get_text()\n",
    "        #\n",
    "        # 2. Remove non-letters\n",
    "        review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "        #\n",
    "        # 3. Convert words to lower case and split them\n",
    "        words = review_text.lower().split()\n",
    "        #\n",
    "        # 4. Optionally remove stop words (false by default)\n",
    "        if remove_stopwords:\n",
    "            stops = set(stopwords.words(\"english\"))\n",
    "            words = [w for w in words if not w in stops]\n",
    "        #\n",
    "        # 5. Return a list of words\n",
    "        return(words)\n",
    "\n",
    "    # Define a function to split a review into parsed sentences\n",
    "    @staticmethod\n",
    "    def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "        # Function to split a review into parsed sentences. Returns a\n",
    "        # list of sentences, where each sentence is a list of words\n",
    "        #\n",
    "        # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "        raw_sentences = tokenizer.tokenize(review.decode('utf8').strip())\n",
    "        #\n",
    "        # 2. Loop over each sentence\n",
    "        sentences = []\n",
    "        for raw_sentence in raw_sentences:\n",
    "            # If a sentence is empty, skip it\n",
    "            if len(raw_sentence) > 0:\n",
    "                # Otherwise, call review_to_wordlist to get a list of words\n",
    "                sentences.append( KaggleWord2VecUtility.review_to_wordlist( raw_sentence, \\\n",
    "                  remove_stopwords ))\n",
    "        #\n",
    "        # Return the list of sentences (each sentence is a list of words,\n",
    "        # so this returns a list of lists\n",
    "        return sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
