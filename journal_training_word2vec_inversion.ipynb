{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get subset of MUSC records where journals equal those two chosen.\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.etree.ElementTree import Element\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#on VM\n",
    "os.chdir('/mnt/mypartition/Desktop2/pubmed_nlp_research/DeepLearningMovies_datasets/')\n",
    "import KaggleWord2VecUtility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#on local:\n",
    "import sys\n",
    "sys.path.append('/home/sarahwie/Documents/pubmed-nlp-research/DeepLearningMovies_datasets/')\n",
    "from KaggleWord2VecUtility import KaggleWord2VecUtility\n",
    "DATADIR='/home/sarahwie/Documents/pubmed-nlp-research/DeepLearningMovies_datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one-time run\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting subset based on 2 journals (with abstracts and no Unicode encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree = ET.parse('/mnt/mypartition/Desktop2/testXMLwrite.xml')\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#using this file for our example\n",
    "xml_file = ET.parse('/home/sarahwie/Documents/zip/subset/zip/medline16n0189.xml')\n",
    "root = xml_file.getroot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code subsets the records down to those with the titles we are looking for (as well as those with abstracts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Can delete I think for real run\n",
    "#make new XML file\n",
    "rootNew = Element('MedlineCitationSet')\n",
    "children = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'unicode'>\n"
     ]
    }
   ],
   "source": [
    "#can delete I think for real run\n",
    "#replace titles here with those we're using\n",
    "titles = ['Physical review letters', 'Stroke; a journal of cerebral circulation']\n",
    "\n",
    "i = 0\n",
    "for record in root.findall('MedlineCitation'):\n",
    "    title = record.find('Article').find('Journal').find('Title').text\n",
    "    \n",
    "    if title in titles:\n",
    "        #for test run, also check if abstract\n",
    "        #try:\n",
    "        abstract = record.find('Article').find('Abstract').find('AbstractText').text\n",
    "        if type(abstract) != str:\n",
    "            i = i + 1\n",
    "            print type(abstract)\n",
    "        #append to subset file\n",
    "        children.append(record)\n",
    "        #except AttributeError:\n",
    "            #dont include if no abstract\n",
    "            #something random to hold place of indent:\n",
    "            #x = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rootNew.extend(children)\n",
    "\n",
    "#create tree and write to file\n",
    "tree = ET.ElementTree(rootNew)\n",
    "tree.write(\"subsetTwoJournals.xml\")\n",
    "        \n",
    "pickle.dump(rootNew, open('rootTwoJournals.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n"
     ]
    }
   ],
   "source": [
    "print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp-vm/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:3: DeprecationWarning: This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "#Can delete I think for real run\n",
    "#size of new subset\n",
    "print len(rootNew.getchildren())\n",
    "#verify that this is the correct number: yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting training and testing sets as pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sentiment                                    journal  \\\n",
      "0            0  Stroke; a journal of cerebral circulation   \n",
      "1            0  Stroke; a journal of cerebral circulation   \n",
      "2            0  Stroke; a journal of cerebral circulation   \n",
      "3            0  Stroke; a journal of cerebral circulation   \n",
      "4            0  Stroke; a journal of cerebral circulation   \n",
      "5            0  Stroke; a journal of cerebral circulation   \n",
      "6            0  Stroke; a journal of cerebral circulation   \n",
      "7            0  Stroke; a journal of cerebral circulation   \n",
      "8            0  Stroke; a journal of cerebral circulation   \n",
      "9            0  Stroke; a journal of cerebral circulation   \n",
      "10           0  Stroke; a journal of cerebral circulation   \n",
      "11           0  Stroke; a journal of cerebral circulation   \n",
      "12           0  Stroke; a journal of cerebral circulation   \n",
      "13           0  Stroke; a journal of cerebral circulation   \n",
      "14           0  Stroke; a journal of cerebral circulation   \n",
      "15           0  Stroke; a journal of cerebral circulation   \n",
      "16           0  Stroke; a journal of cerebral circulation   \n",
      "17           0  Stroke; a journal of cerebral circulation   \n",
      "18           0  Stroke; a journal of cerebral circulation   \n",
      "19           0  Stroke; a journal of cerebral circulation   \n",
      "20           0  Stroke; a journal of cerebral circulation   \n",
      "21           0  Stroke; a journal of cerebral circulation   \n",
      "22           0  Stroke; a journal of cerebral circulation   \n",
      "23           0  Stroke; a journal of cerebral circulation   \n",
      "24           0  Stroke; a journal of cerebral circulation   \n",
      "25           0  Stroke; a journal of cerebral circulation   \n",
      "26           0  Stroke; a journal of cerebral circulation   \n",
      "27           0  Stroke; a journal of cerebral circulation   \n",
      "28           0  Stroke; a journal of cerebral circulation   \n",
      "29           0  Stroke; a journal of cerebral circulation   \n",
      "..         ...                                        ...   \n",
      "411          1                    Physical review letters   \n",
      "412          1                    Physical review letters   \n",
      "413          0  Stroke; a journal of cerebral circulation   \n",
      "414          0  Stroke; a journal of cerebral circulation   \n",
      "415          0  Stroke; a journal of cerebral circulation   \n",
      "416          0  Stroke; a journal of cerebral circulation   \n",
      "417          0  Stroke; a journal of cerebral circulation   \n",
      "418          0  Stroke; a journal of cerebral circulation   \n",
      "419          1                    Physical review letters   \n",
      "420          0  Stroke; a journal of cerebral circulation   \n",
      "421          0  Stroke; a journal of cerebral circulation   \n",
      "422          0  Stroke; a journal of cerebral circulation   \n",
      "423          0  Stroke; a journal of cerebral circulation   \n",
      "424          1                    Physical review letters   \n",
      "425          0  Stroke; a journal of cerebral circulation   \n",
      "426          0  Stroke; a journal of cerebral circulation   \n",
      "427          1                    Physical review letters   \n",
      "428          1                    Physical review letters   \n",
      "429          1                    Physical review letters   \n",
      "430          1                    Physical review letters   \n",
      "431          0  Stroke; a journal of cerebral circulation   \n",
      "432          0  Stroke; a journal of cerebral circulation   \n",
      "433          0  Stroke; a journal of cerebral circulation   \n",
      "434          0  Stroke; a journal of cerebral circulation   \n",
      "435          0  Stroke; a journal of cerebral circulation   \n",
      "436          0  Stroke; a journal of cerebral circulation   \n",
      "437          0  Stroke; a journal of cerebral circulation   \n",
      "438          1                    Physical review letters   \n",
      "439          0  Stroke; a journal of cerebral circulation   \n",
      "440          0  Stroke; a journal of cerebral circulation   \n",
      "\n",
      "                                              abstract  \n",
      "0    A standardized neurologic assessment scoring i...  \n",
      "1    Proven effective therapy to prevent ischemic d...  \n",
      "2    We describe a case of homozygous sickle cell a...  \n",
      "3    In 27 cats treated to vary arterial serum gluc...  \n",
      "4    We investigated the effect of mild whole-body ...  \n",
      "5    We developed techniques to assess the utility ...  \n",
      "6    We investigated the relation of plasma lipids ...  \n",
      "7    We investigated the effects of multiple episod...  \n",
      "8    We employed fluorocarbon-23 (trifluoromethane)...  \n",
      "9    We measured cerebral intracellular pH using in...  \n",
      "10   Ipsilateral motor or sensory symptoms associat...  \n",
      "11   We used 31P nuclear magnetic resonance spectro...  \n",
      "12   We studied the effect of focal cerebral ischem...  \n",
      "13   A 43-year-old woman suffered a blast-type inju...  \n",
      "14   We sought to determine whether there are racia...  \n",
      "15   The frequency of angiographically defined asym...  \n",
      "16   Cerebral infarction in sickle cell disease is ...  \n",
      "17   The purpose of this study was to analyze recov...  \n",
      "18   The purpose of this study is to examine the im...  \n",
      "19   Spontaneous echo contrast is a dynamic smokeli...  \n",
      "20   There is a strong link between antiphospholipi...  \n",
      "21   Patients with carotid stenosis have a high fre...  \n",
      "22   This study was performed to document the progr...  \n",
      "23   Ensuring the reliability and validity of outco...  \n",
      "24   The purpose of this study was to evaluate the ...  \n",
      "25   Animal stroke models demonstrate excitatory am...  \n",
      "26   Compaction of extracellular matrix (ECM) latti...  \n",
      "27   We sought (1) to compare the frequency and sev...  \n",
      "28   We sought to determine knowledge at the time o...  \n",
      "29   Medical and neurological complications after a...  \n",
      "..                                                 ...  \n",
      "411  We present a search for chargino-neutralino as...  \n",
      "412  The first observation of the production of a W...  \n",
      "413  Preclinical and retrospective clinical data in...  \n",
      "414  Stroke mortality has been declining since the ...  \n",
      "415  The Interventional Management of Stroke (IMS)-...  \n",
      "416  Endovascular strategies provide unique opportu...  \n",
      "417  Inflammatory biomarkers predict incident and r...  \n",
      "418  Studies assessing the relationship between chr...  \n",
      "419  A search for forbidden and exotic Z boson deca...  \n",
      "420  Because stroke is among the leading causes of ...  \n",
      "421  Blood pressure (BP) reduction lowers vascular ...  \n",
      "422  Interventional Management of Stroke (IMS) III ...  \n",
      "423  The Interventional Management of Stroke (IMS) ...  \n",
      "424  We present a measurement of the ratio of the t...  \n",
      "425  The aim of this updated guideline is to provid...  \n",
      "426  Mounting evidence points to a decline in strok...  \n",
      "427  We report the first observation of single-top-...  \n",
      "428  We report evidence for s-channel single-top-qu...  \n",
      "429  The first search for single-top-quark producti...  \n",
      "430  We measure the inclusive forward-backward asym...  \n",
      "431  The Carotid Revascularization Endarterectomy V...  \n",
      "432  Expert consensus guidelines recommend low-dens...  \n",
      "433  Evidence indicates that center volume of cases...  \n",
      "434  We explored changes in the patient population ...  \n",
      "435  The new pooled cohort risk (PCR) equations is ...  \n",
      "436  Our objective was to use decision analytic mod...  \n",
      "437  The aim of this updated statement is to provid...  \n",
      "438  We report final measurements of direct CP-viol...  \n",
      "439  Diffusion MRI is a promising, clinically feasi...  \n",
      "440  Randomized trials have indicated a benefit for...  \n",
      "\n",
      "[441 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#convert XML subset to pandas dataframe\n",
    "def iter_docs(author):\n",
    "    author_attr = author.attrib\n",
    "    titles_pos = ['Physical review letters']\n",
    "    titles_neg = ['Stroke; a journal of cerebral circulation']\n",
    "    for record in author.findall('MedlineCitation'):\n",
    "        doc_dict = author_attr.copy()\n",
    "        #doc_dict.update(record.attrib)\n",
    "        title = record.find('Article').find('Journal').find('Title').text\n",
    "        doc_dict['journal'] = title\n",
    "        abstract = record.find('Article').find('Abstract').find('AbstractText').text\n",
    "        #make sure all abstracts are in string format to avoid later sentence parsing issues (69 in test run were unicode)\n",
    "        if type(abstract) != str:\n",
    "            abstract = abstract.encode('utf8')\n",
    "        doc_dict['abstract'] = abstract\n",
    "        #add sentiment label based on journal title\n",
    "        if title in titles_neg:\n",
    "            doc_dict['sentiment'] = 0\n",
    "        elif title in titles_pos:\n",
    "            doc_dict['sentiment'] = 1\n",
    "        yield doc_dict\n",
    "        \n",
    "df = pd.DataFrame(list(iter_docs(rootNew)))\n",
    "df = df[['sentiment', 'journal', 'abstract']]\n",
    "print df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stroke; a journal of cerebral circulation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>journal</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Stroke; a journal of cerebral circulation</td>\n",
       "      <td>A standardized neurologic assessment scoring i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Stroke; a journal of cerebral circulation</td>\n",
       "      <td>Proven effective therapy to prevent ischemic d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Stroke; a journal of cerebral circulation</td>\n",
       "      <td>We describe a case of homozygous sickle cell a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                    journal  \\\n",
       "0          0  Stroke; a journal of cerebral circulation   \n",
       "1          0  Stroke; a journal of cerebral circulation   \n",
       "2          0  Stroke; a journal of cerebral circulation   \n",
       "\n",
       "                                            abstract  \n",
       "0  A standardized neurologic assessment scoring i...  \n",
       "1  Proven effective therapy to prevent ischemic d...  \n",
       "2  We describe a case of homozygous sickle cell a...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print df.loc[1][1]\n",
    "#don't know why no quotation marks like when using pandas.to_csv in original w2v inversion\n",
    "#but seems to work fine so ignore for now\n",
    "df.loc[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#split into train and test\n",
    "#since so little data, consider using cross-validation?\n",
    "fracTrain = 0.75\n",
    "nSamples = df.shape[0]\n",
    "order = np.random.permutation(nSamples) # come up with a random ordering\n",
    "splitIndex = int(np.round(nSamples*fracTrain))\n",
    "train1 = df.ix[order[:splitIndex],:]\n",
    "test1 = df.ix[order[splitIndex:],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(331, 3)\n",
      "(110, 3)\n"
     ]
    }
   ],
   "source": [
    "print train1.shape\n",
    "print test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sentiment                  journal  \\\n",
      "177          1  Physical review letters   \n",
      "\n",
      "                                              abstract  \n",
      "177  We report a measurement of the Lambda b0 lifet...  \n"
     ]
    }
   ],
   "source": [
    "#print first record in test set (note has different index value, indexed instead by location (pos. 0))\n",
    "print test1.iloc[[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Parsing Sentences & Building Word2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gets a list of positive review indexes and negative review indexes. Takes those respective reviews and parses them into sentences and appends to a list of positive and negative sentences, respectively. Also does the same for the unlabelled training sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp-vm/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "# ****** Split the training set into clean sentences\n",
    "#\n",
    "sentences_pos = []  # Initialize an empty list of sentences\n",
    "sentences_neg = []  # Initialize an empty list of sentences\n",
    "\n",
    "#here change to include all journal name labels of positive and negative\n",
    "inxs_pos = np.where(train1['sentiment'] == 1)[0].tolist()\n",
    "inxs_neg = np.where(train1['sentiment'] == 0)[0].tolist()\n",
    "\n",
    "print \"Parsing sentences from training set\"\n",
    "for inx in inxs_pos:\n",
    "    review = train1[\"abstract\"].iloc[inx]\n",
    "    sentences_pos += KaggleWord2VecUtility.review_to_sentences(review, tokenizer)\n",
    "\n",
    "for inx in inxs_neg:\n",
    "    review = train1[\"abstract\"].iloc[inx]\n",
    "    sentences_neg += KaggleWord2VecUtility.review_to_sentences(review, tokenizer)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'we', u'report', u'the', u'first', u'observation', u'of', u'diffractive', u'j', u'psi', u'mu', u'mu', u'production', u'in', u'pp', u'collisions', u'at', u'root', u'square', u's', u'tev'], [u'diffractive', u'events', u'are', u'identified', u'by', u'their', u'rapidity', u'gap', u'signature']]\n"
     ]
    }
   ],
   "source": [
    "print sentences_pos[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same with all sentences to build the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    }
   ],
   "source": [
    "# ****** Split the labeled and unlabeled training sets into clean sentences\n",
    "#\n",
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print \"Parsing sentences from training set\"\n",
    "for review in train1[\"abstract\"]:\n",
    "    sentences += KaggleWord2VecUtility.review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Build Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=0, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "\n",
    "## create a w2v learner \n",
    "basemodel = Word2Vec(\n",
    "    workers=multiprocessing.cpu_count(), # use your cores\n",
    "    iter=3, # iter = sweeps of SGD through the data; more is better\n",
    "    hs=1, negative=0 # we only have scoring for the hierarchical softmax setup\n",
    "    )\n",
    "print basemodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "basemodel.build_vocab(sentences) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Word2Vec model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34731"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model 0 corresponds with Stroke\n",
    "#and model 1 with Physical Review Letters\n",
    "\n",
    "from copy import deepcopy\n",
    "models = [deepcopy(basemodel) for i in range(2)]\n",
    "models[0].train(sentences_neg, total_examples=len(sentences_neg) )\n",
    "models[1].train(sentences_pos, total_examples=len(sentences_pos) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Applying inversion on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "docprob takes two lists\n",
    "* docs: a list of documents, each of which is a list of sentences\n",
    "* models: the candidate word2vec models (each potential class)\n",
    "\n",
    "it returns the array of class probabilities.  Everything is done in-memory.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd # for quick summing within doc\n",
    "\n",
    "def docprob(docs, mods):\n",
    "    # score() takes a list [s] of sentences here; could also be a sentence generator\n",
    "    sentlist = [s for d in docs for s in d]\n",
    "    # the log likelihood of each sentence in this review under each w2v representation\n",
    "    llhd = np.array( [ m.score(sentlist, len(sentlist)) for m in mods ] )\n",
    "    # now exponentiate to get likelihoods, \n",
    "    lhd = np.exp(llhd - llhd.max(axis=0)) # subtract row max to avoid numeric overload\n",
    "    # normalize across models (stars) to get sentence-star probabilities\n",
    "    #all this transposing business does is make it so the total probability of a word \n",
    "    #   equals 1 between the 2 arrays (positive prob and negative).\n",
    "    #and the pandas data frame just puts everything into rows/columns format for easy viz\n",
    "    prob = pd.DataFrame( (lhd/lhd.sum(axis=0)).transpose() )\n",
    "    # and finally average the sentence probabilities to get the review probability\n",
    "    prob[\"doc\"] = [i for i,d in enumerate(docs) for s in d]\n",
    "    prob = prob.groupby(\"doc\").mean()\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in the test set as a list of a list of words\n",
    "docs = []\n",
    "for review in test1[\"abstract\"]:\n",
    "    docs.append(KaggleWord2VecUtility.review_to_sentences(review, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'we', u'report', u'a', u'measurement', u'of', u'the', u'lambda', u'b', u'lifetime', u'in', u'the', u'exclusive', u'decay', u'lambda', u'b', u'j', u'psi', u'lambda', u'in', u'pp', u'collisions', u'at', u'square', u'root', u's', u'tev', u'using', u'an', u'integrated', u'luminosity', u'of', u'fb', u'of', u'data', u'collected', u'by', u'the', u'cdf', u'ii', u'detector', u'at', u'the', u'fermilab', u'tevatron'], [u'using', u'fully', u'reconstructed', u'decays', u'we', u'measure', u'tau', u'lambda', u'b', u'stat', u'syst', u'ps'], [u'this', u'is', u'the', u'single', u'most', u'precise', u'measurement', u'of', u'tau', u'lambda', u'b', u'and', u'is', u'sigma', u'higher', u'than', u'the', u'current', u'world', u'average']]\n"
     ]
    }
   ],
   "source": [
    "print docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the probs (note we give docprob our test set plus the models)\n",
    "probs = docprob(docs,models).astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.637194</td>\n",
       "      <td>0.362807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.874565</td>\n",
       "      <td>0.125435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.945536</td>\n",
       "      <td>0.0544643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.945557</td>\n",
       "      <td>0.0544429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.844883</td>\n",
       "      <td>0.155117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.999294</td>\n",
       "      <td>0.000706446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.947303</td>\n",
       "      <td>0.052697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.942912</td>\n",
       "      <td>0.0570877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.993642</td>\n",
       "      <td>0.00635769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.42004</td>\n",
       "      <td>0.57996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.979169</td>\n",
       "      <td>0.0208309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.813981</td>\n",
       "      <td>0.186019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.912569</td>\n",
       "      <td>0.0874306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.998714</td>\n",
       "      <td>0.00128609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.383071</td>\n",
       "      <td>0.61693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.805726</td>\n",
       "      <td>0.194274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.996986</td>\n",
       "      <td>0.00301362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.787626</td>\n",
       "      <td>0.212374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.677531</td>\n",
       "      <td>0.322469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.933291</td>\n",
       "      <td>0.0667093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.941126</td>\n",
       "      <td>0.0588735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.75613</td>\n",
       "      <td>0.24387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.825515</td>\n",
       "      <td>0.174485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.580352</td>\n",
       "      <td>0.419648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.921528</td>\n",
       "      <td>0.0784722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.753214</td>\n",
       "      <td>0.246786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.997281</td>\n",
       "      <td>0.00271938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.664903</td>\n",
       "      <td>0.335097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.0344827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.583658</td>\n",
       "      <td>0.416342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.751799</td>\n",
       "      <td>0.248201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.731557</td>\n",
       "      <td>0.268443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.934494</td>\n",
       "      <td>0.0655064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.9543</td>\n",
       "      <td>0.0456997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.747528</td>\n",
       "      <td>0.252472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.824652</td>\n",
       "      <td>0.175348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.223217</td>\n",
       "      <td>0.776783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.996781</td>\n",
       "      <td>0.00321941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.817516</td>\n",
       "      <td>0.182484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.98841</td>\n",
       "      <td>0.0115902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.834202</td>\n",
       "      <td>0.165798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.83002</td>\n",
       "      <td>0.16998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.941661</td>\n",
       "      <td>0.0583386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.31011</td>\n",
       "      <td>0.68989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.711067</td>\n",
       "      <td>0.288933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.786331</td>\n",
       "      <td>0.213669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.850521</td>\n",
       "      <td>0.149479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.994411</td>\n",
       "      <td>0.00558885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.693574</td>\n",
       "      <td>0.306426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.689161</td>\n",
       "      <td>0.310839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.40336</td>\n",
       "      <td>0.59664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.615613</td>\n",
       "      <td>0.384388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.987758</td>\n",
       "      <td>0.012242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.677435</td>\n",
       "      <td>0.322565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.873934</td>\n",
       "      <td>0.126066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.686088</td>\n",
       "      <td>0.313912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.930759</td>\n",
       "      <td>0.0692412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.623838</td>\n",
       "      <td>0.376162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.535888</td>\n",
       "      <td>0.464112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.903845</td>\n",
       "      <td>0.0961548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0            1\n",
       "doc                       \n",
       "0    0.637194     0.362807\n",
       "1    0.874565     0.125435\n",
       "2    0.945536    0.0544643\n",
       "3    0.945557    0.0544429\n",
       "4    0.844883     0.155117\n",
       "5    0.999294  0.000706446\n",
       "6    0.947303     0.052697\n",
       "7    0.942912    0.0570877\n",
       "8    0.993642   0.00635769\n",
       "9     0.42004      0.57996\n",
       "10   0.979169    0.0208309\n",
       "11   0.813981     0.186019\n",
       "12   0.912569    0.0874306\n",
       "13   0.998714   0.00128609\n",
       "14   0.383071      0.61693\n",
       "15   0.805726     0.194274\n",
       "16   0.996986   0.00301362\n",
       "17   0.787626     0.212374\n",
       "18   0.677531     0.322469\n",
       "19   0.933291    0.0667093\n",
       "20   0.941126    0.0588735\n",
       "21    0.75613      0.24387\n",
       "22   0.825515     0.174485\n",
       "23   0.580352     0.419648\n",
       "24   0.921528    0.0784722\n",
       "25   0.753214     0.246786\n",
       "26   0.997281   0.00271938\n",
       "27   0.664903     0.335097\n",
       "28   0.965517    0.0344827\n",
       "29   0.583658     0.416342\n",
       "..        ...          ...\n",
       "80   0.751799     0.248201\n",
       "81   0.731557     0.268443\n",
       "82   0.934494    0.0655064\n",
       "83     0.9543    0.0456997\n",
       "84   0.747528     0.252472\n",
       "85   0.824652     0.175348\n",
       "86   0.223217     0.776783\n",
       "87   0.996781   0.00321941\n",
       "88   0.817516     0.182484\n",
       "89    0.98841    0.0115902\n",
       "90   0.834202     0.165798\n",
       "91    0.83002      0.16998\n",
       "92   0.941661    0.0583386\n",
       "93    0.31011      0.68989\n",
       "94   0.711067     0.288933\n",
       "95   0.786331     0.213669\n",
       "96   0.850521     0.149479\n",
       "97   0.994411   0.00558885\n",
       "98   0.693574     0.306426\n",
       "99   0.689161     0.310839\n",
       "100   0.40336      0.59664\n",
       "101  0.615613     0.384388\n",
       "102  0.987758     0.012242\n",
       "103  0.677435     0.322565\n",
       "104  0.873934     0.126066\n",
       "105  0.686088     0.313912\n",
       "106  0.930759    0.0692412\n",
       "107  0.623838     0.376162\n",
       "108  0.535888     0.464112\n",
       "109  0.903845    0.0961548\n",
       "\n",
       "[110 rows x 2 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = np.ones((probs.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(110,)\n"
     ]
    }
   ],
   "source": [
    "print np.shape(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions[np.where(probs.iloc[:,0] > 0.5)] = 0 # The first column is the negative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "        0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "        0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(110,)\n",
      "(110,)\n"
     ]
    }
   ],
   "source": [
    "print predictions.shape\n",
    "print test1[\"sentiment\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.581818181818\n"
     ]
    }
   ],
   "source": [
    "print np.size(np.where(predictions == test1[\"sentiment\"]))*1./np.size(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Import KaggleWord2VecUtility since didn't work from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "class KaggleWord2VecUtility(object):\n",
    "    \"\"\"KaggleWord2VecUtility is a utility class for processing raw HTML text into segments for further learning\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def review_to_wordlist( review, remove_stopwords=False ):\n",
    "        # Function to convert a document to a sequence of words,\n",
    "        # optionally removing stop words.  Returns a list of words.\n",
    "        #\n",
    "        # 1. Remove HTML\n",
    "        review_text = BeautifulSoup(review).get_text()\n",
    "        #\n",
    "        # 2. Remove non-letters\n",
    "        review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "        #\n",
    "        # 3. Convert words to lower case and split them\n",
    "        words = review_text.lower().split()\n",
    "        #\n",
    "        # 4. Optionally remove stop words (false by default)\n",
    "        if remove_stopwords:\n",
    "            stops = set(stopwords.words(\"english\"))\n",
    "            words = [w for w in words if not w in stops]\n",
    "        #\n",
    "        # 5. Return a list of words\n",
    "        return(words)\n",
    "\n",
    "    # Define a function to split a review into parsed sentences\n",
    "    @staticmethod\n",
    "    def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "        # Function to split a review into parsed sentences. Returns a\n",
    "        # list of sentences, where each sentence is a list of words\n",
    "        #\n",
    "        # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "        raw_sentences = tokenizer.tokenize(review.decode('utf8').strip())\n",
    "        #\n",
    "        # 2. Loop over each sentence\n",
    "        sentences = []\n",
    "        for raw_sentence in raw_sentences:\n",
    "            # If a sentence is empty, skip it\n",
    "            if len(raw_sentence) > 0:\n",
    "                # Otherwise, call review_to_wordlist to get a list of words\n",
    "                sentences.append( KaggleWord2VecUtility.review_to_wordlist( raw_sentence, \\\n",
    "                  remove_stopwords ))\n",
    "        #\n",
    "        # Return the list of sentences (each sentence is a list of words,\n",
    "        # so this returns a list of lists\n",
    "        return sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
