{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get subset of MUSC records where journals equal those two chosen.\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.etree.ElementTree import Element\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk.data\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "from copy import deepcopy\n",
    "import datetime\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#on VM\n",
    "os.chdir('/mnt/mypartition/Desktop2/pubmed_nlp_research/DeepLearningMovies_datasets/')\n",
    "import KaggleWord2VecUtility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#on local:\n",
    "import sys\n",
    "sys.path.append('/home/sarahwie/Documents/pubmed-nlp-research/DeepLearningMovies_datasets/')\n",
    "from KaggleWord2VecUtility import KaggleWord2VecUtility\n",
    "DATADIR='/home/sarahwie/Documents/pubmed-nlp-research/DeepLearningMovies_datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one-time run\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting subset based on 2 journals (with abstracts and no Unicode encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree = ET.parse('/mnt/mypartition/Desktop2/testXMLwrite.xml')\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#using this file for our example\n",
    "xml_file = ET.parse('/home/sarahwie/Documents/zip/subset/zip/medline16n0189.xml')\n",
    "root = xml_file.getroot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code subsets the records down to those with the titles we are looking for (as well as those with abstracts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make new XML file\n",
    "rootNew = Element('MedlineCitationSet')\n",
    "children = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'unicode'>\n",
      "<type 'unicode'>\n"
     ]
    }
   ],
   "source": [
    "#replace titles here with those we're using\n",
    "#titles = ['Physical review letters', 'Stroke; a journal of cerebral circulation']\n",
    "titles = ['The Journal of biological chemistry','Journal of bacteriology']\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "k = 0\n",
    "l = 0\n",
    "for record in root.findall('MedlineCitation'):\n",
    "    title = record.find('Article').find('Journal').find('Title').text\n",
    "    \n",
    "    if title in titles:\n",
    "        j = j + 1\n",
    "        #for test run, also check if abstract\n",
    "        try:\n",
    "            abstract = record.find('Article').find('Abstract').find('AbstractText').text\n",
    "            if type(abstract) != str:\n",
    "                i = i + 1\n",
    "                print type(abstract)\n",
    "            #append to subset file\n",
    "            children.append(record)\n",
    "            k = k + 1\n",
    "        except AttributeError:\n",
    "            #dont include if no abstract\n",
    "            #something random to hold place of indent:\n",
    "            l = l + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "563\n",
      "422\n",
      "141\n"
     ]
    }
   ],
   "source": [
    "print i\n",
    "print j\n",
    "print k\n",
    "print l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rootNew.extend(children)\n",
    "\n",
    "#create tree and write to file\n",
    "tree = ET.ElementTree(rootNew)\n",
    "tree.write(\"/home/sarahwie/Documents/pubmed-nlp-research/output/subsetTwoJournals.xml\")\n",
    "        \n",
    "pickle.dump(rootNew, open('/home/sarahwie/Documents/pubmed-nlp-research/pickled_objects/rootTwoJournals.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarahwie/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:2: DeprecationWarning: This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "#size of new subset\n",
    "print len(rootNew.getchildren())\n",
    "#verify that this is the correct number: yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert XML to Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a36431848065>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[0mdoc_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter_docs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrootNew\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentiment'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'journal'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'abstract'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#convert XML subset to pandas dataframe\n",
    "def iter_docs(author):\n",
    "    author_attr = author.attrib\n",
    "    print author.attrib\n",
    "    #titles_pos = ['Physical review letters']\n",
    "    #titles_neg = ['Stroke; a journal of cerebral circulation']\n",
    "    titles_pos = ['The Journal of biological chemistry']\n",
    "    titles_neg = ['Journal of bacteriology']\n",
    "    for record in author.findall('MedlineCitation'):\n",
    "        doc_dict = author_attr.copy()\n",
    "        print doc_dict\n",
    "        #doc_dict.update(record.attrib)\n",
    "        title = record.find('Article').find('Journal').find('Title').text\n",
    "        doc_dict['journal'] = title\n",
    "        abstract = record.find('Article').find('Abstract').find('AbstractText').text\n",
    "        #make sure all abstracts are in string format to avoid later sentence parsing issues (69 in test run were unicode)\n",
    "        if type(abstract) != str:\n",
    "            abstract = abstract.encode('utf8')\n",
    "        doc_dict['abstract'] = abstract\n",
    "        #add sentiment label based on journal title\n",
    "        if title in titles_neg:\n",
    "            doc_dict['sentiment'] = 0\n",
    "        elif title in titles_pos:\n",
    "            doc_dict['sentiment'] = 1\n",
    "        yield doc_dict\n",
    "        \n",
    "df = pd.DataFrame(list(iter_docs(rootNew)))\n",
    "df = df[['sentiment', 'journal', 'abstract']]\n",
    "\n",
    "#shuffle rows\n",
    "df = df.iloc[np.random.permutation(len(df))]\n",
    "df = df.reset_index(drop=True)\n",
    "print df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Journal of bacteriology\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>journal</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Journal of bacteriology</td>\n",
       "      <td>ompB mutants of Escherichia coli K-12 are mark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Journal of bacteriology</td>\n",
       "      <td>Several lysosomal glycosidase activities were ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Journal of bacteriology</td>\n",
       "      <td>The initiation of patulin biosynthesis in subm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                  journal  \\\n",
       "0          0  Journal of bacteriology   \n",
       "1          0  Journal of bacteriology   \n",
       "2          0  Journal of bacteriology   \n",
       "\n",
       "                                            abstract  \n",
       "0  ompB mutants of Escherichia coli K-12 are mark...  \n",
       "1  Several lysosomal glycosidase activities were ...  \n",
       "2  The initiation of patulin biosynthesis in subm...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print df.loc[1][1]\n",
    "#don't know why no quotation marks like when using pandas.to_csv in original w2v inversion\n",
    "#but seems to work fine so ignore for now\n",
    "df.loc[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sarahwie/Documents/pubmed-nlp-research'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(df, open('/home/sarahwie/Documents/pubmed-nlp-research/pickled_objects/dfJournals.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get training and testing sets using 5-fold stratified cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d92a2591992e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/home/sarahwie/Documents/pubmed-nlp-research/pickled_objects/dfJournals.p'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "df = pickle.load(open('/home/sarahwie/Documents/pubmed-nlp-research/pickled_objects/dfJournals.p', 'rb'))\n",
    "print len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "docprob takes two lists\n",
    "* docs: a list of documents, each of which is a list of sentences\n",
    "* models: the candidate word2vec models (each potential class)\n",
    "\n",
    "it returns the array of class probabilities.  Everything is done in-memory.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd # for quick summing within doc\n",
    "\n",
    "def docprob(docs, mods):\n",
    "    # score() takes a list [s] of sentences here; could also be a sentence generator\n",
    "    sentlist = [s for d in docs for s in d]\n",
    "    # the log likelihood of each sentence in this review under each w2v representation\n",
    "    llhd = np.array( [ m.score(sentlist, len(sentlist)) for m in mods ] )\n",
    "    # now exponentiate to get likelihoods, \n",
    "    lhd = np.exp(llhd - llhd.max(axis=0)) # subtract row max to avoid numeric overload\n",
    "    # normalize across models (stars) to get sentence-star probabilities\n",
    "    #all this transposing business does is make it so the total probability of a word \n",
    "    #   equals 1 between the 2 arrays (positive prob and negative).\n",
    "    #and the pandas data frame just puts everything into rows/columns format for easy viz\n",
    "    prob = pd.DataFrame( (lhd/lhd.sum(axis=0)).transpose() )\n",
    "    # and finally average the sentence probabilities to get the review probability\n",
    "    prob[\"doc\"] = [i for i,d in enumerate(docs) for s in d]\n",
    "    prob = prob.groupby(\"doc\").mean()\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing inversion method: why is everything being scored as 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ROUND', 1)\n",
      "161\n",
      "176\n",
      "41\n",
      "44\n",
      "Parsing sentences from training set\n",
      "Building and training w2v models\n",
      "Parsing test sentences\n",
      "scoring test set\n",
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "85\n",
      "0.5\n",
      "0:00:02\n"
     ]
    }
   ],
   "source": [
    "a = datetime.datetime.now().replace(microsecond=0)\n",
    "\n",
    "#5-fold stratified cross validation\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "y = df['sentiment'].values\n",
    "skf = StratifiedKFold(y, n_folds=5, shuffle=True)\n",
    "\n",
    "i = 1\n",
    "for train_index, test_index in skf:\n",
    "    \n",
    "    if i == 1:\n",
    "        print(\"ROUND\", i)\n",
    "\n",
    "        #use the indexes to subset the df pandas dataframe\n",
    "        train1, test1 = df.iloc[train_index], df.iloc[test_index]\n",
    "        \n",
    "        print len(np.where(train1['sentiment'] == 0)[0])\n",
    "        print len(np.where(train1['sentiment'] == 1)[0])\n",
    "        print len(np.where(test1['sentiment'] == 0)[0])\n",
    "        print len(np.where(test1['sentiment'] == 1)[0])\n",
    "\n",
    "        # ****** Split the training set into clean sentences\n",
    "        #\n",
    "        sentences_pos = []  # Initialize an empty list of sentences\n",
    "        sentences_neg = []  # Initialize an empty list of sentences\n",
    "\n",
    "        #here change to include all journal name labels of positive and negative\n",
    "        inxs_pos = np.where(train1['sentiment'] == 1)[0].tolist()\n",
    "        inxs_neg = np.where(train1['sentiment'] == 0)[0].tolist()\n",
    "        \n",
    "        #print inxs_pos\n",
    "        #print inxs_neg\n",
    "        #print train1\n",
    "        \n",
    "        print \"Parsing sentences from training set\"\n",
    "        for inx in inxs_pos:\n",
    "            review = train1[\"abstract\"].iloc[inx]\n",
    "            sentences_pos += KaggleWord2VecUtility.review_to_sentences(review, tokenizer)\n",
    "\n",
    "        for inx in inxs_neg:\n",
    "            review = train1[\"abstract\"].iloc[inx]\n",
    "            sentences_neg += KaggleWord2VecUtility.review_to_sentences(review, tokenizer) \n",
    "\n",
    "        # ****** Split the labeled and unlabeled training sets into clean sentences\n",
    "        #\n",
    "        sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "        for review in train1[\"abstract\"]:\n",
    "            sentences += KaggleWord2VecUtility.review_to_sentences(review, tokenizer)\n",
    "\n",
    "        print \"Building and training w2v models\"\n",
    "        ## create a w2v learner \n",
    "        basemodel = Word2Vec(\n",
    "            workers=multiprocessing.cpu_count(), # use your cores\n",
    "            iter=3, # iter = sweeps of SGD through the data; more is better\n",
    "            hs=1, negative=0 # we only have scoring for the hierarchical softmax setup\n",
    "            )\n",
    "        basemodel.build_vocab(sentences) \n",
    "\n",
    "        #train models\n",
    "        models = [deepcopy(basemodel) for y in range(2)]\n",
    "        models[0].train(sentences_neg, total_examples=len(sentences_neg) )\n",
    "        models[1].train(sentences_pos, total_examples=len(sentences_pos) )\n",
    "\n",
    "        print \"Parsing test sentences\"\n",
    "        # read in the test set as a list of a list of words\n",
    "        docs = []\n",
    "        for review in test1[\"abstract\"]:\n",
    "            docs.append(KaggleWord2VecUtility.review_to_sentences(review, tokenizer))\n",
    "\n",
    "\n",
    "        print \"scoring test set\"\n",
    "        # get the probs (note we give docprob our test set plus the models)\n",
    "        probs = docprob(docs,models).astype(object)\n",
    "\n",
    "        predictions = np.ones((probs.shape[0]))\n",
    "        \n",
    "        #print test1.loc[0:50,['sentiment','abstract']]\n",
    "        #print probs[0:10]\n",
    "\n",
    "        predictions[np.where(probs.iloc[:,0] > 0.5)] = 0 # The first column is the negative model\n",
    "        print predictions\n",
    "\n",
    "        #think these line up\n",
    "        #ok so issue is just that document probabilities are so much higher scored for 0 than 1 for some reason.\n",
    "        #Cant figure out why...most likely not a bug but just the word2vec model not performing great\n",
    "        score = roc_auc_score(test1[\"sentiment\"], predictions)\n",
    "        #score = np.size(np.where(predictions == test1[\"sentiment\"]))*1./np.size(predictions)\n",
    "        print score\n",
    "\n",
    "    i = i + 1\n",
    "\n",
    "    \n",
    "b = datetime.datetime.now().replace(microsecond=0)\n",
    "print(b-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ROUND', 1)\n",
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarahwie/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building and training w2v models\n",
      "Parsing test sentences\n",
      "scoring test set\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "85\n",
      "0.510532150776\n",
      "('ROUND', 2)\n",
      "Parsing sentences from training set\n",
      "Building and training w2v models\n",
      "Parsing test sentences\n",
      "scoring test set\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "85\n",
      "0.487804878049\n",
      "('ROUND', 3)\n",
      "Parsing sentences from training set\n",
      "Building and training w2v models\n",
      "Parsing test sentences\n",
      "scoring test set\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "84\n",
      "0.510227272727\n",
      "('ROUND', 4)\n",
      "Parsing sentences from training set\n",
      "Building and training w2v models\n",
      "Parsing test sentences\n",
      "scoring test set\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "84\n",
      "0.5\n",
      "('ROUND', 5)\n",
      "Parsing sentences from training set\n",
      "Building and training w2v models\n",
      "Parsing test sentences\n",
      "scoring test set\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "84\n",
      "0.5\n",
      "('average of 5 rotations:', 0.50171286031042128)\n",
      "0:00:11\n"
     ]
    }
   ],
   "source": [
    "a = datetime.datetime.now().replace(microsecond=0)\n",
    "\n",
    "#5-fold stratified cross validation\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "#because no validation set, 4/5 of values go to train and 1/5 to test\n",
    "#is this too high?***\n",
    "#even though we shuffle, not as randomly distributed as the former method was\n",
    "y = df['sentiment'].values\n",
    "skf = StratifiedKFold(y, n_folds=5, shuffle=True)\n",
    "\n",
    "i = 1\n",
    "avg = []\n",
    "for train_index, test_index in skf:\n",
    "    print(\"ROUND\", i)\n",
    "    i = i + 1\n",
    "    #use the indexes to subset the df pandas dataframe\n",
    "    train1, test1 = df.iloc[train_index], df.iloc[test_index]\n",
    "    \n",
    "    # ****** Split the training set into clean sentences\n",
    "    #\n",
    "    sentences_pos = []  # Initialize an empty list of sentences\n",
    "    sentences_neg = []  # Initialize an empty list of sentences\n",
    "\n",
    "    #here change to include all journal name labels of positive and negative\n",
    "    inxs_pos = np.where(train1['sentiment'] == 1)[0].tolist()\n",
    "    inxs_neg = np.where(train1['sentiment'] == 0)[0].tolist()\n",
    "\n",
    "    print \"Parsing sentences from training set\"\n",
    "    for inx in inxs_pos:\n",
    "        review = train1[\"abstract\"].iloc[inx]\n",
    "        sentences_pos += KaggleWord2VecUtility.review_to_sentences(review, tokenizer)\n",
    "\n",
    "    for inx in inxs_neg:\n",
    "        review = train1[\"abstract\"].iloc[inx]\n",
    "        sentences_neg += KaggleWord2VecUtility.review_to_sentences(review, tokenizer) \n",
    "        \n",
    "    # ****** Split the labeled and unlabeled training sets into clean sentences\n",
    "    #\n",
    "    sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "    for review in train1[\"abstract\"]:\n",
    "        sentences += KaggleWord2VecUtility.review_to_sentences(review, tokenizer)\n",
    "    \n",
    "    print \"Building and training w2v models\"\n",
    "    ## create a w2v learner \n",
    "    basemodel = Word2Vec(\n",
    "        workers=multiprocessing.cpu_count(), # use your cores\n",
    "        iter=3, # iter = sweeps of SGD through the data; more is better\n",
    "        hs=1, negative=0 # we only have scoring for the hierarchical softmax setup\n",
    "        )\n",
    "    basemodel.build_vocab(sentences) \n",
    "    \n",
    "    #train models\n",
    "    models = [deepcopy(basemodel) for y in range(2)]\n",
    "    models[0].train(sentences_neg, total_examples=len(sentences_neg) )\n",
    "    models[1].train(sentences_pos, total_examples=len(sentences_pos) )\n",
    "    \n",
    "    print \"Parsing test sentences\"\n",
    "    # read in the test set as a list of a list of words\n",
    "    docs = []\n",
    "    for review in test1[\"abstract\"]:\n",
    "        docs.append(KaggleWord2VecUtility.review_to_sentences(review, tokenizer))\n",
    "    \n",
    "    print \"scoring test set\"\n",
    "    # get the probs (note we give docprob our test set plus the models)\n",
    "    probs = docprob(docs,models).astype(object)\n",
    "    \n",
    "    predictions = np.ones((probs.shape[0]))\n",
    "    \n",
    "    predictions[np.where(probs.iloc[:,0] > 0.5)] = 0 # The first column is the negative model\n",
    "    print predictions\n",
    "    print np.size(predictions)\n",
    "    \n",
    "    score = roc_auc_score(test1[\"sentiment\"], predictions)\n",
    "    #score = np.size(np.where(predictions == test1[\"sentiment\"]))*1./np.size(predictions)\n",
    "    print score\n",
    "    #append to average\n",
    "    avg.append(score)\n",
    "\n",
    "\n",
    "print(\"average of 5 rotations:\", sum(avg)/float(len(avg)))\n",
    "    \n",
    "b = datetime.datetime.now().replace(microsecond=0)\n",
    "print(b-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   3   5   6   7   9  11  12  13  14  15  16  17  18  19  20  21\n",
      "  22  23  25  26  27  28  29  30  32  33  34  35  36  37  38  41  42  44\n",
      "  45  46  47  48  49  50  51  52  54  55  56  57  58  59  60  61  64  65\n",
      "  67  68  69  70  71  72  73  74  75  76  77  78  79  82  83  84  86  87\n",
      "  89  90  91  92  93  95  96  97  98  99 100 101 102 103 104 106 107 108\n",
      " 109 112 113 114 116 118 119 120 121 122 123 124 125 126 127 129 130 131\n",
      " 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 150 151 153\n",
      " 154 155 156 157 158 159 160 161 162 164 165 166 167 168 169 170 171 172\n",
      " 174 178 179 180 181 183 184 185 186 188 189 190 191 192 193 194 198 199\n",
      " 202 203 204 206 207 208 209 210 212 213 214 215 216 219 220 221 223 224\n",
      " 225 226 227 228 229 231 232 233 234 235 236 237 238 239 240 242 243 244\n",
      " 246 247 248 249 250 251 252 253 254 255 256 257 259 260 261 262 263 264\n",
      " 265 266 267 268 270 272 273 274 275 276 278 279 280 282 284 285 286 287\n",
      " 288 289 290 292 293 294 295 296 297 299 300 301 302 303 305 307 308 309\n",
      " 311 314 316 317 318 320 322]\n",
      "[  2   4   8  10  24  31  39  40  43  53  62  63  66  80  81  85  88  94\n",
      " 105 110 111 115 117 128 147 148 149 152 163 173 175 176 177 182 187 195\n",
      " 196 197 200 201 205 211 217 218 222 230 241 245 258 269 271 277 281 283\n",
      " 291 298 304 306 310 312 313 315 319 321]\n",
      "     sentiment                              journal  \\\n",
      "0            0                       Brain research   \n",
      "1            0                       Brain research   \n",
      "2            0                       Brain research   \n",
      "3            0                       Brain research   \n",
      "4            0                       Brain research   \n",
      "5            0                       Brain research   \n",
      "6            0                       Brain research   \n",
      "7            0                       Brain research   \n",
      "8            0                       Brain research   \n",
      "9            0                       Brain research   \n",
      "10           0                       Brain research   \n",
      "11           0                       Brain research   \n",
      "12           0                       Brain research   \n",
      "13           0                       Brain research   \n",
      "14           0                       Brain research   \n",
      "15           1  The Journal of biological chemistry   \n",
      "16           1  The Journal of biological chemistry   \n",
      "17           1  The Journal of biological chemistry   \n",
      "18           1  The Journal of biological chemistry   \n",
      "19           1  The Journal of biological chemistry   \n",
      "20           1  The Journal of biological chemistry   \n",
      "21           1  The Journal of biological chemistry   \n",
      "22           1  The Journal of biological chemistry   \n",
      "23           1  The Journal of biological chemistry   \n",
      "24           1  The Journal of biological chemistry   \n",
      "25           1  The Journal of biological chemistry   \n",
      "26           1  The Journal of biological chemistry   \n",
      "27           1  The Journal of biological chemistry   \n",
      "28           1  The Journal of biological chemistry   \n",
      "29           1  The Journal of biological chemistry   \n",
      "..         ...                                  ...   \n",
      "249          1  The Journal of biological chemistry   \n",
      "250          1  The Journal of biological chemistry   \n",
      "251          1  The Journal of biological chemistry   \n",
      "252          1  The Journal of biological chemistry   \n",
      "253          1  The Journal of biological chemistry   \n",
      "254          1  The Journal of biological chemistry   \n",
      "255          1  The Journal of biological chemistry   \n",
      "256          1  The Journal of biological chemistry   \n",
      "257          1  The Journal of biological chemistry   \n",
      "258          1  The Journal of biological chemistry   \n",
      "259          1  The Journal of biological chemistry   \n",
      "260          1  The Journal of biological chemistry   \n",
      "261          1  The Journal of biological chemistry   \n",
      "262          1  The Journal of biological chemistry   \n",
      "263          1  The Journal of biological chemistry   \n",
      "264          1  The Journal of biological chemistry   \n",
      "265          1  The Journal of biological chemistry   \n",
      "266          1  The Journal of biological chemistry   \n",
      "267          1  The Journal of biological chemistry   \n",
      "268          1  The Journal of biological chemistry   \n",
      "269          1  The Journal of biological chemistry   \n",
      "270          1  The Journal of biological chemistry   \n",
      "271          1  The Journal of biological chemistry   \n",
      "272          1  The Journal of biological chemistry   \n",
      "273          1  The Journal of biological chemistry   \n",
      "274          1  The Journal of biological chemistry   \n",
      "275          1  The Journal of biological chemistry   \n",
      "276          1  The Journal of biological chemistry   \n",
      "277          1  The Journal of biological chemistry   \n",
      "278          1  The Journal of biological chemistry   \n",
      "\n",
      "                                              abstract  \n",
      "0    The dorsolateral quadrant of the lateral septa...  \n",
      "1    Changes of the internal (d) and external diame...  \n",
      "2    The neuranatomical location and pharmacologica...  \n",
      "3    The neuroanatomical organization of the effere...  \n",
      "4    The role of nerve growth factor (NGF) in the d...  \n",
      "5    The recurrence every 24 h of glucocorticoid el...  \n",
      "6    Dopamine-beta-hydroxylase (DBH) and norepineph...  \n",
      "7    Locally applied kainic acid was used in order ...  \n",
      "8    Using extracellular unit recording and microio...  \n",
      "9    The organization of globus pallidus (GP) proje...  \n",
      "10   The development of opiate mechanisms in the gu...  \n",
      "11   A regular slow wave theta rhythm can be record...  \n",
      "12   Fibers descending in the lateral columns (LC) ...  \n",
      "13   The incorporation of 35SO4(2-) and [3H]galacto...  \n",
      "14   Selectivity in the reinnervation of denervated...  \n",
      "15   The location of the functionally important -SH...  \n",
      "16   The pattern of proteins synthesized by chicken...  \n",
      "17   Preferential labeling of COOH-terminal sequenc...  \n",
      "18   Urinary acidification in the turtle urinary bl...  \n",
      "19   It has been demonstrated that the reaction of ...  \n",
      "20   The tetrameric form of a Desulfovibrio gigas f...  \n",
      "21   A new pigment from Halobacterium halobium has ...  \n",
      "22   The binding reaction of NADH to liver alcohol ...  \n",
      "23   The pokeweed antiviral protein and the toxin r...  \n",
      "24   Transport of methotrexate by L1210 sensitive a...  \n",
      "25   The rate of transport of alpha-aminoisobutyric...  \n",
      "26   Surface aromatic residues of bovine alpha-lact...  \n",
      "27   Cvc- mutants of Escherichia coli are deficient...  \n",
      "28   Total glycopeptides from human K-562 cells, la...  \n",
      "29   Liver glycogen phosphorylase associated with t...  \n",
      "..                                                 ...  \n",
      "249  A DNA-dependent ATPase with a molecular weight...  \n",
      "250  Submitochondrial particles obtained from skele...  \n",
      "251  Rabbits were made anemic to different extents ...  \n",
      "252  The fibroblast is a differentiated mesenchymal...  \n",
      "253  Conformational difference surrounding the coen...  \n",
      "254  The kinetic properties of human placental aden...  \n",
      "255  In order to examine the mechanism of the acute...  \n",
      "256  Fourteen clones were isolated in culture from ...  \n",
      "257  The bile acid-conjugating enzyme cholyl-coenzy...  \n",
      "258  The protein synthesis elongation factor EF-Tu,...  \n",
      "259  A theoretical analysis has been derived which ...  \n",
      "260  The mechanism of beef heart mitochondrial ATPa...  \n",
      "261  The reaction mechanism of melilotate hydroxyla...  \n",
      "262  A beta-galactoside alpha 1 leads to 2 fucosylt...  \n",
      "263  The acceptor substrate specificity and kinetic...  \n",
      "264  Metallothionein was purified under anaerobic c...  \n",
      "265  Membrane fluidity increases within 10 min afte...  \n",
      "266  We introduce two new, rapid procedures. One is...  \n",
      "267  A serine esterase with potent proteolytic acti...  \n",
      "268  Neurites were prepared by a novel method from ...  \n",
      "269  Immunoprecipitation of uniformly labeled yeast...  \n",
      "270  The uptake and metabolism of lymphatic large c...  \n",
      "271  Glutamine synthetase specific activity increas...  \n",
      "272  An icosadeoxyribonucleotide containing the sev...  \n",
      "273  pppA(2'p5'A)n-1 ((2'-5')(A)n) synthetase is on...  \n",
      "274  Stopped flow kinetic studies have been used to...  \n",
      "275  Alkaline-sensitive strains of Escherichia coli...  \n",
      "276  5-Dimethylaminonaphthalene-1-sulfonyl (dansyl)...  \n",
      "277  Removal of the terminal sialic acid residues f...  \n",
      "278  Monovalent Fab fragments from immunoglobulins,...  \n",
      "\n",
      "[259 rows x 3 columns]\n",
      "     sentiment                              journal  \\\n",
      "146          0                       Brain research   \n",
      "147          0                       Brain research   \n",
      "148          0                       Brain research   \n",
      "149          0                       Brain research   \n",
      "150          0                       Brain research   \n",
      "151          0                       Brain research   \n",
      "152          0                       Brain research   \n",
      "153          0                       Brain research   \n",
      "154          0                       Brain research   \n",
      "155          0                       Brain research   \n",
      "156          0                       Brain research   \n",
      "157          0                       Brain research   \n",
      "158          0                       Brain research   \n",
      "159          0                       Brain research   \n",
      "160          0                       Brain research   \n",
      "161          0                       Brain research   \n",
      "162          0                       Brain research   \n",
      "163          0                       Brain research   \n",
      "164          0                       Brain research   \n",
      "165          0                       Brain research   \n",
      "279          1  The Journal of biological chemistry   \n",
      "280          1  The Journal of biological chemistry   \n",
      "281          1  The Journal of biological chemistry   \n",
      "282          1  The Journal of biological chemistry   \n",
      "283          1  The Journal of biological chemistry   \n",
      "284          1  The Journal of biological chemistry   \n",
      "285          1  The Journal of biological chemistry   \n",
      "286          1  The Journal of biological chemistry   \n",
      "287          1  The Journal of biological chemistry   \n",
      "288          1  The Journal of biological chemistry   \n",
      "..         ...                                  ...   \n",
      "293          1  The Journal of biological chemistry   \n",
      "294          1  The Journal of biological chemistry   \n",
      "295          1  The Journal of biological chemistry   \n",
      "296          1  The Journal of biological chemistry   \n",
      "297          1  The Journal of biological chemistry   \n",
      "298          1  The Journal of biological chemistry   \n",
      "299          1  The Journal of biological chemistry   \n",
      "300          1  The Journal of biological chemistry   \n",
      "301          1  The Journal of biological chemistry   \n",
      "302          1  The Journal of biological chemistry   \n",
      "303          1  The Journal of biological chemistry   \n",
      "304          1  The Journal of biological chemistry   \n",
      "305          1  The Journal of biological chemistry   \n",
      "306          1  The Journal of biological chemistry   \n",
      "307          1  The Journal of biological chemistry   \n",
      "308          1  The Journal of biological chemistry   \n",
      "309          1  The Journal of biological chemistry   \n",
      "310          1  The Journal of biological chemistry   \n",
      "311          1  The Journal of biological chemistry   \n",
      "312          1  The Journal of biological chemistry   \n",
      "313          1  The Journal of biological chemistry   \n",
      "314          1  The Journal of biological chemistry   \n",
      "315          1  The Journal of biological chemistry   \n",
      "316          1  The Journal of biological chemistry   \n",
      "317          1  The Journal of biological chemistry   \n",
      "318          1  The Journal of biological chemistry   \n",
      "319          1  The Journal of biological chemistry   \n",
      "320          1  The Journal of biological chemistry   \n",
      "321          1  The Journal of biological chemistry   \n",
      "322          1  The Journal of biological chemistry   \n",
      "\n",
      "                                              abstract  \n",
      "146  Various stressful manipulations in rats (cold-...  \n",
      "147  The location of both afferent and efferent car...  \n",
      "148  Afferents of the lateral (LH) and ventromedial...  \n",
      "149  Efferent projections of the optic tectum were ...  \n",
      "150  The afferent projections to the primate amygda...  \n",
      "151  Astrogliogenesis in the human fetal cerebellum...  \n",
      "152  The ependymal and supraependymal cells of the ...  \n",
      "153  Surgical stress did not elevate plasma cortico...  \n",
      "154  The connection between the date of formation o...  \n",
      "155  The effects of unilateral medial forebrain bun...  \n",
      "156  The interaction between tonic labyrinth or nec...  \n",
      "157  When frog nerve-muscle preparations are stimul...  \n",
      "158  The purpose of this study was to examine the t...  \n",
      "159  The quantitative aspects of formaldehyde induc...  \n",
      "160  The rate and pattern of seizure development pr...  \n",
      "161  Primary cultures of fetal mouse brain and spin...  \n",
      "162  Unilateral transections of the brachium conjun...  \n",
      "163  L-[35S]Methionine was injected into the dorsal...  \n",
      "164  A study has been made of the regulation of evo...  \n",
      "165  The effect of monomeric acrylamide, a potent n...  \n",
      "279  In Escherichia coli, amino acid starvation tri...  \n",
      "280  The carbon monoxide binding kinetics of the is...  \n",
      "281  The activities of the mitochondrial NAD(P)+- a...  \n",
      "282  Cytochrome f has been purified from spinach ch...  \n",
      "283  The thermal and guanidine hydrochloride (GdnHC...  \n",
      "284  The cholinergic mouse neuroblastoma cell line ...  \n",
      "285  Heparosan N-sulfate D-glucuronosyl 5-epimerase...  \n",
      "286  The association rate constants for the interac...  \n",
      "287  Creatine kinase does not catalyze the scrambli...  \n",
      "288  Human plasm contains at least two distinct kin...  \n",
      "..                                                 ...  \n",
      "293  Saturation analysis of equilibrium binding of ...  \n",
      "294  In the rat hepatocyte, whether freshly separat...  \n",
      "295  The first quantitative measurements of the eff...  \n",
      "296  Kinetic studies on the unfolding of pepsinogen...  \n",
      "297  Cyanogen bromide fragments of alpha 1-protease...  \n",
      "298  Human alpha 1-protease inhibitor has three oli...  \n",
      "299  Previous observations suggested that pyridoxal...  \n",
      "300  Cytosol prepared from homogenates of bone from...  \n",
      "301  In previous studies in this laboratory, highly...  \n",
      "302  Tryptophan hydroxylase (EC 1.14.16.4) from rat...  \n",
      "303  The reactions of human hemoglobin with a serie...  \n",
      "304  The reactions of 13 isonitriles with deoxyhemo...  \n",
      "305  An enzyme present in rabbit liver microsomes h...  \n",
      "306  Several fluorinated derivatives of p-hydroxybe...  \n",
      "307  An eight-iron, eight-sulfur ferredoxin from Rh...  \n",
      "308  A dianionic spin label, 1-L-glutamate-5-N-(1-o...  \n",
      "309  Endogenous lactose-binding proteins from adult...  \n",
      "310  The specificity of the cyclic AMP-dependent pr...  \n",
      "311  An incomplete precursor of lipid A produced by...  \n",
      "312  The incomplete lipid A precursor produced by a...  \n",
      "313  The effect of cerulenin on conversion of an ac...  \n",
      "314  The intrinsic isotope effect on the reduction ...  \n",
      "315  Two human breast cancer cell lines (T-47D and ...  \n",
      "316  A wide variety of double-stranded DNA template...  \n",
      "317  Late in the life cycle of the single-stranded ...  \n",
      "318  A DNA-dependent RNA polymerase has been purifi...  \n",
      "319  We have obtained evidence in vivo for the intr...  \n",
      "320  The major aminoacyl-tRNA synthetase complex (t...  \n",
      "321  A DNA-dependent RNA polymerase has been extrac...  \n",
      "322  The irreversible inactivation by heat of the F...  \n",
      "\n",
      "[64 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#confirming index match to dataframe for the last fold\n",
    "print train_index\n",
    "print test_index\n",
    "print y_train\n",
    "print y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentiment         journal  \\\n",
      "2          0  Brain research   \n",
      "\n",
      "                                            abstract  \n",
      "2  The neuranatomical location and pharmacologica...  \n"
     ]
    }
   ],
   "source": [
    "#print first record in test set (note has different index value, indexed instead by location (pos. 0))\n",
    "print test1.iloc[[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Import KaggleWord2VecUtility since didn't work from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "class KaggleWord2VecUtility(object):\n",
    "    \"\"\"KaggleWord2VecUtility is a utility class for processing raw HTML text into segments for further learning\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def review_to_wordlist( review, remove_stopwords=False ):\n",
    "        # Function to convert a document to a sequence of words,\n",
    "        # optionally removing stop words.  Returns a list of words.\n",
    "        #\n",
    "        # 1. Remove HTML\n",
    "        review_text = BeautifulSoup(review).get_text()\n",
    "        #\n",
    "        # 2. Remove non-letters\n",
    "        review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "        #\n",
    "        # 3. Convert words to lower case and split them\n",
    "        words = review_text.lower().split()\n",
    "        #\n",
    "        # 4. Optionally remove stop words (false by default)\n",
    "        if remove_stopwords:\n",
    "            stops = set(stopwords.words(\"english\"))\n",
    "            words = [w for w in words if not w in stops]\n",
    "        #\n",
    "        # 5. Return a list of words\n",
    "        return(words)\n",
    "    \n",
    "    @staticmethod\n",
    "    def review_to_words( review, remove_stopwords=False ):\n",
    "        # Function to convert a raw review to a string of words\n",
    "        # The input is a single string (a raw movie review), and \n",
    "        # the output is a single string (a preprocessed movie review)\n",
    "        #\n",
    "        # 1. Remove HTML\n",
    "        review_text = BeautifulSoup(review).get_text() \n",
    "        #\n",
    "        # 2. Remove non-letters        \n",
    "        review_text = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "        #\n",
    "        # 3. Convert to lower case, split into individual words\n",
    "        words = review_text.lower().split()                             \n",
    "        #\n",
    "        # 4. Optionally remove stop words (false by default)\n",
    "        if remove_stopwords:\n",
    "            stops = set(stopwords.words(\"english\"))\n",
    "            words = [w for w in words if not w in stops]   \n",
    "        #\n",
    "        # 6. Join the words back into one string separated by space, \n",
    "        # and return the result.\n",
    "        return( \" \".join( words ))   \n",
    "\n",
    "    # Define a function to split a review into parsed sentences\n",
    "    @staticmethod\n",
    "    def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "        # Function to split a review into parsed sentences. Returns a\n",
    "        # list of sentences, where each sentence is a list of words\n",
    "        #\n",
    "        # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "        raw_sentences = tokenizer.tokenize(review.decode('utf8').strip())\n",
    "        #\n",
    "        # 2. Loop over each sentence\n",
    "        sentences = []\n",
    "        for raw_sentence in raw_sentences:\n",
    "            # If a sentence is empty, skip it\n",
    "            if len(raw_sentence) > 0:\n",
    "                # Otherwise, call review_to_wordlist to get a list of words\n",
    "                sentences.append( KaggleWord2VecUtility.review_to_wordlist( raw_sentence, \\\n",
    "                  remove_stopwords ))\n",
    "        #\n",
    "        # Return the list of sentences (each sentence is a list of words,\n",
    "        # so this returns a list of lists\n",
    "        return sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
